---
date: "2025-06-04"
---
# OpenAI Gym API and Gymnasium

## Creating an environment

The best starting point for working with environments in Gymnasium is the [official documentation](https://gymnasium.farama.org/), currently^[This has been checked as of the summer of 2025] maintained by the [Farama Foundation](https://farama.org/). I highly recommend consulting it - it makes it much easier to understand the structure of the action and observation spaces.

To create an environment, use `gym.make()`, passing the name of the environment as a string. The available environments and their different versions can be found in the documentation.
```{python}
# === creating an environment ===
import gymnasium as gym

env = gym.make("CartPole-v1")
env.action_space
```

According to the documentation, the `CartPole-v1` environment has 4 observations and 2 possible actions. Let’s confirm this by inspecting the action space:
```{python}
# === checking out the action_space ===
env.action_space
```

This confirms that we have two discrete actions `{0, 1}` (for CartPole and also generally the discrete actions are simply numbered starting from 0). Also note that in Gymnasium the `action_space` is usually fixed, i.e., independent of the state. We will see how to deal with changing action spaces when they matter.

Now let's check the observation space:
```{python}
# === checking out the observation_space ===
env.observation_space
```

The Box space represents a 4-dimensional continuous space, with lower and upper bounds for each dimension. The third component (the shape attribute) indicates that each observation is a vector of 4 values. The first and second components specify the lower and upper bounds for each of the 4 dimension of the observation, respectively. For more detail on what each dimension represents, refer to the documentation. According to Gymnasium, a Box is "a space that represents closed boxes in Euclidean space."

## The random CartPole agent

Let’s take the first step towards building a real agent: a random agent that takes actions randomly at each time step.

The idea is simple: the agent randomly samples actions from the environment’s action space until the episode ends - either by failure (the pole falling, cart to far off) or by timeout (very unlikely for a random agent).

```{python}
# === the random agent ===
import gymnasium as gym


def run_random_episode(env: gym.Env) -> float:  # <1>
    env.reset()  # <2>
    total_reward = 0.0
    done = False

    while not done:
        action = env.action_space.sample() #<3>
        _, reward, terminated, truncated, _ = env.step(action)  # <4>
        total_reward += reward
        done = terminated or truncated

    return total_reward
```

1.  use type hints like `env: gym.Env` as lightweight documentation. You’ll see this used frequently
2. in this random agent, we don’t store the initial observation from `env.reset()` because the agent doesn’t need it. Normally you would see something like `state, _ = env.reset()`
3. returns a sample (random) action for the current state of the environment
4. Similarly to `env.reset()`, we ignore the new state returned by `env.step(action)`

::: {.callout-note}
Make sure that you click on the code annotations for more infos.

If you're feeling completely lost, check out [this Gymnasium basics guide](https://gymnasium.farama.org/introduction/basic_usage/) which explains more of the fundamentals
:::
    
Let's see how well the random agent performs over a few episodes:
```{python}
import gymnasium as gym

env = gym.make("CartPole-v1")
episodes = 5
for episode in range(1, episodes + 1):
    print(f"\n=== Episode {episode}/{episodes} ===")  # <1>
    reward = run_random_episode(env)
    print(f"Reward: {reward}")
```

1. if this is your first time seeing Python's f-strings, they are a great way to include code in text. For example, f"5+5 is {5+5}" evaluates to "5+5 is 10"

The total reward in each episode corresponds to how long the pole stays balanced, because in `CartPole-v1`, the agent receives a reward of +1 for each step.

Obviously, it doesn't perform well. Our overall goal is to create agents that improve these rewards using reinforcement learning techniques.
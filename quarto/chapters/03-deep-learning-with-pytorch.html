<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-06-12">

<title>3&nbsp; Deep Learning with PyTorch – Notes for "Deep Reinforcement Learning Hands-On" by Maxim Lapan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../quarto/chapters/04-the-cross-entropy-method.html" rel="next">
<link href="../../quarto/chapters/02-gymnasium.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-6fc64a0c1cda8d1c841de64652c337fd.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-6fc64a0c1cda8d1c841de64652c337fd.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-fefeb8870b793b5cefa2e64bf46ee768.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-5399be3d0b0da586ba133997a49fd142.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../quarto/chapters/03-deep-learning-with-pytorch.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning with PyTorch</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Notes for “Deep Reinforcement Learning Hands-On” by Maxim Lapan</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../quarto/chapters/01-what-is-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">What Is Reinforcement Learning?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../quarto/chapters/02-gymnasium.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">OpenAI Gym API and Gymnasium</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../quarto/chapters/03-deep-learning-with-pytorch.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning with PyTorch</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../quarto/chapters/04-the-cross-entropy-method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The cross entropy method</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#pytorch" id="toc-pytorch" class="nav-link active" data-scroll-target="#pytorch"><span class="header-section-number">3.1</span> PyTorch</a></li>
  <li><a href="#tensors" id="toc-tensors" class="nav-link" data-scroll-target="#tensors"><span class="header-section-number">3.2</span> Tensors</a>
  <ul class="collapse">
  <li><a href="#creating-tensors-and-tensor-shapes" id="toc-creating-tensors-and-tensor-shapes" class="nav-link" data-scroll-target="#creating-tensors-and-tensor-shapes"><span class="header-section-number">3.2.1</span> Creating Tensors and Tensor Shapes</a></li>
  <li><a href="#indexing" id="toc-indexing" class="nav-link" data-scroll-target="#indexing"><span class="header-section-number">3.2.2</span> Indexing</a></li>
  </ul></li>
  <li><a href="#linear-neural-nets" id="toc-linear-neural-nets" class="nav-link" data-scroll-target="#linear-neural-nets"><span class="header-section-number">3.3</span> Linear neural nets</a></li>
  <li><a href="#sec-stochastic-gradient-descent" id="toc-sec-stochastic-gradient-descent" class="nav-link" data-scroll-target="#sec-stochastic-gradient-descent"><span class="header-section-number">3.4</span> Stochastic Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#single-gradient-descent-step" id="toc-single-gradient-descent-step" class="nav-link" data-scroll-target="#single-gradient-descent-step"><span class="header-section-number">3.4.1</span> Single Gradient Descent Step</a></li>
  <li><a href="#stochastic-gd-and-deterministic-gd" id="toc-stochastic-gd-and-deterministic-gd" class="nav-link" data-scroll-target="#stochastic-gd-and-deterministic-gd"><span class="header-section-number">3.4.2</span> Stochastic GD and Deterministic GD</a></li>
  <li><a href="#batches" id="toc-batches" class="nav-link" data-scroll-target="#batches"><span class="header-section-number">3.4.3</span> Batches</a></li>
  </ul></li>
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients"><span class="header-section-number">3.5</span> Gradients</a>
  <ul class="collapse">
  <li><a href="#why-bother" id="toc-why-bother" class="nav-link" data-scroll-target="#why-bother"><span class="header-section-number">3.5.1</span> Why Bother</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation"><span class="header-section-number">3.5.2</span> Backpropagation</a></li>
  <li><a href="#grad_fn" id="toc-grad_fn" class="nav-link" data-scroll-target="#grad_fn"><span class="header-section-number">3.5.3</span> grad_fn</a></li>
  <li><a href="#gradients-and-control-structures" id="toc-gradients-and-control-structures" class="nav-link" data-scroll-target="#gradients-and-control-structures"><span class="header-section-number">3.5.4</span> Gradients and Control Structures</a></li>
  <li><a href="#sec-viewBackward0" id="toc-sec-viewBackward0" class="nav-link" data-scroll-target="#sec-viewBackward0"><span class="header-section-number">3.5.5</span> The mysterious of the viewBackward0</a></li>
  </ul></li>
  <li><a href="#nn-building-blocks" id="toc-nn-building-blocks" class="nav-link" data-scroll-target="#nn-building-blocks"><span class="header-section-number">3.6</span> NN building blocks</a>
  <ul class="collapse">
  <li><a href="#relu" id="toc-relu" class="nav-link" data-scroll-target="#relu"><span class="header-section-number">3.6.1</span> ReLU</a></li>
  </ul></li>
  <li><a href="#example---gan-on-atari-images" id="toc-example---gan-on-atari-images" class="nav-link" data-scroll-target="#example---gan-on-atari-images"><span class="header-section-number">3.7</span> Example - GAN on Atari images</a>
  <ul class="collapse">
  <li><a href="#running" id="toc-running" class="nav-link" data-scroll-target="#running"><span class="header-section-number">3.7.1</span> Running</a></li>
  <li><a href="#viewing-the-data" id="toc-viewing-the-data" class="nav-link" data-scroll-target="#viewing-the-data"><span class="header-section-number">3.7.2</span> Viewing the Data</a></li>
  <li><a href="#discussing-the-code" id="toc-discussing-the-code" class="nav-link" data-scroll-target="#discussing-the-code"><span class="header-section-number">3.7.3</span> Discussing the code</a></li>
  <li><a href="#discriminator-vs-generator-loss" id="toc-discriminator-vs-generator-loss" class="nav-link" data-scroll-target="#discriminator-vs-generator-loss"><span class="header-section-number">3.7.4</span> Discriminator vs Generator Loss</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning with PyTorch</span></h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 12, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="pytorch" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="pytorch"><span class="header-section-number">3.1</span> PyTorch</h2>
<p>PyTorch is a library for deep learning. In this chapter, we’ll introduce its most core features, and we’ll cover everything else along the way later on.</p>
<p>We can import it with:</p>
<div id="9a73aff0" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === importing PyTorch ===</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code></pre></div>
</div>
</section>
<section id="tensors" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="tensors"><span class="header-section-number">3.2</span> Tensors</h2>
<p>The term ‘tensor’ can have different meanings depending on the field (see <a href="https://en.wikipedia.org/wiki/Tensor_(disambiguation)">Wikipedia</a>). In PyTorch, a tensor is essentially a multidimensional array with a lot of useful functionality built in. They are the fundamental data structure used for computations involving neural networks (NNs).</p>
<section id="creating-tensors-and-tensor-shapes" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="creating-tensors-and-tensor-shapes"><span class="header-section-number">3.2.1</span> Creating Tensors and Tensor Shapes</h3>
<p>We can ‘lift’ a python list into the universe of tensors. For example, here we convert a list into a rank-1 tensor (aka a vector):</p>
<div id="4dce34ce" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === a vector in PyTorch ===</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>tensor([1, 2, 3])</code></pre>
</div>
</div>
<p>The rank of a tensor indicates how many indices you need to specify to access a single element.</p>
<div id="a512103c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === ranks of tensors ===</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># a rank-1 tensor takes 1 index</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Element at index 0: </span><span class="sc">{</span>v[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># a rank-1 tensor takes 2 indices</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Element at (0,1): </span><span class="sc">{</span>m[<span class="dv">0</span>][<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># a rank-3 tensor takes 3 indices</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor([[[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]]])</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Element at (0,1,0): </span><span class="sc">{</span>t[<span class="dv">0</span>][<span class="dv">1</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Element at index 0: 1
Element at (0,1): 2
Element at (0,1,0): 3</code></pre>
</div>
</div>
<p>The term ‘dimension’ is often used interchangeably with rank. This was a bit confusing for me initially! Let me briefly explain why the term ‘dimension’ is used:</p>
<p>It’s not the same as the ‘dimension’ used when discussing vectors (e.g., a 3-dimensional vector), which refers to the degrees of freedom within the underlying vector space. Instead, it refers to the tensor’s size as a discrete space - the rank represents the number of independent directions you can move between elements.</p>
<p>I’ll stick to ‘rank’ when talking about tensors, and generally use the terminology from <a href="https://www.tensorflow.org/guide/tensor">TensorFlow’s introduction to tensors</a>.</p>
<p>Instead of converting Python arrays into tensors, we can also create them directly:</p>
<div id="07a9caaf" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === common tensor creation methods ===</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st"> a rank-2 tensor full of 0s"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.zeros((<span class="dv">3</span>, <span class="dv">2</span>)))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st"> a rank-2 tensor full of 7s"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.full((<span class="dv">3</span>, <span class="dv">2</span>), <span class="dv">7</span>))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st"> a rank-2 tensor of random integers [0-9]"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">3</span>, <span class="dv">2</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 a rank-2 tensor full of 0s
tensor([[0., 0.],
        [0., 0.],
        [0., 0.]])

 a rank-2 tensor full of 7s
tensor([[7, 7],
        [7, 7],
        [7, 7]])

 a rank-2 tensor of random integers [0-9]
tensor([[7, 0],
        [3, 5],
        [5, 0]])</code></pre>
</div>
</div>
<p>The tuple (3, 2) in the code above defines the tensor’s shape: how many axes it has and the size of each. So in this example, the first axis has size 3 and the second has size 2.</p>
<p>We can query the shape of a tensor with <code>.shape</code>.</p>
<div id="c297ddd0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the shape of a tensor ===</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.zeros((<span class="dv">3</span>, <span class="dv">2</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([3, 2])</code></pre>
</div>
</div>
<p>The return type is <code>torch.Size</code>, which is PyTorch’s internal way of representing shape.</p>
<p>A special case of tensors are scalars, rank-0 tensors, whose shape is <span class="math inline">\(()\)</span>.</p>
<div id="6a46da0d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === scalar tensor ===</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor(<span class="dv">2</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(2)
torch.Size([])</code></pre>
</div>
</div>
</section>
<section id="indexing" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="indexing"><span class="header-section-number">3.2.2</span> Indexing</h3>
<p>To illustrate tensor indexing, we consider a tensor of shape (2, 3, 4) where each element’s value represents its coordinates in mathematical notation:</p>
<div id="378ebd31" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === index positions of tensors ===</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># each entry in this tensor shows it's coordinate in mathematical notation</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor(</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">111</span>, <span class="dv">112</span>, <span class="dv">113</span>, <span class="dv">114</span>], [<span class="dv">121</span>, <span class="dv">122</span>, <span class="dv">123</span>, <span class="dv">124</span>], [<span class="dv">131</span>, <span class="dv">132</span>, <span class="dv">133</span>, <span class="dv">134</span>]],</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">211</span>, <span class="dv">212</span>, <span class="dv">213</span>, <span class="dv">214</span>], [<span class="dv">221</span>, <span class="dv">222</span>, <span class="dv">223</span>, <span class="dv">224</span>], [<span class="dv">231</span>, <span class="dv">232</span>, <span class="dv">233</span>, <span class="dv">234</span>]],</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The element at mathematical coordinates (2,3,4) is: </span><span class="sc">{</span>t[<span class="dv">1</span>][<span class="dv">2</span>][<span class="dv">3</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[111, 112, 113, 114],
         [121, 122, 123, 124],
         [131, 132, 133, 134]],

        [[211, 212, 213, 214],
         [221, 222, 223, 224],
         [231, 232, 233, 234]]])
The element at mathematical coordinates (2,3,4) is: 234</code></pre>
</div>
</div>
<p>So, the order of the indices reflects the order of the axes in the shape. Therefore, <code>t[0][1][2]</code> refers to the element at index 0 along the first axis, index 1 along the second axis, and index 2 along the third axis.</p>
</section>
</section>
<section id="linear-neural-nets" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="linear-neural-nets"><span class="header-section-number">3.3</span> Linear neural nets</h2>
<p>Let’s explore the simplest type of neural network: the linear neural network. The basic ideas used here apply to more complex ones too.</p>
<p>To work with neural networks in PyTorch, we import the <code>nn</code> module:</p>
<div id="3b6dea29" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span></code></pre></div>
</div>
<p>A linear neural network consists of input and output nodes, where each input signal is multiplied by a weight and passed to an output node. Each output node also has a constant bias added to it.</p>
<p>Let’s look at a little example.</p>
<div id="f52e8c1f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="annotated-cell-9"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-9-1"><a href="#annotated-cell-9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === linear nn with custom parameters ===</span></span>
<span id="annotated-cell-9-2"><a href="#annotated-cell-9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># create a linear layer</span></span>
<span id="annotated-cell-9-3"><a href="#annotated-cell-9-3" aria-hidden="true" tabindex="-1"></a>linear_nn <span class="op">=</span> nn.Linear(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="annotated-cell-9-4"><a href="#annotated-cell-9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-5"><a href="#annotated-cell-9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># manually set the weights</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="1">1</button><span id="annotated-cell-9-6" class="code-annotation-target"><a href="#annotated-cell-9-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> torch.FloatTensor([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="annotated-cell-9-7"><a href="#annotated-cell-9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-8"><a href="#annotated-cell-9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># manually set the bias</span></span>
<span id="annotated-cell-9-9"><a href="#annotated-cell-9-9" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> torch.FloatTensor([<span class="dv">2</span>, <span class="op">-</span><span class="dv">3</span>])</span>
<span id="annotated-cell-9-10"><a href="#annotated-cell-9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-11"><a href="#annotated-cell-9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># assign the weights and bias to the linear layer</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="2">2</button><span id="annotated-cell-9-12" class="code-annotation-target"><a href="#annotated-cell-9-12" aria-hidden="true" tabindex="-1"></a>linear_nn.weight <span class="op">=</span> nn.Parameter(weights)</span>
<span id="annotated-cell-9-13"><a href="#annotated-cell-9-13" aria-hidden="true" tabindex="-1"></a>linear_nn.bias <span class="op">=</span> nn.Parameter(bias)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-9" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="6,9" data-code-annotation="1">neural networks in PyTorch require floating-point tensors. That’s why we used <code>torch.FloatTensor</code> to create the weights and biases</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="12,13" data-code-annotation="2">a <code>nn.Parameter</code> is a tensor that is recognized as a learnable parameter of the neural network. We’ll come back to that later when we discuss stochastic gradient descent (<a href="#sec-stochastic-gradient-descent" class="quarto-xref"><span>Section 3.4</span></a>)</span>
</dd>
</dl>
</div>
</div>
<p>When we apply this network to to a vector <span class="math inline">\((x,y,z)\)</span>, the computation is equivalent to: <span class="math display">\[
\mathrm{linear\_nn}(x,y,z) =
\begin{pmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -1
\end{pmatrix}
\begin{pmatrix}
x \\ y \\ z
\end{pmatrix}
+
\begin{pmatrix}
2 \\ -3
\end{pmatrix}
\]</span></p>
<p>For example for the input <span class="math inline">\((1,0,0)\)</span> we should get <span class="math inline">\((3,-3)\)</span>. We can check this by using <code>.forward(x)</code> to pass a tensor into the network:</p>
<div id="a17a39a1" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="annotated-cell-10"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-10-1"><a href="#annotated-cell-10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === applying the network to an input ===</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-10" data-target-annotation="1">1</button><span id="annotated-cell-10-2" class="code-annotation-target"><a href="#annotated-cell-10-2" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="annotated-cell-10-3"><a href="#annotated-cell-10-3" aria-hidden="true" tabindex="-1"></a>linear_nn.forward(v)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-10" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-10" data-code-lines="2" data-code-annotation="1">instead of explicitly using torch.FloatTensor, we can implicitly coerce PyTorch into creating the correct tensor type by specifying float literals (e.g., 1.0 instead of 1).</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([ 3., -3.], grad_fn=&lt;ViewBackward0&gt;)</code></pre>
</div>
</div>
<p>We get the expected result, but there’s an additional detail: <code>grad_fn=&lt;ViewBackward0&gt;</code>. This is the first indication of PyTorch’s internal gradient tracking mechanism - a core requirement for stochastic gradient descent (SGD), which we’ll explore in the next chapter. We’ll return uncover the mystery of ViewBackward0 in <a href="#sec-viewBackward0" class="quarto-xref"><span>Section 3.5.5</span></a>.</p>
</section>
<section id="sec-stochastic-gradient-descent" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-stochastic-gradient-descent"><span class="header-section-number">3.4</span> Stochastic Gradient Descent</h2>
<p>We can model the behaviour of a neural network as a function <span class="math inline">\(f(x_i;\theta)\)</span> that maps inputs to outputs, defined by a set of learnable parameters, denoted as <span class="math inline">\(θ\)</span>.</p>
<p>At its core, training a neural network means finding model parameters that minimize a loss function <span class="math display">\[
L(\theta) = \frac{1}{N} \sum_{i=1}^N \ell(f(x_i;\theta), y_i),
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\((x_i, y_i)\)</span> represents a training sample (input-output pair)</li>
<li><span class="math inline">\(\ell\)</span> measures prediction error for individual samples</li>
<li><span class="math inline">\(L\)</span> represents the average loss across all <span class="math inline">\(N\)</span> samples in a batch</li>
<li><span class="math inline">\(f(x_i, \theta)\)</span> is the network’s prediction given input <span class="math inline">\(x_i\)</span> and parameters <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>This is called ‘stochastic’ gradient descent, because the <span class="math inline">\(x_i,y_i\)</span> are sampled by some stochastic process.</p>
<p>For basic stochastic gradient descent, we try to minimise the loss by iteratively adjusting the parameters against the gradient: <span class="math display">\[
\theta \gets \theta - \eta \nabla_{\theta}L(\theta),
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate, which controls the step size of each update.</p>
<section id="single-gradient-descent-step" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="single-gradient-descent-step"><span class="header-section-number">3.4.1</span> Single Gradient Descent Step</h3>
<p>Let’s look at a simple example of such an update step, using a tiny linear neural network with just one input and one output. Its parameters are <span class="math inline">\(\theta = (w, b)\)</span>. We’ll train it on a single sample <span class="math inline">\((x,y)=(3,5)\)</span> - we ignore batches for now - using the squared error: <span class="math display">\[
L(f(x;\theta), y) = (f(x;\theta) - y)^2
\]</span></p>
<p>We want to work out the gradient by hand to see which way it points for parameters <span class="math inline">\(\theta =(0,0)\)</span>. The loss is: <span class="math display">\[
\begin{split}
L((w,b)) &amp;= (3w + b - 5)^2\\
&amp;= 9w^2 + 6w(b-5) + (b-5)^2
\end{split}
\]</span></p>
<p>So the gradient is <span class="math display">\[
\nabla_\theta L =
\begin{pmatrix}
18w + 6b - 30 \\
6w + 2b - 10
\end{pmatrix}
\quad
\Rightarrow
\quad
\nabla_\theta L(0,0) = \begin{pmatrix}
- 30 \\
- 10
\end{pmatrix}
\]</span></p>
<p>Now, letting PyTorch calculate the gradient, we get the same result (which feels a bit magical):</p>
<div id="52eb5f76" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="annotated-cell-11"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-11-1"><a href="#annotated-cell-11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === one SGD step: calculating gradient ===</span></span>
<span id="annotated-cell-11-2"><a href="#annotated-cell-11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="annotated-cell-11-3"><a href="#annotated-cell-11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="annotated-cell-11-4"><a href="#annotated-cell-11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-5"><a href="#annotated-cell-11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># linear model: f(x) = 0.0 x + 0.0</span></span>
<span id="annotated-cell-11-6"><a href="#annotated-cell-11-6" aria-hidden="true" tabindex="-1"></a>model_net <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-11" data-target-annotation="1">1</button><span id="annotated-cell-11-7" class="code-annotation-target"><a href="#annotated-cell-11-7" aria-hidden="true" tabindex="-1"></a>model_net.weight <span class="op">=</span> nn.Parameter(torch.tensor([[<span class="fl">0.0</span>]]))</span>
<span id="annotated-cell-11-8"><a href="#annotated-cell-11-8" aria-hidden="true" tabindex="-1"></a>model_net.bias <span class="op">=</span> nn.Parameter(torch.tensor([<span class="fl">0.0</span>]))</span>
<span id="annotated-cell-11-9"><a href="#annotated-cell-11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-10"><a href="#annotated-cell-11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># sample</span></span>
<span id="annotated-cell-11-11"><a href="#annotated-cell-11-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">3.0</span>])</span>
<span id="annotated-cell-11-12"><a href="#annotated-cell-11-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="fl">5.0</span>])</span>
<span id="annotated-cell-11-13"><a href="#annotated-cell-11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-14"><a href="#annotated-cell-11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-15"><a href="#annotated-cell-11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># loss function</span></span>
<span id="annotated-cell-11-16"><a href="#annotated-cell-11-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(prediction, target) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="annotated-cell-11-17"><a href="#annotated-cell-11-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (prediction <span class="op">-</span> target).<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="annotated-cell-11-18"><a href="#annotated-cell-11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-19"><a href="#annotated-cell-11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-20"><a href="#annotated-cell-11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># gradient calculation</span></span>
<span id="annotated-cell-11-21"><a href="#annotated-cell-11-21" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> loss(model_net.forward(x), y)</span>
<span id="annotated-cell-11-22"><a href="#annotated-cell-11-22" aria-hidden="true" tabindex="-1"></a>L.backward()  <span class="co"># computes ∇L for each model parameter</span></span>
<span id="annotated-cell-11-23"><a href="#annotated-cell-11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-11-24"><a href="#annotated-cell-11-24" aria-hidden="true" tabindex="-1"></a><span class="co"># this is how we access the components of ∇L</span></span>
<span id="annotated-cell-11-25"><a href="#annotated-cell-11-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"∂L/∂w: </span><span class="sc">{</span>model_net<span class="sc">.</span>weight<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="annotated-cell-11-26"><a href="#annotated-cell-11-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"∂L/∂b: </span><span class="sc">{</span>model_net<span class="sc">.</span>bias<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-11" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-11" data-code-lines="7" data-code-annotation="1">the weight parameter is a tensor of shape (inputs, outputs) = (1, 1), which as a list is just [[w]].</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>∂L/∂w: tensor([[-30.]])
∂L/∂b: tensor([-10.])</code></pre>
</div>
</div>
<p>Note the nice terminology. We use <code>.forward(x)</code> to push a tensor x through our net. And with <code>.backward()</code> we propagate the loss through the net backwards: During the forward flow, the data enters the network and flows in one direction, from input layer to the output layer. During the backward pass, it calculates the gradients of the loss function with respect to all the learnable parameters (weights and biases) in the network. This backward pass is tracing the influence to the loss back through all the calculations that contributed to it. It’s in the opposite direction of the forward flow.</p>
<p>To complete the “one stochastic gradient descent step” we need to update the model parameters, by walking against the gradient:</p>
<div id="ff0f80e4" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="annotated-cell-12"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-12-1"><a href="#annotated-cell-12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === one SGD step: updating model parameters ===</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-12" data-target-annotation="1">1</button><span id="annotated-cell-12-2" class="code-annotation-target"><a href="#annotated-cell-12-2" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># η</span></span>
<span id="annotated-cell-12-3"><a href="#annotated-cell-12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-12-4"><a href="#annotated-cell-12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># θ ← θ - η ∇L(θ)</span></span>
<span id="annotated-cell-12-5"><a href="#annotated-cell-12-5" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> model_net.weight <span class="op">-</span> learning_rate <span class="op">*</span> model_net.weight.grad</span>
<span id="annotated-cell-12-6"><a href="#annotated-cell-12-6" aria-hidden="true" tabindex="-1"></a>new_bias <span class="op">=</span> model_net.bias <span class="op">-</span> learning_rate <span class="op">*</span> model_net.bias.grad</span>
<span id="annotated-cell-12-7"><a href="#annotated-cell-12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-12-8"><a href="#annotated-cell-12-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The new parameters are:"</span>)</span>
<span id="annotated-cell-12-9"><a href="#annotated-cell-12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"weight: </span><span class="sc">{</span>new_weight<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="annotated-cell-12-10"><a href="#annotated-cell-12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"bias: </span><span class="sc">{</span>new_bias<span class="sc">}</span><span class="ss">"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-12" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-12" data-code-lines="2" data-code-annotation="1">The learning rate here is chosen somewhat arbitrarily. In practice, it has to be small enough to avoid overshooting but big enough to achieve progress.</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>The new parameters are:
weight: tensor([[0.3000]], grad_fn=&lt;SubBackward0&gt;)
bias: tensor([0.1000], grad_fn=&lt;SubBackward0&gt;)</code></pre>
</div>
</div>
<p>And the new parameters do perform better regarding the loss</p>
<div id="ca6d4fda" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="annotated-cell-13"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-13-1"><a href="#annotated-cell-13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === one SGD step: new vs old parameters ===</span></span>
<span id="annotated-cell-13-2"><a href="#annotated-cell-13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-13-3"><a href="#annotated-cell-13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model with new parameters</span></span>
<span id="annotated-cell-13-4"><a href="#annotated-cell-13-4" aria-hidden="true" tabindex="-1"></a>new_model_net <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="annotated-cell-13-5"><a href="#annotated-cell-13-5" aria-hidden="true" tabindex="-1"></a>new_model_net.weight <span class="op">=</span> nn.Parameter(new_weight)</span>
<span id="annotated-cell-13-6"><a href="#annotated-cell-13-6" aria-hidden="true" tabindex="-1"></a>new_model_net.bias <span class="op">=</span> nn.Parameter(new_bias)</span>
<span id="annotated-cell-13-7"><a href="#annotated-cell-13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-13-8"><a href="#annotated-cell-13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions</span></span>
<span id="annotated-cell-13-9"><a href="#annotated-cell-13-9" aria-hidden="true" tabindex="-1"></a>old_prediction <span class="op">=</span> model_net.forward(x)</span>
<span id="annotated-cell-13-10"><a href="#annotated-cell-13-10" aria-hidden="true" tabindex="-1"></a>new_prediction <span class="op">=</span> new_model_net.forward(x)</span>
<span id="annotated-cell-13-11"><a href="#annotated-cell-13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-13-12"><a href="#annotated-cell-13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># losses</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-13" data-target-annotation="1">1</button><span id="annotated-cell-13-13" class="code-annotation-target"><a href="#annotated-cell-13-13" aria-hidden="true" tabindex="-1"></a>old_loss <span class="op">=</span> loss(old_prediction, y).item()</span>
<span id="annotated-cell-13-14"><a href="#annotated-cell-13-14" aria-hidden="true" tabindex="-1"></a>new_loss <span class="op">=</span> loss(new_prediction, y).item()</span>
<span id="annotated-cell-13-15"><a href="#annotated-cell-13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-13-16"><a href="#annotated-cell-13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-13-17"><a href="#annotated-cell-13-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"loss old: </span><span class="sc">{</span>old_loss<span class="sc">}</span><span class="ss"> &gt; loss new: </span><span class="sc">{</span>new_loss<span class="sc">}</span><span class="ss">"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-13" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-13" data-code-lines="13,14" data-code-annotation="1">we can use <code>item()</code> to get a number from a tensor that has one element</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>loss old: 25.0 &gt; loss new: 16.0</code></pre>
</div>
</div>
</section>
<section id="stochastic-gd-and-deterministic-gd" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="stochastic-gd-and-deterministic-gd"><span class="header-section-number">3.4.2</span> Stochastic GD and Deterministic GD</h3>
<p>Let’s extend our one-step example into a proper little stochastic gradient descent (SGD) routine. The goal is to train our model function <span class="math inline">\(f(x;\theta)\)</span> to approximate a target function <span class="math inline">\(f(x;\theta_*)\)</span>, where the target parameters are <span class="math inline">\(\theta_* = (2,1)\)</span>.</p>
<p>The premise is that we don’t know the target parameters - we can only draw samples from the target function. In our example, we sample the values of <span class="math inline">\(x\)</span> uniformly from the interval <span class="math inline">\([0,1)\)</span>.</p>
<p>Now, we set up a small program that performs gradient descent: <span class="math display">\[
\theta_{t+1} \gets \theta_t - \eta \nabla_{\theta}L(f(x_t;\theta_t),y_t),
\]</span></p>
<p>with <span class="math inline">\(L(x,y) = (x - y)^2\)</span>, <span class="math inline">\(y_t = f(x_t, \theta_*)\)</span>, and <span class="math inline">\(\theta_0 = (0,0)\)</span>.</p>
<div id="fd67a558" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="annotated-cell-14"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-14-1"><a href="#annotated-cell-14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === stochastic GD ===</span></span>
<span id="annotated-cell-14-2"><a href="#annotated-cell-14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span> Parameter <span class="op">=</span> <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>]</span>
<span id="annotated-cell-14-3"><a href="#annotated-cell-14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-4"><a href="#annotated-cell-14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-5"><a href="#annotated-cell-14-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stochastic_gradient_descent(steps, target_parameters, seed<span class="op">=</span><span class="dv">0</span>) <span class="op">-&gt;</span> <span class="bu">list</span>[Parameter]:</span>
<span id="annotated-cell-14-6"><a href="#annotated-cell-14-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""performs SGD for a specified number of steps"""</span></span>
<span id="annotated-cell-14-7"><a href="#annotated-cell-14-7" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-14" data-target-annotation="1">1</button><span id="annotated-cell-14-8" class="code-annotation-target"><a href="#annotated-cell-14-8" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="annotated-cell-14-9"><a href="#annotated-cell-14-9" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># η</span></span>
<span id="annotated-cell-14-10"><a href="#annotated-cell-14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-11"><a href="#annotated-cell-14-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># target function with given target_parameters</span></span>
<span id="annotated-cell-14-12"><a href="#annotated-cell-14-12" aria-hidden="true" tabindex="-1"></a>    target_function <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="annotated-cell-14-13"><a href="#annotated-cell-14-13" aria-hidden="true" tabindex="-1"></a>    target_function.weight <span class="op">=</span> nn.Parameter(torch.tensor([[target_parameters[<span class="dv">0</span>]]]))</span>
<span id="annotated-cell-14-14"><a href="#annotated-cell-14-14" aria-hidden="true" tabindex="-1"></a>    target_function.bias <span class="op">=</span> nn.Parameter(torch.tensor(target_parameters[<span class="dv">1</span>]))</span>
<span id="annotated-cell-14-15"><a href="#annotated-cell-14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-16"><a href="#annotated-cell-14-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model function initialized with (w,b) = (0,0)</span></span>
<span id="annotated-cell-14-17"><a href="#annotated-cell-14-17" aria-hidden="true" tabindex="-1"></a>    model_net <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="annotated-cell-14-18"><a href="#annotated-cell-14-18" aria-hidden="true" tabindex="-1"></a>    model_net.weight <span class="op">=</span> nn.Parameter(torch.tensor([[<span class="fl">0.0</span>]]))</span>
<span id="annotated-cell-14-19"><a href="#annotated-cell-14-19" aria-hidden="true" tabindex="-1"></a>    model_net.bias <span class="op">=</span> nn.Parameter(torch.tensor([<span class="fl">0.0</span>]))</span>
<span id="annotated-cell-14-20"><a href="#annotated-cell-14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-21"><a href="#annotated-cell-14-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(x, y):</span>
<span id="annotated-cell-14-22"><a href="#annotated-cell-14-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the loss gradient</span></span>
<span id="annotated-cell-14-23"><a href="#annotated-cell-14-23" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> model_net.forward(x)</span>
<span id="annotated-cell-14-24"><a href="#annotated-cell-14-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (prediction <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="annotated-cell-14-25"><a href="#annotated-cell-14-25" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="annotated-cell-14-26"><a href="#annotated-cell-14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-27"><a href="#annotated-cell-14-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># walk the parameters against gradient</span></span>
<span id="annotated-cell-14-28"><a href="#annotated-cell-14-28" aria-hidden="true" tabindex="-1"></a>        model_net.weight <span class="op">=</span> nn.Parameter(</span>
<span id="annotated-cell-14-29"><a href="#annotated-cell-14-29" aria-hidden="true" tabindex="-1"></a>            model_net.weight <span class="op">-</span> learning_rate <span class="op">*</span> model_net.weight.grad</span>
<span id="annotated-cell-14-30"><a href="#annotated-cell-14-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="annotated-cell-14-31"><a href="#annotated-cell-14-31" aria-hidden="true" tabindex="-1"></a>        model_net.bias <span class="op">=</span> nn.Parameter(model_net.bias <span class="op">-</span> learning_rate <span class="op">*</span> model_net.bias.grad)</span>
<span id="annotated-cell-14-32"><a href="#annotated-cell-14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-33"><a href="#annotated-cell-14-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># perform training steps and record parameter trajectory</span></span>
<span id="annotated-cell-14-34"><a href="#annotated-cell-14-34" aria-hidden="true" tabindex="-1"></a>    trajectory <span class="op">=</span> []</span>
<span id="annotated-cell-14-35"><a href="#annotated-cell-14-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="annotated-cell-14-36"><a href="#annotated-cell-14-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># record θ</span></span>
<span id="annotated-cell-14-37"><a href="#annotated-cell-14-37" aria-hidden="true" tabindex="-1"></a>        current_parameter <span class="op">=</span> (</span>
<span id="annotated-cell-14-38"><a href="#annotated-cell-14-38" aria-hidden="true" tabindex="-1"></a>            model_net.weight.data[<span class="dv">0</span>].item(),</span>
<span id="annotated-cell-14-39"><a href="#annotated-cell-14-39" aria-hidden="true" tabindex="-1"></a>            model_net.bias.data.data[<span class="dv">0</span>].item(),</span>
<span id="annotated-cell-14-40"><a href="#annotated-cell-14-40" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="annotated-cell-14-41"><a href="#annotated-cell-14-41" aria-hidden="true" tabindex="-1"></a>        trajectory.append(current_parameter)</span>
<span id="annotated-cell-14-42"><a href="#annotated-cell-14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-43"><a href="#annotated-cell-14-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># generate sample</span></span>
<span id="annotated-cell-14-44"><a href="#annotated-cell-14-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.rand((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="annotated-cell-14-45"><a href="#annotated-cell-14-45" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> target_function.forward(x)</span>
<span id="annotated-cell-14-46"><a href="#annotated-cell-14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-47"><a href="#annotated-cell-14-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># train model network</span></span>
<span id="annotated-cell-14-48"><a href="#annotated-cell-14-48" aria-hidden="true" tabindex="-1"></a>        train(x, y)</span>
<span id="annotated-cell-14-49"><a href="#annotated-cell-14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-50"><a href="#annotated-cell-14-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectory</span>
<span id="annotated-cell-14-51"><a href="#annotated-cell-14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-52"><a href="#annotated-cell-14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-53"><a href="#annotated-cell-14-53" aria-hidden="true" tabindex="-1"></a><span class="co"># execute SGD</span></span>
<span id="annotated-cell-14-54"><a href="#annotated-cell-14-54" aria-hidden="true" tabindex="-1"></a>target_parameters <span class="op">=</span> (<span class="fl">2.0</span>, <span class="fl">1.0</span>)</span>
<span id="annotated-cell-14-55"><a href="#annotated-cell-14-55" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> stochastic_gradient_descent(<span class="dv">200</span>, target_parameters, seed<span class="op">=</span><span class="dv">7</span>)</span>
<span id="annotated-cell-14-56"><a href="#annotated-cell-14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-57"><a href="#annotated-cell-14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-14-58"><a href="#annotated-cell-14-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"target parameters: </span><span class="sc">{</span>target_parameters<span class="sc">}</span><span class="ss">"</span>)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-14" data-target-annotation="2">2</button><span id="annotated-cell-14-59" class="code-annotation-target"><a href="#annotated-cell-14-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final θ = </span><span class="sc">{</span>trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-14" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-14" data-code-lines="8" data-code-annotation="1">it’s sometimes handy for debugging and demos to seed your random number generators (RNGs), as I’ve done here</span>
</dd>
<dt data-target-cell="annotated-cell-14" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-14" data-code-lines="59" data-code-annotation="2">in Python, <code>a_list[-1]</code> accesses the last element. Here, that’s our final parameter estimate</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>target parameters: (2.0, 1.0)
final θ = (1.9013594388961792, 1.0614407062530518)</code></pre>
</div>
</div>
<p>We can see that the result of stochastic gradient descent gets us quite close to the target parameters.</p>
<p>This simple recipe:</p>
<pre><code>1. sample data
2. compute loss
3. update parameters</code></pre>
<p>is one of the core routines behind training neural networks, even in far more complex learning systems. Of course, there should be some theory justifying how we sample data and define the loss function.</p>
<p>I want to push a little bit into this direction for our example - not too far, just enough to build some intuition. It might still be kind of abstract and not really palpable what is happening here.</p>
<p>Let’s see what stochastic gradient descent does on average. To find out, we compute the expected loss: <span class="math display">\[
\begin{split}
F(w,b) &amp;= \mathbb{E}_{X \sim U(0,1)}[L(w,b)] \\
&amp;= \mathbb{E}_{X\sim U(0,1)}\big[(wX + b - (2X +1))^2\big] \\
&amp;= \int_0^1 ((w-2)x + (b-1))^2 \mathrm{d}x\\
&amp;= \frac{(w-2)^2}{3} + (w-2)(b-1) + (b-1)^2
\end{split}
\]</span></p>
<p>and its gradient <span class="math display">\[
\nabla F(w,b) =
\begin{pmatrix}
\frac{2}{3}(w-2) + (b-1)\\
(w-2) + 2(b-1)
\end{pmatrix}
\]</span></p>
<p>Using this expected gradient removes the randomness - that’s why it’s called deterministic gradient descent. Of course, in practice, we don’t have access to this expectation.</p>
<p>We can write the deterministic GD update rule as: <span class="math display">\[
\begin{pmatrix}
w_{t+1}\\
b_{t+1}
\end{pmatrix}
=
\begin{pmatrix}
w_{t}\\
b_{t}
\end{pmatrix}
- \eta
\underbrace{
\begin{pmatrix}
\frac{2}{3} &amp; 1 \\
1 &amp; 2
\end{pmatrix}
}_{\mathbf{A}}
\left(
\underbrace{
\begin{pmatrix}
w_{t}\\
b_{t}
\end{pmatrix}}_{\mathbf{\theta}} -
\underbrace{\begin{pmatrix}
2\\
1
\end{pmatrix}}
_{\mathbf{\theta}_*}
\right)
\]</span></p>
<p>So, there is some theory behind this that this converges when we let <span class="math inline">\(\eta\)</span> become sufficiently small, but we are aiming for intuition here. Let’s just visualise the vector field defined by the update rule <span class="math inline">\(-\mathbf{A} (\theta - \theta_*)\)</span>, and see how both deterministic and stochastic GD move through parameter space.</p>
<div id="3b0fb9c0" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_trajectory(title, target_parameters, trajectory, label):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ── Settings ───────────────────────────────────</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    w_star, b_star <span class="op">=</span> target_parameters</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.array([[<span class="dv">2</span> <span class="op">/</span> <span class="dv">3</span>, <span class="fl">1.0</span>], [<span class="fl">1.0</span>, <span class="fl">2.0</span>]])  <span class="co"># Hessian of expected MSE</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    num_steps <span class="op">=</span> <span class="bu">len</span>(trajectory)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ── Build grid for vector field ───────────────</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    w_vals <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">15</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    b_vals <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">15</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    W, B <span class="op">=</span> np.meshgrid(w_vals, b_vals)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> np.zeros_like(W)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros_like(B)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute field:   (U,V) = - ∇F = -A·(θ−θ*)</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(W.shape[<span class="dv">0</span>]):</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(W.shape[<span class="dv">1</span>]):</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> np.array([W[i, j], B[i, j]])</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">=</span> A.dot(theta <span class="op">-</span> np.array([w_star, b_star]))</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>            U[i, j] <span class="op">=</span> <span class="op">-</span>grad[<span class="dv">0</span>]</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>            V[i, j] <span class="op">=</span> <span class="op">-</span>grad[<span class="dv">1</span>]</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ── Deterministic GD trajectory ──────────────</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    det_path <span class="op">=</span> [(<span class="fl">0.0</span>, <span class="fl">0.0</span>)]</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>        w, b <span class="op">=</span> det_path[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> A.dot(np.array([w, b]) <span class="op">-</span> np.array([w_star, b_star]))</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>        det_path.append((w <span class="op">-</span> lr <span class="op">*</span> grad[<span class="dv">0</span>], b <span class="op">-</span> lr <span class="op">*</span> grad[<span class="dv">1</span>]))</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    w_det, b_det <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>det_path)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># from PyTorch</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>    w_torch, b_torch <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>trajectory)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ── Plot vector field + paths ────────────────</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    plt.quiver(W, B, U, V, angles<span class="op">=</span><span class="st">"xy"</span>, scale_units<span class="op">=</span><span class="st">"xy"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    plt.plot(w_det, b_det, <span class="st">"o-"</span>, label<span class="op">=</span><span class="st">"deterministic GD"</span>)</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    plt.plot(w_torch, b_torch, <span class="st">".-"</span>, label<span class="op">=</span>label)</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    plt.scatter([w_star], [b_star], marker<span class="op">=</span><span class="st">"x"</span>, s<span class="op">=</span><span class="dv">100</span>, label<span class="op">=</span><span class="st">"Optimum"</span>)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>    plt.text(w_star, b_star, <span class="st">"  (2,1)"</span>, va<span class="op">=</span><span class="st">"bottom"</span>)</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Weight $w$"</span>)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Bias $b$"</span>)</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>plot_trajectory(</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"SGD and GD Trajectories and Gradient Vector Field (</span><span class="sc">{</span><span class="bu">len</span>(trajectory)<span class="sc">}</span><span class="ss"> steps)"</span>,</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>    target_parameters,</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>    trajectory,</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>    <span class="st">"stochastic GD"</span>,</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-deep-learning-with-pytorch_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s nice to see that even though the stochastic GD bumbles along the 2D-plane, it kind of follows the path of the deterministic GD.</p>
</section>
<section id="batches" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="batches"><span class="header-section-number">3.4.3</span> Batches</h3>
<p>So far we’ve been training on one sample at a time. In reality, neural networks are designed to consume batches of inputs - and PyTorch has been quietly reshaping our single inputs into 1-element batches form behind the scenes.</p>
<p>Instead of feeding tensors one by one, we can stack them and process them together:</p>
<div id="38b92b4a" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === batched input ===</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>model_net <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>model_net.weight <span class="op">=</span> nn.Parameter(torch.tensor([[<span class="fl">1.0</span>]]))</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>model_net.bias <span class="op">=</span> nn.Parameter(torch.tensor([<span class="fl">2.0</span>]))</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> torch.Tensor([[<span class="fl">1.0</span>], [<span class="fl">2.0</span>], [<span class="dv">3</span>]])</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>model_net.forward(batch)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([[3.],
        [4.],
        [5.]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>This forward call effectively maps the network across each row of the batch, giving the same result as looping over the elements.</p>
<p>More generally, if a network is designed for input shape <code>s_in</code> and returns outputs of shape <code>s_out</code>, then it actually consumes batches of inputs with shape <code>(b, s_in)</code> and return outputs with shape <code>(b, s_out)</code>.</p>
<p>So, how is PyTorch treating unbatched samples as batches? It’s essentially reshaping a single-element tensor like <code>[x]</code> into <code>[[x]]</code> - a (1,1)-shaped tensor. This can be done manually using .unsqueeze(0):</p>
<div id="d209daf1" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === adding extra axis ===</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>t.unsqueeze(<span class="dv">0</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([[1, 2]])</code></pre>
</div>
</div>
<p>This adds an extra axis of size 1. To remove such singleton dimensions, we can use .squeeze():</p>
<div id="00752346" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === removing axes of size 1 ==</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"before squeeze: </span><span class="sc">{</span>t<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> t.squeeze()</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"after squeeze: </span><span class="sc">{</span>t<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>before squeeze: torch.Size([2, 1, 2, 1])
after squeeze: torch.Size([2, 2])</code></pre>
</div>
</div>
<p>Back to gradient descent. Our original loss function for gradient descent also included batching: <span class="math display">\[
L(\theta) = \frac{1}{N} \sum_{i = 1}^N \ell(f(x_i);\theta, y_i),
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the mean of <span class="math inline">\(\ell\)</span> over the batch. This reduces the noisiness of updates.</p>
<p>Let’s modify our SGD to use batches:</p>
<div id="d3a2e145" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="annotated-cell-20"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-20-1"><a href="#annotated-cell-20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === batch GD ===</span></span>
<span id="annotated-cell-20-2"><a href="#annotated-cell-20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_gradient_descent(</span>
<span id="annotated-cell-20-3"><a href="#annotated-cell-20-3" aria-hidden="true" tabindex="-1"></a>    batch_size, steps, target_parameters, seed<span class="op">=</span><span class="dv">0</span></span>
<span id="annotated-cell-20-4"><a href="#annotated-cell-20-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">list</span>[Parameter]:</span>
<span id="annotated-cell-20-5"><a href="#annotated-cell-20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""performs batch GD for a specified number of steps"""</span></span>
<span id="annotated-cell-20-6"><a href="#annotated-cell-20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-7"><a href="#annotated-cell-20-7" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="annotated-cell-20-8"><a href="#annotated-cell-20-8" aria-hidden="true" tabindex="-1"></a>    step_size <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="annotated-cell-20-9"><a href="#annotated-cell-20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-10"><a href="#annotated-cell-20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># target function with given target_parameters</span></span>
<span id="annotated-cell-20-11"><a href="#annotated-cell-20-11" aria-hidden="true" tabindex="-1"></a>    target_function <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="annotated-cell-20-12"><a href="#annotated-cell-20-12" aria-hidden="true" tabindex="-1"></a>    target_function.weight <span class="op">=</span> nn.Parameter(torch.tensor([[target_parameters[<span class="dv">0</span>]]]))</span>
<span id="annotated-cell-20-13"><a href="#annotated-cell-20-13" aria-hidden="true" tabindex="-1"></a>    target_function.bias <span class="op">=</span> nn.Parameter(torch.tensor([target_parameters[<span class="dv">1</span>]]))</span>
<span id="annotated-cell-20-14"><a href="#annotated-cell-20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-15"><a href="#annotated-cell-20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model function initialized with (w,b) = (0,0)</span></span>
<span id="annotated-cell-20-16"><a href="#annotated-cell-20-16" aria-hidden="true" tabindex="-1"></a>    model_net <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="annotated-cell-20-17"><a href="#annotated-cell-20-17" aria-hidden="true" tabindex="-1"></a>    model_net.weight <span class="op">=</span> nn.Parameter(torch.tensor([[<span class="fl">0.0</span>]]))</span>
<span id="annotated-cell-20-18"><a href="#annotated-cell-20-18" aria-hidden="true" tabindex="-1"></a>    model_net.bias <span class="op">=</span> nn.Parameter(torch.tensor([<span class="fl">0.0</span>]))</span>
<span id="annotated-cell-20-19"><a href="#annotated-cell-20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-20"><a href="#annotated-cell-20-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(x_batch, y_batch):</span>
<span id="annotated-cell-20-21"><a href="#annotated-cell-20-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the mean loss gradient</span></span>
<span id="annotated-cell-20-22"><a href="#annotated-cell-20-22" aria-hidden="true" tabindex="-1"></a>        prediction_batch <span class="op">=</span> model_net.forward(x_batch)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-20" data-target-annotation="1">1</button><span id="annotated-cell-20-23" class="code-annotation-target"><a href="#annotated-cell-20-23" aria-hidden="true" tabindex="-1"></a>        mean_loss <span class="op">=</span> (prediction_batch <span class="op">-</span> y_batch).<span class="bu">pow</span>(<span class="dv">2</span>).mean()</span>
<span id="annotated-cell-20-24"><a href="#annotated-cell-20-24" aria-hidden="true" tabindex="-1"></a>        mean_loss.backward()</span>
<span id="annotated-cell-20-25"><a href="#annotated-cell-20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-26"><a href="#annotated-cell-20-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># walk the parameters against gradient</span></span>
<span id="annotated-cell-20-27"><a href="#annotated-cell-20-27" aria-hidden="true" tabindex="-1"></a>        model_net.weight <span class="op">=</span> nn.Parameter(</span>
<span id="annotated-cell-20-28"><a href="#annotated-cell-20-28" aria-hidden="true" tabindex="-1"></a>            model_net.weight <span class="op">-</span> step_size <span class="op">*</span> model_net.weight.grad</span>
<span id="annotated-cell-20-29"><a href="#annotated-cell-20-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="annotated-cell-20-30"><a href="#annotated-cell-20-30" aria-hidden="true" tabindex="-1"></a>        model_net.bias <span class="op">=</span> nn.Parameter(model_net.bias <span class="op">-</span> step_size <span class="op">*</span> model_net.bias.grad)</span>
<span id="annotated-cell-20-31"><a href="#annotated-cell-20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-32"><a href="#annotated-cell-20-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># perform training steps and record parameter trajectory</span></span>
<span id="annotated-cell-20-33"><a href="#annotated-cell-20-33" aria-hidden="true" tabindex="-1"></a>    trajectory <span class="op">=</span> []</span>
<span id="annotated-cell-20-34"><a href="#annotated-cell-20-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="annotated-cell-20-35"><a href="#annotated-cell-20-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># record θ</span></span>
<span id="annotated-cell-20-36"><a href="#annotated-cell-20-36" aria-hidden="true" tabindex="-1"></a>        current_parameter <span class="op">=</span> (</span>
<span id="annotated-cell-20-37"><a href="#annotated-cell-20-37" aria-hidden="true" tabindex="-1"></a>            model_net.weight.data[<span class="dv">0</span>].item(),</span>
<span id="annotated-cell-20-38"><a href="#annotated-cell-20-38" aria-hidden="true" tabindex="-1"></a>            model_net.bias.data.data[<span class="dv">0</span>].item(),</span>
<span id="annotated-cell-20-39"><a href="#annotated-cell-20-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="annotated-cell-20-40"><a href="#annotated-cell-20-40" aria-hidden="true" tabindex="-1"></a>        trajectory.append(current_parameter)</span>
<span id="annotated-cell-20-41"><a href="#annotated-cell-20-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-42"><a href="#annotated-cell-20-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># generate batch sample</span></span>
<span id="annotated-cell-20-43"><a href="#annotated-cell-20-43" aria-hidden="true" tabindex="-1"></a>        x_batch <span class="op">=</span> torch.rand((batch_size, <span class="dv">1</span>))</span>
<span id="annotated-cell-20-44"><a href="#annotated-cell-20-44" aria-hidden="true" tabindex="-1"></a>        y_batch <span class="op">=</span> target_function.forward(x_batch)</span>
<span id="annotated-cell-20-45"><a href="#annotated-cell-20-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-46"><a href="#annotated-cell-20-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># train model network</span></span>
<span id="annotated-cell-20-47"><a href="#annotated-cell-20-47" aria-hidden="true" tabindex="-1"></a>        train(x_batch, y_batch)</span>
<span id="annotated-cell-20-48"><a href="#annotated-cell-20-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-49"><a href="#annotated-cell-20-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trajectory</span>
<span id="annotated-cell-20-50"><a href="#annotated-cell-20-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-51"><a href="#annotated-cell-20-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-52"><a href="#annotated-cell-20-52" aria-hidden="true" tabindex="-1"></a><span class="co"># execute batch GD</span></span>
<span id="annotated-cell-20-53"><a href="#annotated-cell-20-53" aria-hidden="true" tabindex="-1"></a>target_parameters <span class="op">=</span> (<span class="fl">2.0</span>, <span class="fl">1.0</span>)</span>
<span id="annotated-cell-20-54"><a href="#annotated-cell-20-54" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> batch_gradient_descent(<span class="dv">20</span>, <span class="dv">200</span>, target_parameters)</span>
<span id="annotated-cell-20-55"><a href="#annotated-cell-20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-56"><a href="#annotated-cell-20-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-20-57"><a href="#annotated-cell-20-57" aria-hidden="true" tabindex="-1"></a>plot_trajectory(</span>
<span id="annotated-cell-20-58"><a href="#annotated-cell-20-58" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"SGD and batch GD Trajectories and Gradient Vector Field (</span><span class="sc">{</span><span class="bu">len</span>(trajectory)<span class="sc">}</span><span class="ss"> steps)"</span>,</span>
<span id="annotated-cell-20-59"><a href="#annotated-cell-20-59" aria-hidden="true" tabindex="-1"></a>    target_parameters,</span>
<span id="annotated-cell-20-60"><a href="#annotated-cell-20-60" aria-hidden="true" tabindex="-1"></a>    trajectory,</span>
<span id="annotated-cell-20-61"><a href="#annotated-cell-20-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch GD"</span>,</span>
<span id="annotated-cell-20-62"><a href="#annotated-cell-20-62" aria-hidden="true" tabindex="-1"></a>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-20" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-20" data-code-lines="23" data-code-annotation="1">the code for computing the squared errors is identical to the single-sample version (except the mean at the end). Many scalar operations naturally extend to tensors</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-deep-learning-with-pytorch_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The batched version stays closer to the deterministic path. That’s expected - averaging reduces the variance. However, we’ve now done 200 steps with batches of 20, so in effect, we’ve increased the amount of data processed by a factor of 20.</p>
<p>But are we paying for that in compute? Let’s benchmark:</p>
<div id="f408cb9f" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statistics</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_experiment(</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    gradient_descent_type, <span class="op">*</span>args, target_parameters<span class="op">=</span>(<span class="fl">2.0</span>, <span class="fl">1.0</span>), iterations<span class="op">=</span><span class="dv">20</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">    times performance and evaluates accuracy for stochastic or batch GD.</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> [</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        gradient_descent_type(<span class="op">*</span>args, target_parameters, seed<span class="op">=</span>i)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    elapsed_time <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    squared_errors <span class="op">=</span> [</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>        (w <span class="op">-</span> target_parameters[<span class="dv">0</span>]) <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> (b <span class="op">-</span> target_parameters[<span class="dv">1</span>]) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (w, b) <span class="kw">in</span> results</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    mean_squared_error <span class="op">=</span> statistics.mean(squared_errors)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"time: </span><span class="sc">{</span>elapsed_time<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mean squared error: </span><span class="sc">{</span>mean_squared_error<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"=== Comparison between SGD and batch GD ==="</span>)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss"> steps averaged over </span><span class="sc">{</span>iterations<span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Run plain stochastic gradient descent (200 steps total)</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- stochastic GD ---"</span>)</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>run_experiment(stochastic_gradient_descent, <span class="dv">200</span>)</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Run batched gradient descent (20 batches of 200 steps total)</span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- batch GD (batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">) ---"</span>)</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>run_experiment(batch_gradient_descent, <span class="dv">20</span>, <span class="dv">200</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>=== Comparison between SGD and batch GD ===
200 steps averaged over 20 iterations

--- stochastic GD ---
time: 0.46 seconds
mean squared error: 0.0094

--- batch GD (batch size: 20) ---
time: 0.51 seconds
mean squared error: 0.0086</code></pre>
</div>
</div>
<p>The non-batched version runs faster, but not 20× faster. Maybe 10% or so. On the other hand, the batched version gives a lower mean squared error.</p>
<p>Choosing a good batch size is part of what’s known as hyperparameter tuning.</p>
</section>
</section>
<section id="gradients" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="gradients"><span class="header-section-number">3.5</span> Gradients</h2>
<p>Alright, it’s time to see how PyTorch computes all the gradients we’ve been happily using in gradient descent. Let’s look at a simple example that performs some operations on a scalar tensor and computes its gradient (more precisely, its derivative in this case):</p>
<div id="3c6f40f2" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === calculating derivative ===</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(<span class="fl">3.0</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>a.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> a.<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> b <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>c.backward()</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>a.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(30.)</code></pre>
</div>
</div>
<p>We needed to set <code>requires_grad = True</code> so that PyTorch would track the gradient of <code>a</code>. We haven’t seen this before because for network parameters (<code>nn.Parameter</code>), it’s automatically set to <code>True</code>.</p>
<section id="why-bother" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="why-bother"><span class="header-section-number">3.5.1</span> Why Bother</h3>
<p>Before diving deeper, I’ll admit I wasn’t originally keen on writing this. It sounded technical, and the buzzwordy explanation that ‘PyTorch backpropagates gradients using the chain rule’ felt like enough to get a rough intuition. But after watching <span class="citation" data-cites="karpathy2016">Karpathy (<a href="#ref-karpathy2016" role="doc-biblioref">2016</a>)</span>, I found it really interesting. This presentation of PyTorch’s gradient mechanics is very much inspired by that video. The author also gives their own take on <a href="https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b">‘why bother learning what PyTorch does automatically for you’</a>.</p>
</section>
<section id="backpropagation" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="backpropagation"><span class="header-section-number">3.5.2</span> Backpropagation</h3>
<p>PyTorch uses backpropagation to compute gradients. We’ll unpack how this works using the simple example above. The computation has the structure:</p>
<div class="line-block"><span class="math inline">\(a \gets 3\)</span><br>
<span class="math inline">\(b \gets f(a)\)</span><br>
<span class="math inline">\(c \gets  g(b)\)</span></div>
<p>What we want is the derivative <span class="math inline">\((g \circ f)'(a)\)</span>, and we’ll compute it in a way that mirrors how backpropagation works. First, let’s visualise the computation of <span class="math inline">\(c\)</span> - known as the forward pass - using a circuit graph, where each function is a ‘gate’:</p>
<div class="text-center">
<img src="03-deep-learning-with-pytorch_files/mediabag/6e11977ab111471f6084e682821f895dc4e0b165.svg" class="img-fluid">
</div>
<p>Backpropagation works by computing derivatives in the reverse direction, using the chain rule: <span class="math display">\[
(g \circ f)'(a) = g'(f(a))\; f'(a).
\]</span></p>
<p>Or, in Leibniz notation and using <span class="math inline">\(b = f(a)\)</span> and <span class="math inline">\(c = g(b)\)</span>: <span class="math display">\[
\frac{\partial c}{\partial a} = \frac{\partial c}{\partial b} \;\frac{\partial b}{\partial a}.
\]</span></p>
<p>To compute <span class="math inline">\(\frac{\partial c}{\partial a}\)</span>, each gate uses the ‘local’ derivatives of its forward pass function <span class="math inline">\(f'(a) = \frac{\partial b}{\partial a}\)</span> and <span class="math inline">\(g'(b) = \frac{\partial c}{\partial b}\)</span>, and the process starts with the identity derivative <span class="math inline">\(\frac{\partial c}{\partial c} = 1\)</span>. The backward pass then has this structure:</p>
<div class="line-block"><span class="math inline">\(\frac{\partial c}{\partial c} \gets 1\)</span><br>
<span class="math inline">\(\frac{\partial c}{\partial b} \gets \frac{\partial c}{\partial c} \cdot g'(b)\)</span><br>
<span class="math inline">\(\frac{\partial c}{\partial a} \gets \frac{\partial c}{\partial b} \cdot f'(a)\)</span></div>
<p>Each step follows from the chain rule. This is how we traverse the computation graph in reverse - and we can now augment the forward pass diagram with the backward pass:</p>
<div class="text-center">
<img src="03-deep-learning-with-pytorch_files/mediabag/7ffcb6879ffa650b5f7c14469ced78307c93ecd8.svg" class="img-fluid">
</div>
<p><br>
</p>
<p>That’s, in a nutshell, how backpropagation works - and how PyTorch computes gradients.</p>
<p>Finally, let’s verify that this produces the same result of <span class="math inline">\(30\)</span> as in our original example. For <span class="math inline">\(f(a) = a^2\)</span>, <span class="math inline">\(g(b) = 5b\)</span> with <span class="math inline">\(f'(a) = 2a\)</span>, <span class="math inline">\(g'(b) = 5\)</span>, we get:</p>
<div class="text-center">
<img src="03-deep-learning-with-pytorch_files/mediabag/567b6ab3bbc946aac4cd243837eaa8f1f1007095.svg" class="img-fluid">
</div>
<p><br>
</p>
<p>In practice, it’s a bit more complicated of course. For example, we have talked about derivatives but we need to compute gradients - that is, partial derivatives with respect to multiple variables. For example, if we have a function <span class="math inline">\(f(x, y)\)</span>, with gradient <span class="math inline">\(\nabla f = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial x})\)</span> the backward pass through its gate looks like this (forward pass not shown).</p>
<div class="text-center">
<img src="03-deep-learning-with-pytorch_files/mediabag/1df0548df11b6506b6cdb6ce0eb4a48bda0996a6.svg" class="img-fluid">
</div>
<p>In general, each gate in the backward pass receives an upstream gradient, multiplies it by its local gradients, and sends the results downstream.</p>
</section>
<section id="grad_fn" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="grad_fn"><span class="header-section-number">3.5.3</span> grad_fn</h3>
<p>For PyTorch to find it’s way ‘back’ in the backpropagation under the hood, it dynamically builds a computational graph during the forward pass, storing the sequence of operations. Also, each non-leaf tensor stores a reference to the local gradient functions in the <code>grad_fn</code> attribute.</p>
<p>When we call <code>.backward()</code>, PyTorch traverses that graph in reverse order, applying those gradient functions to propagate gradients back to the leaves, just as we’ve discussed.</p>
<p>For example, when we compute <code>x + y</code>, the resulting tensor stores <code>grad_fn = &lt;AddBackward0&gt;</code> - the ‘backward’ function that computes the gradient of the addition:</p>
<div id="ed14d38b" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === the grad_fn attribute ===</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">+</span> y</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor(3., grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>We haven’t called <code>.backward()</code> here, so no gradients have been computed yet. But PyTorch has already set up the computation graph, so it’s ready to go as soon as one needs gradients with respect to <code>x</code> and <code>y</code>.</p>
</section>
<section id="gradients-and-control-structures" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="gradients-and-control-structures"><span class="header-section-number">3.5.4</span> Gradients and Control Structures</h3>
<p>Because backpropagation uses the computational graph, it handles control structures just fine - as long as operations are traceable.</p>
<p>Here’s an example where we compute <span class="math inline">\(b = 10 \cdot a\)</span> using a loop. PyTorch still gets the correct gradient for <span class="math inline">\(\frac{\partial b}{\partial a}\)</span>:</p>
<div id="8cb43177" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === gradients calculation and control structures ===</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    b <span class="op">+=</span> a</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>b.backward()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"∂b/∂a: </span><span class="sc">{</span>a<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>∂b/∂a: 10.0</code></pre>
</div>
</div>
</section>
<section id="sec-viewBackward0" class="level3" data-number="3.5.5">
<h3 data-number="3.5.5" class="anchored" data-anchor-id="sec-viewBackward0"><span class="header-section-number">3.5.5</span> The mysterious of the viewBackward0</h3>
<p>Now that we understand backpropagation and how neural networks process batches of inputs, we can revisit something we encountered earlier when introducing linear neural networks.</p>
<p>When we applied a linear neural network to some date we saw the attribute <code>grad_fn=&lt;ViewBackward0&gt;</code> popping up, like here</p>
<div id="2df4d520" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="annotated-cell-25"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-25-1"><a href="#annotated-cell-25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === unbatched input ===</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-25" data-target-annotation="1">1</button><span id="annotated-cell-25-2" class="code-annotation-target"><a href="#annotated-cell-25-2" aria-hidden="true" tabindex="-1"></a>linear_nn <span class="op">=</span> nn.Linear(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="annotated-cell-25-3"><a href="#annotated-cell-25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-25-4"><a href="#annotated-cell-25-4" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="annotated-cell-25-5"><a href="#annotated-cell-25-5" aria-hidden="true" tabindex="-1"></a>linear_nn.forward(v)  <span class="co"># grad_fn=&lt;ViewBackward0&gt;</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-25" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-25" data-code-lines="2" data-code-annotation="1">if we initialize a network it’s parameters are randomly distributed</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([0.0356, 0.6774], grad_fn=&lt;ViewBackward0&gt;)</code></pre>
</div>
</div>
<p>So what’s going on?</p>
<p>We know that <code>grad_fn</code> stores the backward function of the final operation. In this case, apparently a view, i.e., an operation that returns a tensor on the same data but a different shape.</p>
<p>The code for handling linear layers is written in C++ and lies under <code>aten/src/ATen/native/Linear.cpp</code> in the PyTorch repository (at least, I think that’s the relevant code; I’m not 100% sure). The relevant part, which is called when the input is not a rank-2 tensor with some comments by me (I’m not 100% sure about the comments either)::</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="at">static</span> <span class="kw">inline</span> Tensor _flatten_nd_linear<span class="op">(</span><span class="at">const</span> Tensor<span class="op">&amp;</span> input<span class="op">,</span> <span class="at">const</span> Tensor<span class="op">&amp;</span> weight<span class="op">,</span> <span class="at">const</span> Tensor<span class="op">&amp;</span> bias<span class="op">)</span> <span class="op">{</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// get the sizes of the input tensor</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> input_sizes <span class="op">=</span> input<span class="op">.</span>sym_sizes<span class="op">();</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">// calculate the flattened rank size</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    c10<span class="op">::</span>SymInt flattened_dim <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int64_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">,</span> ndim <span class="op">=</span> input_sizes<span class="op">.</span>size<span class="op">();</span> i <span class="op">&lt;</span> ndim <span class="op">-</span> <span class="dv">1</span><span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        flattened_dim <span class="op">=</span> flattened_dim <span class="op">*</span> input_sizes<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">// reshape the input tensor to flatten all but the last axis</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> inp_reshape <span class="op">=</span> input<span class="op">.</span>reshape_symint<span class="op">({</span>flattened_dim<span class="op">,</span> input_sizes<span class="op">.</span>at<span class="op">(</span>input_sizes<span class="op">.</span>size<span class="op">()</span> <span class="op">-</span><span class="dv">1</span><span class="op">)});</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// perform the linear operation</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">const</span> <span class="kw">auto</span> result <span class="op">=</span> at<span class="op">::</span>addmm<span class="op">(</span>bias<span class="op">,</span> inp_reshape<span class="op">,</span> weight<span class="op">.</span>t<span class="op">());</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// calculate the new size of the output tensor</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">auto</span> new_size <span class="op">=</span> input_sizes<span class="op">.</span>slice<span class="op">(</span><span class="dv">0</span><span class="op">,</span> input_sizes<span class="op">.</span>size<span class="op">()</span> <span class="op">-</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    c10<span class="op">::</span>SymDimVector sizes_vec<span class="op">(</span>new_size<span class="op">.</span>begin<span class="op">(),</span> new_size<span class="op">.</span>end<span class="op">());</span></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>    sizes_vec<span class="op">.</span>push_back<span class="op">(</span>result<span class="op">.</span>sym_size<span class="op">(</span><span class="dv">1</span><span class="op">));</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">// reshape the output tensor to match the original input shape</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result<span class="op">.</span>view_symint<span class="op">(</span>sizes_vec<span class="op">);</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>So basically, the linear networks expect inputs of dimension 2. If we provide a tensor without a batch dimension, PyTorch internally reshapes it, creating a view with an extra axis of size 1. Then it runs the computation, and finally reshapes the output back to having no batches.</p>
<p>When we explicitly provide a batch, that internal reshaping isn’t necessary. The final computation is then matrix multiplication plus bias addition, whose gradient function shows up as grad_fn = <code>&lt;AddmmBackward0&gt;</code>:</p>
<div id="b423a85e" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === batched input ===</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>linear_nn <span class="op">=</span> nn.Linear(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>linear_nn.forward(v)  <span class="co"># grad_fn=&lt;AddmmBackward0&gt;</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor([[-0.1479, -0.1826]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="nn-building-blocks" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="nn-building-blocks"><span class="header-section-number">3.6</span> NN building blocks</h2>
<p>Until now, we have only worked with a single linear layer. We can connect layers so that they feed into each other using <code>nn.Sequential</code></p>
<div id="cfce1d90" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="annotated-cell-28"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-28-1"><a href="#annotated-cell-28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === combining networks ===</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-28" data-target-annotation="1">1</button><span id="annotated-cell-28-2" class="code-annotation-target"><a href="#annotated-cell-28-2" aria-hidden="true" tabindex="-1"></a>layer1 <span class="op">=</span> nn.Linear(<span class="dv">4</span>, <span class="dv">125</span>)</span>
<span id="annotated-cell-28-3"><a href="#annotated-cell-28-3" aria-hidden="true" tabindex="-1"></a>layer2 <span class="op">=</span> nn.Linear(<span class="dv">125</span>, <span class="dv">125</span>)</span>
<span id="annotated-cell-28-4"><a href="#annotated-cell-28-4" aria-hidden="true" tabindex="-1"></a>layer3 <span class="op">=</span> nn.Linear(<span class="dv">125</span>, <span class="dv">2</span>)</span>
<span id="annotated-cell-28-5"><a href="#annotated-cell-28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-28-6"><a href="#annotated-cell-28-6" aria-hidden="true" tabindex="-1"></a>sequential_nn <span class="op">=</span> nn.Sequential(layer1, layer2, layer3)</span>
<span id="annotated-cell-28-7"><a href="#annotated-cell-28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-28-8"><a href="#annotated-cell-28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-28-9"><a href="#annotated-cell-28-9" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="fl">4.0</span>]).unsqueeze(<span class="dv">0</span>)</span>
<span id="annotated-cell-28-10"><a href="#annotated-cell-28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-28-11"><a href="#annotated-cell-28-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying sequential_nn produces the same as...</span></span>
<span id="annotated-cell-28-12"><a href="#annotated-cell-28-12" aria-hidden="true" tabindex="-1"></a>r_seq <span class="op">=</span> sequential_nn(tensor)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-28" data-target-annotation="2">2</button><span id="annotated-cell-28-13" class="code-annotation-target"><a href="#annotated-cell-28-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"sequential_nn: </span><span class="sc">{</span>r_seq<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="annotated-cell-28-14"><a href="#annotated-cell-28-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ... applying the individual layers in order</span></span>
<span id="annotated-cell-28-15"><a href="#annotated-cell-28-15" aria-hidden="true" tabindex="-1"></a>r_con <span class="op">=</span> layer3(layer2(layer1(tensor)))</span>
<span id="annotated-cell-28-16"><a href="#annotated-cell-28-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"concatenated: </span><span class="sc">{</span>r_con<span class="sc">}</span><span class="ss">"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-28" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-28" data-code-lines="2,4" data-code-annotation="1">We have to make sure manually that our layers fit together. Otherwise, we might get something like this: <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x126 and 125x2)</code></span>
</dd>
<dt data-target-cell="annotated-cell-28" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-28" data-code-lines="13,16" data-code-annotation="2">I haven’t used the <code>forward</code> function for the input but just applied the layer to the input directly. This is possible, because PyTorch’s neural networks are callable objects, which just invoke the the forward function</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>sequential_nn: tensor([[ 0.4786, -0.5196]], grad_fn=&lt;AddmmBackward0&gt;)
concatenated: tensor([[ 0.4786, -0.5196]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<p>Combining three linear layers doesn’t make much sense actually, as it is essentially multiplying three matrices together, resulting in a single matrix. Therefore, in our example, we could have simply used a single layer with 4 inputs and 2 outputs.</p>
<p>With linear layers, we are essentially limited to affine linear transformations. [Very technically, this is not entirely true. I recall seeing a YouTube video where only linear layers were used, and the non-linearity came from floating-point imprecision. However, I can’t find it.]</p>
<section id="relu" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="relu"><span class="header-section-number">3.6.1</span> ReLU</h3>
<p>To move beyond the linear world, we need to add some non-linear layers. A frequently used function used in layers is the ‘Rectified Linear Unit’ (ReLU), which is defined by this simple function: <span class="math display">\[
\mathrm{ReLU}(x) = \max(0,x).
\]</span></p>
<p>and get’s applied to each input.</p>
<p>In this example we can see that the third component gets clipped to 0.</p>
<div id="d7672498" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === ReLU clips input ===</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> nn.ReLU()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>layer(torch.tensor([<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]).unsqueeze(<span class="dv">0</span>))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([[1, 0, 0]])</code></pre>
</div>
</div>
<p>So, if we are using a network like this, it cannot be simplified to just one layer:</p>
<div id="7f5fc48f" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="annotated-cell-30"><pre class="sourceCode python code-annotation-code code-annotated"><code class="sourceCode python"><span id="annotated-cell-30-1"><a href="#annotated-cell-30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># === combining linear with relu ===</span></span>
<span id="annotated-cell-30-2"><a href="#annotated-cell-30-2" aria-hidden="true" tabindex="-1"></a>sequential_nn <span class="op">=</span> nn.Sequential(</span>
<span id="annotated-cell-30-3"><a href="#annotated-cell-30-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">6</span>, <span class="dv">125</span>), nn.ReLU(), nn.Linear(<span class="dv">125</span>, <span class="dv">125</span>), nn.ReLU(), nn.Linear(<span class="dv">125</span>, <span class="dv">2</span>)</span>
<span id="annotated-cell-30-4"><a href="#annotated-cell-30-4" aria-hidden="true" tabindex="-1"></a>)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-30" data-target-annotation="1">1</button><span id="annotated-cell-30-5" class="code-annotation-target"><a href="#annotated-cell-30-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sequential_nn)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-30" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-30" data-code-lines="5" data-code-annotation="1">PyTorch provides us with a quick overview of a neural network whenever we print it.</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sequential(
  (0): Linear(in_features=6, out_features=125, bias=True)
  (1): ReLU()
  (2): Linear(in_features=125, out_features=125, bias=True)
  (3): ReLU()
  (4): Linear(in_features=125, out_features=2, bias=True)
)</code></pre>
</div>
</div>
<p>We will a network like this to solve the CartPole environment.</p>
</section>
</section>
<section id="example---gan-on-atari-images" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="example---gan-on-atari-images"><span class="header-section-number">3.7</span> Example - GAN on Atari images</h2>
<p>This final example of in <span class="citation" data-cites="lappan2024">(<a href="#ref-lappan2024" role="doc-biblioref">Lapan 2024, chap. 3</a>)</span> is quite big and might look daunting. Although it may look intimidating initially, nearly half of it consists of neural network declarations, but there’s still plenty of new stuff. For starters, simply running the code, viewing the results in tensorboard, and identifying familiar elements from this chapter should be enough.</p>
<p>I had to ‘repair’ the code, so it would run for me. You can that code under <code>/code/chapter_03/03_atari_gan.py</code>.</p>
<section id="running" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="running"><span class="header-section-number">3.7.1</span> Running</h3>
<p>I use my IDE to run the code, but you can also use the command:</p>
<pre class="shell"><code>python3 code/chapter_03/03_atari_gan.py</code></pre>
<p>You should see output similar to this in the console:</p>
<pre class="shell"><code>A.L.E: Arcade Learning Environment (version 0.11.0+dfae0bd)
[Powered by Stella]
INFO:__main__:Iter 100 in 32.86s: gen_loss=5.530e+00, dis_loss=5.460e-02
INFO:__main__:Iter 200 in 33.80s: gen_loss=7.217e+00, dis_loss=5.016e-03</code></pre>
<p>The actual data produced will be saved in the <code>/runs</code> folder and can be viewed using TensorBoard.</p>
</section>
<section id="viewing-the-data" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="viewing-the-data"><span class="header-section-number">3.7.2</span> Viewing the Data</h3>
<p>If TensorBoard is installed (it should if you installed the requirements.txt), you can run:</p>
<pre class="shell"><code>tensorboard --logdir runs</code></pre>
<p>You should see something like this in the console (don’t worry about the reduced feature set message):</p>
<pre class="shell"><code>TensorFlow installation not found - running with reduced feature set.

NOTE: Using experimental fast data loading logic. To disable, pass
    "--load_fast=false" and report issues on GitHub. More details:
    https://github.com/tensorflow/tensorboard/issues/4784

Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)</code></pre>
<p>Clicking the link will open TensorBoard in your browser.</p>
<p>It liked watching the data flow into TensorBoard, observing the losses of the discriminator and generator fluctuate, and seeing the latest ‘phases’ the generator went through.</p>
<p>Here are some of the final images created by the generator in my run (yes, they are generated at that size):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/atari_gan_fake.png" class="img-fluid figure-img"></p>
<figcaption>Sample images seen in tensorboard crated by of <code>03_atari_gan.py</code></figcaption>
</figure>
</div>
</section>
<section id="discussing-the-code" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="discussing-the-code"><span class="header-section-number">3.7.3</span> Discussing the code</h3>
<p>The code defines two classes, Discriminator and Generator, which are the neural networks being trained. The Discriminator is presented with images from Atari games and images produced by the Generator. It is trained to distinguish between real and fake images. The Generator, on the other hand, creates images and is trained to convince the Discriminator that these images are real Atari game images.</p>
<p>Here’s a little code snippet that I copied from <code>03_atari_gan.py</code> with some comments. Just to show that it also follows the basic training recipe:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train generator</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>gen_optimizer.zero_grad() <span class="co"># gradients have to reset before computing new ones</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>dis_output_v <span class="op">=</span> net_discr(gen_output_v) <span class="co"># sample data</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>gen_loss_v <span class="op">=</span> objective(dis_output_v, true_labels_v) <span class="co"># compute loss</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>gen_loss_v.backward() <span class="co"># calculate gradient</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>gen_optimizer.step() <span class="co"># update parameters</span></span></code></pre></div>
<p>This snippet uses:</p>
<ul>
<li>Optimizer: This updates the parameters in the direction of the gradient.</li>
<li>Objective: This calculates the loss function.</li>
</ul>
<p>We will delve deeper into these in the next chapter.</p>
</section>
<section id="discriminator-vs-generator-loss" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4" class="anchored" data-anchor-id="discriminator-vs-generator-loss"><span class="header-section-number">3.7.4</span> Discriminator vs Generator Loss</h3>
<p>For fun, I’ve included the losses from my run here with some heavy smoothing to make the trends clearer. It seems that, in principle, when one loss goes up, the other goes down, which makes sense.</p>
<p>Note that the losses are on completely different scales. I think this means that the discriminator is quite confident about identifying fake images. However, the generator keeps using that tiny bit of uncertainty in the discriminator to improve its output quality.</p>
<div id="11c9265c" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.signal <span class="im">import</span> savgol_filter</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co"># adjust these paths to wherever you saved your CSVs</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>disc_csv <span class="op">=</span> <span class="st">"quarto/data/atari_gan_dis_loss.csv"</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>gen_csv <span class="op">=</span> <span class="st">"quarto/data/atari_gan_gen_loss.csv"</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>df_disc <span class="op">=</span> pd.read_csv(disc_csv)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>df_gen <span class="op">=</span> pd.read_csv(gen_csv)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="co"># smooth the losses using Savitzky-Golay filter</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>window_length <span class="op">=</span> <span class="dv">31</span>  <span class="co"># must be odd; larger = smoother</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>polyorder <span class="op">=</span> <span class="dv">3</span>  <span class="co"># polynomial order for the filter</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>df_disc[<span class="st">"Smoothed_Value"</span>] <span class="op">=</span> savgol_filter(</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    df_disc[<span class="st">"Value"</span>], window_length<span class="op">=</span>window_length, polyorder<span class="op">=</span>polyorder</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>df_gen[<span class="st">"Smoothed_Value"</span>] <span class="op">=</span> savgol_filter(</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    df_gen[<span class="st">"Value"</span>], window_length<span class="op">=</span>window_length, polyorder<span class="op">=</span>polyorder</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a><span class="co"># create plot</span></span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>fig, ax1 <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a><span class="co"># left y-axis: Discriminator loss</span></span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>ax1.plot(</span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>    df_disc[<span class="st">"Step"</span>],</span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>    df_disc[<span class="st">"Smoothed_Value"</span>],</span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"tab:blue"</span>,</span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Discriminator Loss"</span>,</span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">"Training Step"</span>)</span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">"Discriminator Loss"</span>, color<span class="op">=</span><span class="st">"tab:blue"</span>)</span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a>ax1.tick_params(axis<span class="op">=</span><span class="st">"y"</span>, labelcolor<span class="op">=</span><span class="st">"tab:blue"</span>)</span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a><span class="co"># right y-axis: Generator loss</span></span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> ax1.twinx()</span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a>ax2.plot(</span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a>    df_gen[<span class="st">"Step"</span>], df_gen[<span class="st">"Smoothed_Value"</span>], color<span class="op">=</span><span class="st">"tab:orange"</span>, label<span class="op">=</span><span class="st">"Generator Loss"</span></span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">"Generator Loss"</span>, color<span class="op">=</span><span class="st">"tab:orange"</span>)</span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a>ax2.tick_params(axis<span class="op">=</span><span class="st">"y"</span>, labelcolor<span class="op">=</span><span class="st">"tab:orange"</span>)</span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a><span class="co"># title and layout</span></span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"GAN Losses (Dual Axis)"</span>)</span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-deep-learning-with-pytorch_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-karpathy2016" class="csl-entry" role="listitem">
Karpathy, Andrej. 2016. <span>“CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1.”</span> <a href="https://www.youtube.com/watch?v=i94OvYb6noo">https://www.youtube.com/watch?v=i94OvYb6noo</a>.
</div>
<div id="ref-lappan2024" class="csl-entry" role="listitem">
Lapan, Maxim. 2024. <em>Deep Reinforcement Learning Hands-on</em>. 3rd ed. Packt Publishing.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            trigger: 'click',
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            positionFixed: true,
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../quarto/chapters/02-gymnasium.html" class="pagination-link" aria-label="OpenAI Gym API and Gymnasium">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">OpenAI Gym API and Gymnasium</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../quarto/chapters/04-the-cross-entropy-method.html" class="pagination-link" aria-label="The cross entropy method">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The cross entropy method</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/julxi/notes_for_deep_rl_hands_on/blob/main/LICENSE">
<p>© 2025 Julian Bitterlich · MIT License</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>
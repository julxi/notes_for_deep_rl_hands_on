{
  "hash": "871fdd6298fdbf89ffb7bf4f23779b43",
  "result": {
    "engine": "jupyter",
    "markdown": "# OpenAI Gym API and Gymnasium\n\n## Creating an environment {.unnumbered}\nThe best starting point for dealing with an environment in gymnasium is reading their documentation. As of now this is here https://gymnasium.farama.org/\n\nI really recommend looking at the documentation. It's much easier that to guess what this means:\n\n::: {#ab19d895 .cell execution_count=2}\n``` {.python .cell-code}\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\n```\n:::\n\n\nBut let's look at it anyway.\nFirst the action space. It says we have two discrete actions, which usually start at `0`. So for cartpole we have `{0,1}` as actions. (for the meaning we need to check the documentation)\n\n::: {#f90b3158 .cell execution_count=3}\n``` {.python .cell-code}\nenv.action_space\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nDiscrete(2)\n```\n:::\n:::\n\n\nThe observation space is a bit complexer.\nWhen we check the shape we see that it has 4 observations\n\n::: {#9c2864a6 .cell execution_count=4}\n``` {.python .cell-code}\nobs_space = env.observation_space\nobs_space.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n(4,)\n```\n:::\n:::\n\n\nand printing the obs_space itself we see that it is a box (cartesian product) and the first list gives the lowest values of each component and the second the highest possible values.\n\n::: {#5457ccb7 .cell execution_count=5}\n``` {.python .cell-code}\nobs_space\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nBox([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n```\n:::\n:::\n\n\n## The random CartPole agent\n\nThe fist tiny step towards building our real agents. The code is so simple that I will just write it down here, and not in an in the chapter folder.\n\nThe idea behind random agent is: agent does random actions.\n(I will always leave some comments about the code after including it)\n\n::: {#2fd5ebe0 .cell execution_count=6}\n``` {.python .cell-code}\nfrom typing import Tuple\nimport gymnasium as gym\n\n\ndef run_random_episode(env: gym.Env) -> Tuple[float, int]:\n    env.reset()\n    total_reward = 0.0\n    done = False\n\n    while not done:\n        action = env.action_space.sample()\n        _, reward, terminated, truncated, _ = env.step(action)\n        total_reward += reward\n        done = terminated or truncated\n\n    return total_reward\n```\n:::\n\n\n::: {.callout-note}\n\n- read https://gymnasium.farama.org/introduction/basic_usage/ for a good introduction.\n- normally we would record the starting state of the environment when resetting like this `state, _ = env.reset()`. Since random actions don't need a state, we don't do it here. The same is true for `env.step(action)` the first return the state is normally important.\n- `env.action_space.sample()` produces a sample (random) action for the current state of the environment\n- episodes can be terminated, i.e., finished because of the episodic nature of the environment or truncated, i.e., finished because of a time out, basically. For \"CartPole-v1\" the timeout is 500 steps.\n\n:::\n    \nLet's see how far the random agent gets:\n\n::: {#89d7f0bc .cell execution_count=7}\n``` {.python .cell-code}\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nepisodes = 5\nfor episode in range(1, episodes + 1):\n    print(f\"\\n=== Episode {episode}/{episodes} ===\")\n    reward = run_random_episode(env)\n    print(f\"Reward: {reward}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Episode 1/5 ===\nReward: 9.0\n\n=== Episode 2/5 ===\nReward: 16.0\n\n=== Episode 3/5 ===\nReward: 30.0\n\n=== Episode 4/5 ===\nReward: 19.0\n\n=== Episode 5/5 ===\nReward: 32.0\n```\n:::\n:::\n\n\nThe reward of each episode reflects the number of steps for each episode since the reward structure is +1 for each step taken.\nOur goal now is to improve these rewards with reinforcement learning techniques.\n\n",
    "supporting": [
      "02-gymnasium_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "b6ce7e0e5914dee958f820a24fd11fdf",
  "result": {
    "engine": "jupyter",
    "markdown": "# Deep Learning with PyTorch\n\nPytorch is a library for deep learning. We will discuss the most important features now and everything else we need later on the way.\n\nWe can import it by\n\n::: {#f0801868 .cell execution_count=2}\n``` {.python .cell-code}\n# === importing pyTorck ===\nimport torch\n```\n:::\n\n\n## Tensors\n\nThe word \"Tensor\" is a bit overloaded in its meaning. In pyTorch it simply means multidimensional array and it serves as the basic units of computation.\n\nWe turn a python list into a 1-dimensional tensor (a.k.a. a vector)\n\n::: {#f4f28491 .cell execution_count=3}\n``` {.python .cell-code}\n# === a vector in pyTorch ===\ntorch.tensor([[1,2,3]])\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\ntensor([[1, 2, 3]])\n```\n:::\n:::\n\n\nThe term \"dimension\" can be a bit confusing. Let me try to clarify it. What it not is, is \"dimension\" when we say \"a 3-dimensional vector\", i.e., the amount of the degrees of freedom we can choose such a vector. What it means instead is the number of indices we need to get down to the \"field\"-level. For example for a 1D tensor (vector) `v` we need one index `v[i]` to access it's field components for a 2D tensor (matrix) `m` we need two indices `m[i][j]`.\nWe can also see the dimensionality of a tensor as a discrete space and the dimension says how many degrees of freedom we have to go to neighbouring cells.\n\nHere are some more ways to define tensors in pyTorch, we always use the same size (3,2) for the tensors.\n\n::: {#ffb26548 .cell execution_count=4}\n``` {.python .cell-code}\n# === some elemental ways to generate tensors ===\nprint(\"\\n a 2D-tensor full of 0s\")\nprint(torch.zeros((3, 2)))\n\nprint(\"\\n a 2D-tensor full of 7s\")\nprint(torch.full((3, 2), 7))\n\nprint(\"\\n a 2D-tensor randomly filled with [0-9]\")\nprint(torch.randint(0, 10, (3, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n a 2D-tensor full of 0s\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\n a 2D-tensor full of 7s\ntensor([[7, 7],\n        [7, 7],\n        [7, 7]])\n\n a 2D-tensor randomly filled with [0-9]\ntensor([[9, 4],\n        [7, 7],\n        [1, 1]])\n```\n:::\n:::\n\n\nNote that the length of the tuple $(3,2)$ defines the dimension. If we want to talk about how \"long\" each dimension we are talking about tensor's shape (alias for size), in our example that is (3,2). The first dimension has size 3 and the second has size 2.\n\n::: {#d243b2a9 .cell execution_count=5}\n``` {.python .cell-code}\n# === the shape of a tensor ===\nt = torch.zeros((3,2))\nprint(t)\nprint(t.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\ntorch.Size([3, 2])\n```\n:::\n:::\n\n\nIn PyTorch, you can combine multiple d-dimensional tensors into one (d+1)-dimensional, which creates a fresh first dimension.\n\n::: {#661e4e39 .cell execution_count=6}\n``` {.python .cell-code}\n# === stacking tensors ===\n# Create three 2D tensors\ntensor1 = torch.tensor([[1, 2], [3, 4]])\ntensor2 = torch.tensor([[5, 6], [7, 8]])\ntensor3 = torch.tensor([[9, 10], [11, 12]])\n\n# Stack the tensors along a new dimension\nstacked_tensor = torch.stack((tensor1, tensor2, tensor3))\nstacked_tensor\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\ntensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 5,  6],\n         [ 7,  8]],\n\n        [[ 9, 10],\n         [11, 12]]])\n```\n:::\n:::\n\n\nSo to get the 2nd tensor back we can just \"slice of\" the second element of the stacked tensor\n\n::: {#c8f6c7b8 .cell execution_count=7}\n``` {.python .cell-code}\n# === slicing tensors ===\nstacked_tensor[1]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\ntensor([[5, 6],\n        [7, 8]])\n```\n:::\n:::\n\n\nJust to make this clear about the indexing of the elements of a tensor, I have provided a tensor of shape (2,3,4) and each cell contains a number describing it's mathematical index as an integer.\n\n::: {#7d8fb635 .cell execution_count=8}\n``` {.python .cell-code}\n# === index positions of tensors ===\n# each entry in this tensor shows it's coordinate in mathematical notation\nt = torch.tensor(\n    [\n        [[111, 112, 113, 114], [121, 122, 123, 124], [131, 132, 133, 134]],\n        [[211, 212, 213, 214], [221, 222, 223, 224], [231, 232, 233, 234]],\n    ]\n)\nprint(t)\nprint(f\"The element at mathematical position (2,3,4) is: {t[1][2][3]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[[111, 112, 113, 114],\n         [121, 122, 123, 124],\n         [131, 132, 133, 134]],\n\n        [[211, 212, 213, 214],\n         [221, 222, 223, 224],\n         [231, 232, 233, 234]]])\nThe element at mathematical position (2,3,4) is: 234\n```\n:::\n:::\n\n\nThat's it for now about tensors. There is quite a bunch of more interesting stuff to show about them which we will discuss soon when we really need them.\n\n## Linear neural nets\n\nWe'll get to know the simplest neural nets. Linear neural nets. But along the way we discuss sme stuff that is important for all neural nets.\n\nFor working with neural nets in general we need to import\n\n::: {#f6d33585 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch.nn as nn\n```\n:::\n\n\nA linear neural net, has a bunch of input and output nodes and forwards each signal coming to an input node to an output node multiplied with the weight of that connection. Additionally each output node has a constant bias applied to it. \n<!-- this description is not really understandable if you don't know them already -->\n\nLet's look at a little example.\n\n::: {#98937a4e .cell execution_count=10}\n``` {.python .cell-code}\n# === manually setting parameters ===\n# Create a linear layer\nlinear_nn = nn.Linear(3, 2)\n\n# Manually set the weights\nweights = torch.tensor([[1.0, 0.0, 1.0], [0.0, 1.0, -1.0]])\n\n# Manually set the bias\nbias = torch.tensor([2.0, -3.0])\n\n# Assign the weights and bias to the linear layer\nlinear_nn.weight = nn.Parameter(weights)\nlinear_nn.bias = nn.Parameter(bias)\n```\n:::\n\n\n::: {.callout-note}\n- we had to specify the tensor's elements as floats, otherwise we would have gotten an error in creating the parameters\n- a parameter is a special kind of tensor that get special handling that we will discuss later\n<!-- is this all accurate? -->\n:::\n\nWhen we apply this network to to a vector $(x,y,z)$ we basically get\n$$\n\\mathrm{linear\\_nn}(x,y,z) = \n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y \\\\ z\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ -3\n\\end{pmatrix}\n$$\n\nFor example for the input $(1,0,0)$ we should get $(3,-3)$. Let's see if it's true\n\n::: {#903c88b3 .cell execution_count=11}\n``` {.python .cell-code}\nv = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntensor([ 3., -3.], grad_fn=<ViewBackward0>)\n```\n:::\n:::\n\n\nBasically we get it, except that we also get some extra information `grad_fn=<ViewBackward0>`. This reference to a gradient node. And to uncover this mystery we have to talk about gradients in pyTorch in general first. And before we do even that, let's see why we need them real quick.\n\n## Stochastic gradient descent\n\nAt its core, training a neural network means finding model parameters θ (weights and biases) that minimize a loss function\n$$\nL(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f(x_i;\\theta), y_i),\n$$\nwhere $\\ell$ measures prediction error on sample $(x_i, y_i)$.\n\nFor basic gradient descent we try to minimize the loss by walking the parameters against the gradient\n$$\n\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L(\\theta)\n$$\n\nWhere $f$ is represented as a neural net and $\\theta$ are it's parameters. In case for a linear neural net that is the weights and biases.\n\nThe shape of $L$ depends on which algorithm for training we need, but what we always need are the derivatives for the net parameters and these can be automatically calculated by pyTorch.\n\nLet's look at a simple example in the code below. The setup is as follows:\nWe want $f$ to be given by the simplest net we can get. A linear neural net of size $(1,1)$, i.e., one input and one output. For this net the parameters are $\\theta = (w,b)$ where $w$ is the single weight and $b$ the single bias of the net. For input $x$ the net produces $f(x) = xw + b$, a linear function.\nAs a loss function we use MSE which is just $MSE(f(x;\\theta), y) = (f(x;\\theta) - y)^2$.\nFor the point point $(x,y) = (3,5)$ we want to see which wey the gradient points for $\\theta = (0,0)$\n$$\nL((w,b)) = (3w + b - 5)^2 = 9w^2 + 6w(b-5) + (b-5)^2\n$$\nSo the gradient is\n$$\n\\nabla_\\theta L((w,b)) = \n\\begin{pmatrix}\n18w + 6b - 30 \\\\\n6w + 2b - 10\n\\end{pmatrix}\n$$\nAnd thus for $(w,b) = (0,0)$ the gradient is $(-30, -10)$.\n\nUsing pyTorch we get the same result (that's kind of magical):\n\n::: {#8226a386 .cell execution_count=12}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch\n\n# linear net representing f(x) = 0.0 x + 0.0\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\nx = torch.tensor([3.0])\ny = torch.tensor([5.0])\nf_x = model_net.forward(x)\n\nloss = (f_x - y).pow(2)\nloss.backward()  # <- this computes gradients for parameters\n\n# and this is how we can access them\nprint(f\"grad(w): {model_net.weight.grad}\")\nprint(f\"grad(b): {model_net.bias.grad}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngrad(w): tensor([[-30.]])\ngrad(b): tensor([-10.])\n```\n:::\n:::\n\n\n::: {.callout-note}\n- the code has some stuff about tensor shapes that can trip you up when you're new\n- the weight parameter is a tensor of shape (inputs, outputs)=(1,1), as a list this is `[[w]]`\n- the same is true for the shapes of the gradients \n- neutral nets like their arguments to be batched. So in our example even though the basic input shape for the nn is (1) it really would like tho have a batch of (1), i.e., a shape (b,1). Our batch has only size 1, so we want to just add an extra dimension at the beginning, which is done by `.unsqueeeze(0)` (new 0 dimension please)\n:::\n\nWe can use this to create or first little toy stochastic gradient descent.\nWe will extend the setup from above. We have our model function $f(\\theta_0)$ with $\\theta_0 = (0,0)$. We want to approximate our target function $f(\\theta_\\infty)$ with $\\theta_\\infty = (2,1)$ using stochastic gradient descent. Of course we should imagine here that we don't know our target function and doesn't even have to be expressible in our parametrization.\nWe will do stochastic gradient descent by sampling $x$ uniformly from $(-1,1)$ (this region is arbitrary) and update according to\n$$\n\\theta_{t+1} \\gets \\theta_t - \\eta \\nabla_{\\theta}MSE(f(x_t;\\theta_t),f(x_t;\\theta_\\infty))\n$$\n\n::: {#1248f9fc .cell execution_count=13}\n``` {.python .cell-code}\n# === Simple example with linear layers ===\ntorch.manual_seed(7)\neta = 0.1\n\n# target function with (w,b) = (2,1)\ntarget_function = nn.Linear(1, 1)\ntarget_function.weight = nn.Parameter(torch.tensor([[2.0]]))\ntarget_function.bias = nn.Parameter(torch.tensor([1.0]))\n\n# model function initialized with (w,b) = (0,0)\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n\n# this is one step\ndef step(x, y):\n    o = model_net.forward(x)\n    loss = (o - y).pow(2)\n    loss.backward()\n\n    model_net.weight = nn.Parameter(model_net.weight - eta * model_net.weight.grad)\n    model_net.bias = nn.Parameter(model_net.bias - eta * model_net.bias.grad)\n\n\n# do 200 steps and record each θ = (w,b)\ntorch_path = []\nfor _ in range(200):\n    torch_path.append(\n        (model_net.weight.data[0].item(), model_net.bias.data.data[0].item())\n    )\n    x = torch.rand((1, 1))\n    y = target_function.forward(x)\n    step(x, y)\nprint(f\"the final θ = {torch_path[-1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthe final θ = (1.9013594388961792, 1.0614407062530518)\n```\n:::\n:::\n\n\nThis actually worked. And it will also work for much more complicated nets and other loss functions. Of course the theory behind it has to be sound.\nActually I want to go a little bit into this direction for our toy example. It might still be kind of very abstract and not really palpable why it does work actually. Especially when you look at the first couple of results it's nto really clear if they are going the right way.\n\n::: {#756e5255 .cell execution_count=14}\n``` {.python .cell-code}\ntorch_path[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[(0.0, 0.0),\n (0.0682234913110733, 0.2931046187877655),\n (0.19812387228012085, 0.5987083911895752),\n (0.23019880056381226, 0.7535954117774963),\n (0.4016020596027374, 1.0257779359817505)]\n```\n:::\n:::\n\n\nWhat we do basically, in each step take a sample of the expected loss\n$$\n\\begin{split}\nF(w,b) &= \\mathbb{E}_{X\\sim U(0,1)}\\big[(wX + b - (2X +1))^2\\big] \\\\\n&= \\int_0^1 ((w-2)x + (b-1))^2 \\mathrm{d}x\\\\\n&= \\frac{(w-2)^2}{3} + (w-2)(b-1) + (b-1)^2\n\\end{split}\n$$\nand the gradient\n$$\n\\nabla F(w,b) =\n\\begin{pmatrix}\n\\frac{2}{3}(w-2) + (b-1)\\\\\n(w-2) + 2(b-1)\n\\end{pmatrix}\n$$\nIn deterministic GD with step size $\\eta$, is then\n$$\n\\begin{pmatrix}\nw_{t+1}\\\\\nb_{t+1}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}\n- \\eta\n\\begin{pmatrix}\n\\frac{2}{3} & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\left(\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix} -\n\\begin{pmatrix}\n2\\\\\n1\n\\end{pmatrix}\n\\right)\n$$\n\nSo there is some theory behind this that this converges when when $\\eta A$ is a contraction and $I - A$ invertible, but let us rather plot the vector field given by the update rule $-A (\\theta - \\theta^*)$ and see how the deterministic GD and the stochastic GD we have computed earlier fare \n\n::: {#e62569cd .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ── Settings ───────────────────────────────────\nw_star, b_star = 2.0, 1.0\nA = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\nlr = 0.1\nnum_steps = 200\n\n# ── Build grid for vector field ───────────────\nw_vals = np.linspace(0, 3, 15)\nb_vals = np.linspace(0, 2, 15)\nW, B = np.meshgrid(w_vals, b_vals)\nU = np.zeros_like(W)\nV = np.zeros_like(B)\n\n# Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\nfor i in range(W.shape[0]):\n    for j in range(W.shape[1]):\n        theta = np.array([W[i, j], B[i, j]])\n        grad = A.dot(theta - np.array([w_star, b_star]))\n        U[i, j] = -grad[0]\n        V[i, j] = -grad[1]\n\n# ── Deterministic GD trajectory ──────────────\ndet_path = [(0.0, 0.0)]\nfor _ in range(num_steps):\n    w, b = det_path[-1]\n    grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n    det_path.append((w - lr * grad[0], b - lr * grad[1]))\nw_det, b_det = zip(*det_path)\n\n\n# from pyTorch\nw_torch, b_torch = zip(*torch_path)\n\n# ── Plot vector field + paths ────────────────\nplt.figure(figsize=(8, 6))\nplt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\nplt.plot(w_det, b_det, \"o-\", label=\"Deterministic GD\")\nplt.plot(w_torch, b_torch, \".-\", label=\"Stochastic GD\")\nplt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\nplt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\nplt.xlabel(\"Weight $w$\")\nplt.ylabel(\"Bias $b$\")\nplt.title(\"Gradient Vector Field\\nand GD Trajectories\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-deep-learning-with-pytorch_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nIt's also so interesting to see that the even though the stochastic GD bumbles along the 2D-plane it arrives at the optimal solution quicker then the deterministic GD. (I have even choosen a quite eratic seed for the stochastic GD)\n\n### Batches\n\nActually nets want batches of data. Until now pyTorch just applied some behind the scenes magic so that we didn't notice.\n\nSo in our model_net we gave it one vector of size 1, i.e., a 1-Dimensional tensor. The basic function actually expects a batch a tensor of shape (b,1) where $b$ is the lengtht of the batch and it returns all the results as a shape (b,1) tensor again. Just to make it general. If we have a nn that is can process tensors of shape s_in and produces tensors of shape s_out we actually should feed the network tensors of shape (b,s_in) and the forward result is a tensor of shape (b,s_out).\nAnd now let's go back again to what happens when we have only one tensor. Then behind the scenes pyTorch takes our shape (1) tensor and transforms it into a shape (1,1) tensor (so [x] became [[x]], which looks pointless without context). For that we can use `unsqueeze`\n\n::: {#7a51fa58 .cell execution_count=16}\n``` {.python .cell-code}\nt = torch.tensor([1, 2])\nt.unsqueeze(0)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\ntensor([[1, 2]])\n```\n:::\n:::\n\n\nIt basically wraps an extra dimension around the dimension that we have specified (here it was the most outer one).\n\nIt basically introduces another dimension of size 1.\nActually `squeeze` does get rid of (pointless) dimensions of size 1\n\n::: {#82197b02 .cell execution_count=17}\n``` {.python .cell-code}\nt = torch.randn(2, 1, 2, 1)\nprint(f\"before squeeze: {t.shape}\")\nt = t.squeeze()\nprint(f\"after squeeze: {t.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbefore squeeze: torch.Size([2, 1, 2, 1])\nafter squeeze: torch.Size([2, 2])\n```\n:::\n:::\n\n\nBut batches are great they can reduce the bumbleness of our updates and pyTorch can compute them also much faster internally then feeding the tensors 1 by 1 in python loops.\n\nLet's look at the linear layer example from above again and introduce some batches.\n\n::: {#2c379ff5 .cell execution_count=18}\n``` {.python .cell-code}\n# === Simple example with linear layers ===\ntorch.manual_seed(0)\nBATCH_SIZE = 20\nSTEP_SIZE = 0.1\nSTEPS = 100\n\n# target function with (w,b) = (2,1)\ntarget_function = nn.Linear(1, 1)\ntarget_function.weight = nn.Parameter(torch.tensor([[2.0]]))\ntarget_function.bias = nn.Parameter(torch.tensor([1.0]))\n\n# model function initialized with (w,b) = (0,0)\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n\n# this is one step\ndef step(x_batch, y_batch):\n    loss = (model_net.forward(x_batch) - y_batch).pow(2).mean()\n    loss.backward()\n\n    model_net.weight = nn.Parameter(\n        model_net.weight - STEP_SIZE * model_net.weight.grad\n    )\n    model_net.bias = nn.Parameter(model_net.bias - STEP_SIZE * model_net.bias.grad)\n\n\n# do steps and record each θ = (w,b)\ntorch_path = []\nfor _ in range(STEPS):\n    torch_path.append(\n        (model_net.weight.data[0].item(), model_net.bias.data.data[0].item())\n    )\n    x_batch = torch.rand((BATCH_SIZE, 1))\n    y_batch = target_function.forward(x_batch)\n    step(x_batch, y_batch)\nprint(f\"the final θ = {torch_path[-1]}\")\nprint((7 / 30, 0.4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthe final θ = (1.693508267402649, 1.1575511693954468)\n(0.23333333333333334, 0.4)\n```\n:::\n:::\n\n\n::: {#96f59d11 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ── Settings ───────────────────────────────────\nw_star, b_star = 2.0, 1.0\nA = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\nlr = 0.1\nnum_steps = 100\n\n# ── Build grid for vector field ───────────────\nw_vals = np.linspace(0, 3, 15)\nb_vals = np.linspace(0, 2, 15)\nW, B = np.meshgrid(w_vals, b_vals)\nU = np.zeros_like(W)\nV = np.zeros_like(B)\n\n# Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\nfor i in range(W.shape[0]):\n    for j in range(W.shape[1]):\n        theta = np.array([W[i, j], B[i, j]])\n        grad = A.dot(theta - np.array([w_star, b_star]))\n        U[i, j] = -grad[0]\n        V[i, j] = -grad[1]\n\n# ── Deterministic GD trajectory ──────────────\ndet_path = [(0.0, 0.0)]\nfor _ in range(num_steps):\n    w, b = det_path[-1]\n    grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n    det_path.append((w - lr * grad[0], b - lr * grad[1]))\nw_det, b_det = zip(*det_path)\n\n\n# from pyTorch\nw_torch, b_torch = zip(*torch_path)\n\n# ── Plot vector field + paths ────────────────\nplt.figure(figsize=(8, 6))\nplt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\nplt.plot(w_det, b_det, \"o-\", label=\"Deterministic GD\")\nplt.plot(w_torch, b_torch, \".-\", label=\"Stochastic GD\")\nplt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\nplt.scatter([7 / 30], [0.4], marker=\"x\", s=100, label=\"first step\")\nplt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\nplt.xlabel(\"Weight $w$\")\nplt.ylabel(\"Bias $b$\")\nplt.title(\"Gradient Vector Field\\nand GD Trajectories\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-deep-learning-with-pytorch_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\nNow we can see that the batched stochastic GD follows the determinsitce GD much more closely.\n\n## Gradients\n\nNow we look at calculating gradients a more detailed.\n\nLet's take something super simple $f(x,y) = x + 2y$ and use pyTorch to compute it's gradient. For that we need the tensors to be float tensors, otherwise we get something like `RuntimeError: Only Tensors of floating point and complex dtype can require gradients` which means a tensor must be a float tensor so we can compute gradients (which is quite sensible, it's hard to compute gradients for integers). We also have to add `requires_grad=True` because for manually created tensors this is false by default.\n\n::: {#80a4cbcc .cell execution_count=20}\n``` {.python .cell-code}\nimport numpy as np\n\nx = torch.tensor([1.0], requires_grad=True)\ny = torch.tensor([2.0], requires_grad=True)\n\nresult = x + 2 * y\nresult.backward()\n\n\nprint(f\"gradient for x:  {x.grad}\")\nprint(f\"gradient for y: {y.grad}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngradient for x:  tensor([1.])\ngradient for y: tensor([2.])\n```\n:::\n:::\n\n\nWhen we print plus now we see a function reference that pyTorch uses to determine these gradients.\n\n::: {#bbc44c27 .cell execution_count=21}\n``` {.python .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntensor([5.], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nWe get a reference to the function `AddBackward`, because this function 'knows' how to handle the backward pass of computing the gradients for an addition (because result came from an addition).\n\nJust for fun we can check other operations\n\n::: {#5247cd22 .cell execution_count=22}\n``` {.python .cell-code}\nprint(x * y)\nprint(x / y)\nprint(max(x, y))\nprint(x < y)\nprint(x.unsqueeze(0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([2.], grad_fn=<MulBackward0>)\ntensor([0.5000], grad_fn=<DivBackward0>)\ntensor([2.], requires_grad=True)\ntensor([True])\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\n```\n:::\n:::\n\n\n### The mysterious of the viewBackward0\n\nBecause we didn't input a batch of tensors internally it created a batch (but just as a view) like this:\n\n::: {#0dac3791 .cell execution_count=23}\n``` {.python .cell-code}\nprint(x.view(-1, x.size(-1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1.]], grad_fn=<ViewBackward0>)\n```\n:::\n:::\n\n\nAnd that's where the view came from\n\n## NN building blocks\n\nWe can connect layers with `Sequential`.\n<!-- hm, actually i dont like this blunt way of introducing concepts with their code counterparts. I know im doing it all the time -->\n\n::: {#24f55306 .cell execution_count=24}\n``` {.python .cell-code}\n# === combining networks ===\ninput_layer = nn.Linear(4, 125)\nhidden_layer = nn.Linear(125, 125)\noutput_layer = nn.Linear(125, 2)\n\nsequential_nn = nn.Sequential(input_layer, hidden_layer, output_layer)\n\n\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0]).unsqueeze(0)\n\n# Applying sequential_nn produces the same as...\nprint(f\"sequential_nn: {sequential_nn(tensor)}\")\n# ... applying the individual layers in order\nprint(f\"concatenated: {output_layer(hidden_layer(input_layer(tensor)))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsequential_nn: tensor([[-0.4515,  0.2871]], grad_fn=<AddmmBackward0>)\nconcatenated: tensor([[-0.4515,  0.2871]], grad_fn=<AddmmBackward0>)\n```\n:::\n:::\n\n\n::: {.callout-note}\n- I haven't used the forward function for the input but just applied the layer to the input directly. This does just use the forward function\n<!-- what python mechanic allows this?-->\n- And yes, we have to make manually that our layers do fit together othewise we might get something like this: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x126 and 125x2)\n- there are, of course, packages that take this work away from you, but we're learning here. So we don't use them\n<!-- is that really true. Can you give an example?-->\n:::\n\nWe combined 3 linear layers, which actually doesn't make much sense as this is basically multiplying three matrices together and this is just a single matrix again. And similarly we could have just done with a single layer from 4 inputs to 2 outputs.\n\nBut we can add other layers in between. For example a Rectified Linear Unit (ReLU) which is very easy\n$$\n\\mathrm{ReLU}(x) = \\max(0,x)\n$$\nand when we add as a layer it just 'rectifies' each input and forwards it (and it doesn't need any size specification).\n\nIn this example we can see that the third component gets clipped.\n\n::: {#3cbc6515 .cell execution_count=25}\n``` {.python .cell-code}\n# === ReLU clips input ===\nlayer = nn.ReLU()\n\nlayer(torch.tensor([1, 0, -1]).unsqueeze(0))\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\ntensor([[1, 0, 0]])\n```\n:::\n:::\n\n\nSo and if we are for example using a net like this, it can't be simplified (or at least nobody knows how and if):\n\n::: {#159dcdc1 .cell execution_count=26}\n``` {.python .cell-code}\n# === combining linear with relu ===\nnn.Sequential(\n    nn.Linear(6, 125), nn.ReLU(), nn.Linear(125, 125), nn.ReLU(), nn.Linear(125, 2)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nSequential(\n  (0): Linear(in_features=6, out_features=125, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=125, out_features=125, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=125, out_features=2, bias=True)\n)\n```\n:::\n:::\n\n\nWe will actually use this (or very similar) net to solve the cartpole environment.\n\n\n\n\nNow let's combine them linear and relu (like you normally don't combine them)\n\n::: {#b5098887 .cell execution_count=27}\n``` {.python .cell-code}\nins = 3\nouts = 2\n\nnet = nn.Sequential(nn.ReLU(), nn.Linear(ins, outs))\n\nvector = torch.tensor([-1, 0, 1], dtype=torch.float32)\n\nloss = net(vector).sum()\nloss.backward()\n\nprint(\"Gradients for the weights:\", net[1].weight.grad)\nprint(\"Gradients for the biases:\", net[1].bias.grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGradients for the weights: tensor([[0., 0., 1.],\n        [0., 0., 1.]])\nGradients for the biases: tensor([1., 1.])\n```\n:::\n:::\n\n\nAs expected the partial derivatives of the weights are all 0 where ReLU returned 0. And the partial derivatives of the biases are of course still 1 in this kind of example\n\n## Optimizing\n\nHow should I slowly introduce optimizing?\n\n::: {#ef1eddca .cell execution_count=28}\n``` {.python .cell-code}\nimport torch.optim as optim\nnet = nn.Sequential(\n  nn.ReLU(),\n  nn.Linear(ins, outs)\n)\n\nnet[1].weight.data = torch.tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.float32)\nprint(\"Old weights:\", net[1].state_dict()['weight'])\n\nvector = torch.tensor([-1,0,1], dtype=torch.float32)\n\n\n\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nloss = net(vector).sum()\n\noptimizer.zero_grad()\nloss.backward()\nprint(\"Gradients for the weights:\", net[1].weight.grad)\nprint(\"Gradients for the biases:\", net[1].bias.grad)\noptimizer.step()\n\nprint(\"New weights:\", net[1].state_dict()['weight'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOld weights: tensor([[1., 1., 1.],\n        [1., 1., 1.]])\nGradients for the weights: tensor([[0., 0., 1.],\n        [0., 0., 1.]])\nGradients for the biases: tensor([1., 1.])\nNew weights: tensor([[1.0000, 1.0000, 0.9900],\n        [1.0000, 1.0000, 0.9900]])\n```\n:::\n:::\n\n\nWhat happens if we step again?:\n\n::: {#ec61eedf .cell execution_count=29}\n``` {.python .cell-code}\noptimizer.step()\n\nprint(\"Newest weights:\", net[1].state_dict()['weight'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNewest weights: tensor([[1.0000, 1.0000, 0.9800],\n        [1.0000, 1.0000, 0.9800]])\n```\n:::\n:::\n\n\n## The CartPole session\n\n::: {#0f457941 .cell execution_count=30}\n``` {.python .cell-code}\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nenv.observation_space\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\nBox([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n```\n:::\n:::\n\n\nBox Space: The Box space represents a continuous space where each dimension (or feature) of the observation has a defined range.\n\nLower Bounds: [-4.8, -inf, -0.41887903, -inf]\n    These are the lower bounds for each of the four dimensions of the observation space.\n    -4.8: The lower bound for the first dimension (cart position).\n    -inf: The lower bound for the second dimension (cart velocity).\n    -0.41887903: The lower bound for the third dimension (pole angle).\n    -inf: The lower bound for the fourth dimension (pole angular velocity).\n\nUpper Bounds: [4.8, inf, 0.41887903, inf]\n    These are the upper bounds for each of the four dimensions of the observation space.\n    4.8: The upper bound for the first dimension (cart position).\n    inf: The upper bound for the second dimension (cart velocity).\n    0.41887903: The upper bound for the third dimension (pole angle).\n    inf: The upper bound for the fourth dimension (pole angular velocity).\n\nShape: (4,)\n    This indicates that the observation space is a 1-dimensional array with 4 elements.\n    Each observation returned by the environment will be a vector of length 4.\n\nData Type: float32\n    This specifies the data type of the observations. In this case, the observations are 32-bit floating-point numbers.\n\n::: {#db58cbfe .cell execution_count=31}\n``` {.python .cell-code}\nenv.observation_space.__dict__\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n{'dtype': dtype('float32'),\n '_shape': (4,),\n 'low': array([-4.8       ,        -inf, -0.41887903,        -inf], dtype=float32),\n 'bounded_below': array([ True, False,  True, False]),\n 'high': array([4.8       ,        inf, 0.41887903,        inf], dtype=float32),\n 'bounded_above': array([ True, False,  True, False]),\n 'low_repr': '[-4.8               -inf -0.41887903        -inf]',\n 'high_repr': '[4.8               inf 0.41887903        inf]',\n '_np_random': None}\n```\n:::\n:::\n\n\nLet's also look at the action_space:\n\n::: {#2200cbaf .cell execution_count=32}\n``` {.python .cell-code}\nenv.action_space.__dict__\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n{'n': np.int64(2),\n 'start': np.int64(0),\n '_shape': (),\n 'dtype': dtype('int64'),\n '_np_random': None}\n```\n:::\n:::\n\n\nAlright, so we have two actions here.\n\n",
    "supporting": [
      "03-deep-learning-with-pytorch_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "bddb6ea57ef1f89cf5fc202ed095975a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: last-modified\n---\n\n# OpenAI Gym API and Gymnasium\n\n## Creating an environment\n\nThe best starting point for working with environments in Gymnasium is the [official documentation](https://gymnasium.farama.org/), currently^[This has been checked as of the summer of 2025] maintained by the [Farama Foundation](https://farama.org/). I highly recommend consulting it - it makes it much easier to understand the structure of the action and observation spaces.\n\nTo create an environment, use `gym.make()`, passing the name of the environment as a string. The available environments and their different versions can be found in the documentation.\n\n::: {#36b65ed3 .cell execution_count=2}\n``` {.python .cell-code}\n# === creating an environment ===\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nenv.action_space\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nDiscrete(2)\n```\n:::\n:::\n\n\nAccording to the documentation, the `CartPole-v1` environment has 4 observations and 2 possible actions. Let’s confirm this by inspecting the action space:\n\n::: {#46f6a53e .cell execution_count=3}\n``` {.python .cell-code}\n# === checking out the action_space ===\nenv.action_space\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nDiscrete(2)\n```\n:::\n:::\n\n\nThis confirms that we have two discrete actions `{0, 1}` (for CartPole and also generally the discrete actions are simply numbered starting from 0). Also note that in Gymnasium the `action_space` is usually fixed, i.e., independent of the state. We will see how to deal with changing action spaces when they matter.\n\nNow let's check the observation space:\n\n::: {#af5f7b69 .cell execution_count=4}\n``` {.python .cell-code}\n# === checking out the observation_space ===\nenv.observation_space\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nBox([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n```\n:::\n:::\n\n\nThe Box space represents a 4-dimensional continuous space, with lower and upper bounds for each dimension. The third component (the shape attribute) indicates that each observation is a vector of 4 values. The first and second components specify the lower and upper bounds for each of the 4 dimension of the observation, respectively. For more detail on what each dimension represents, refer to the documentation. According to Gymnasium, a Box is \"a space that represents closed boxes in Euclidean space.\"\n\n## The random CartPole agent\n\nLet’s take the first step towards building a real agent: a random agent that takes actions randomly at each time step.\n\nThe idea is simple: the agent randomly samples actions from the environment’s action space until the episode ends - either by failure (the pole falling, cart to far off) or by timeout (very unlikely for a random agent).\n\n::: {#faccaddd .cell execution_count=5}\n``` {.python .cell-code}\n# === the random agent ===\nimport gymnasium as gym\n\n\ndef run_random_episode(env: gym.Env) -> float:\n    env.reset()\n    total_reward = 0.0\n    done = False\n\n    while not done:\n        action = env.action_space.sample()\n        _, reward, terminated, truncated, _ = env.step(action)\n        total_reward += reward\n        done = terminated or truncated\n\n    return total_reward\n```\n:::\n\n\nI'm including info boxes like the one below go into more detail of the code at many occasions. They are meant to clarify any potential open points. I often use them to introduce new Python syntax or concepts that will be used in subsequent code without further explanation.\n\n::: {.callout-note}\n\n- if you're feeling lost, check out [this Gymnasium basics guide](https://gymnasium.farama.org/introduction/basic_usage/) which explains more of the fundamentals\n- I use type hints like `env: gym.Env` as lightweight documentation. You’ll see this used frequently\n- in this random agent, we don’t store the initial observation from `env.reset()` because the agent doesn’t use it. Normally, we need store it like so `state, _ = env.reset()`\n- Similarly, we ignore the new state returned by `env.step(action)` for the same reason\n- `env.action_space.sample()` returns a sample (random) action for the current state of the environment\n\n:::\n    \nLet's see how well the random agent fares over a few episodes:\n\n::: {#e2d6922c .cell execution_count=6}\n``` {.python .cell-code}\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nepisodes = 5\nfor episode in range(1, episodes + 1):\n    print(f\"\\n=== Episode {episode}/{episodes} ===\")\n    reward = run_random_episode(env)\n    print(f\"Reward: {reward}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Episode 1/5 ===\nReward: 37.0\n\n=== Episode 2/5 ===\nReward: 15.0\n\n=== Episode 3/5 ===\nReward: 13.0\n\n=== Episode 4/5 ===\nReward: 28.0\n\n=== Episode 5/5 ===\nReward: 38.0\n```\n:::\n:::\n\n\nThe total reward in each episode corresponds to how long the pole stays balanced, because in `CartPole-v1`, the agent receives a reward of +1 for each step.\n\nObviously, it doesn't perform any good. Our overall goal is to create agents that improve these rewards using reinforcement learning techniques.\n\n",
    "supporting": [
      "02-gymnasium_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "5381199826616ef6536e5a9afd2121e2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: last-modified\n---\n\n# Deep Learning with PyTorch\n\n## PyTorch\n\nPyTorch is a library for deep learning. In this chapter, we’ll introduce its most important features, and we’ll cover everything else along the way later on.\n\nWe can import it with:\n\n::: {#bb21faa2 .cell execution_count=2}\n``` {.python .cell-code}\n# === importing pyTorch ===\nimport torch\n```\n:::\n\n\n## Tensors\n\nThe term \"tensor\" can have different meanings depending on the context. In PyTorch, a tensor is essentially a multidimensional array and it's the fundamental data structure used for all computations.\n\nTo turn a Python list into a rank-1 tensor (a vector), we do:\n\n::: {#5ab76bcd .cell execution_count=3}\n``` {.python .cell-code}\n# === a vector in pyTorch ===\ntorch.tensor([1, 2, 3])\n```\n\n::: {.cell-output .cell-output-display execution_count=210}\n```\ntensor([1, 2, 3])\n```\n:::\n:::\n\n\nHere is some key terminology for talking about tensors, based on the terminology used in [TensorFlow's introduction to tensors](https://www.tensorflow.org/guide/tensor):\n\n- axis: each axis corresponds to one index in the tensor.\n- rank: the number of axes of a tensor.\n    - a scalar has rank 0, e.g., `torch.tensor(5)`.\n    - a vector has rank 1, e.g., `torch.tensor([1, 2, 3])`.\n    - a matrix has rank 2, e.g., `torch.tensor([[1, 2], [3, 4]])`.\n- size:\n    - the size of an axis is the number of elements along it\n    - the size of a tensor is the total number of elements it contains\n- shape: a tuple giving the size along each axis.\n    - for `torch.tensor([1, 2, 3])`, it's shape is is `(3,)`.\n    - for `torch.tensor([[1, 2], [3, 4]])`, it's shape is `(2, 2)`.\n\nYou will often see \"dimension\" used interchangeably with rank, or say \"the dimension\" instead of the axis. Let me briefly motivate the use of the word dimension.\n\nIt’s not the same as the “dimension” in the phrase “a 3-dimensional vector”, which refers to the number of degrees of freedom in a vector space.\nHere, dimension refers to the number of indices you need to access the individual elements. For example, a 2D tensor needs two indices `m[i][j]`.\n\nYou could also think of a tensor as a grid or a discrete space, where the rank tells you how many independent directions you have to move between elements.\n\nHere are some more ways to define tensors in pyTorch, we always use the same shape (3,2) for the tensors.\n\n::: {#b6a35280 .cell execution_count=4}\n``` {.python .cell-code}\n# === some elemental ways to generate tensors ===\nprint(\"\\n a rank-2 tensor full of 0s\")\nprint(torch.zeros((3, 2)))\n\nprint(\"\\n a rank-2 tensor full of 7s\")\nprint(torch.full((3, 2), 7))\n\nprint(\"\\n a rank-2 tensor randomly filled with [0-9]\")\nprint(torch.randint(0, 10, (3, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n a rank-2 tensor full of 0s\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\n a rank-2 tensor full of 7s\ntensor([[7, 7],\n        [7, 7],\n        [7, 7]])\n\n a rank-2 tensor randomly filled with [0-9]\ntensor([[1, 1],\n        [1, 6],\n        [3, 7]])\n```\n:::\n:::\n\n\nTo practise the terminology from above: the tuple $(3,2)$ defines the shape. So, the first axis has size 3 and the second axis has size 2.\nWe can query it with `.shape`, which returns an object of type `torch.Size`.\n\n::: {#fd91fe10 .cell execution_count=5}\n``` {.python .cell-code}\n# === the shape of a tensor ===\nt = torch.zeros((3, 2))\nprint(t.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([3, 2])\n```\n:::\n:::\n\n\nWe can combine multiple rank-d tensors into one rank-(d+1) tensor, by stacking them along a new axis\n\n::: {#ed5996b3 .cell execution_count=6}\n``` {.python .cell-code}\n# === stacking tensors ===\n# create three rank-2 tensors\ntensor1 = torch.tensor([[1, 2], [3, 4]])\ntensor2 = torch.tensor([[5, 6], [7, 8]])\ntensor3 = torch.tensor([[9, 10], [11, 12]])\n\n# stack the tensors along a new axis\nstacked_tensor = torch.stack((tensor1, tensor2, tensor3))\nstacked_tensor\n```\n\n::: {.cell-output .cell-output-display execution_count=213}\n```\ntensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 5,  6],\n         [ 7,  8]],\n\n        [[ 9, 10],\n         [11, 12]]])\n```\n:::\n:::\n\n\nTo retrieve the second tensor back, we simply extract the second slice of the stacked tensor like this:\n\n::: {#6086ef6d .cell execution_count=7}\n``` {.python .cell-code}\n# === slicing tensors ===\nstacked_tensor[1]\n```\n\n::: {.cell-output .cell-output-display execution_count=214}\n```\ntensor([[5, 6],\n        [7, 8]])\n```\n:::\n:::\n\n\nTo clarify how indexing works in tensors, here's an example using a tensor of shape (2, 3, 4), where each element contains an integer representing its index written in \"mathematical notation\":\n\n::: {#ca68c724 .cell execution_count=8}\n``` {.python .cell-code}\n# === index positions of tensors ===\n# each entry in this tensor shows it's coordinate in mathematical notation\nt = torch.tensor(\n    [\n        [[111, 112, 113, 114], [121, 122, 123, 124], [131, 132, 133, 134]],\n        [[211, 212, 213, 214], [221, 222, 223, 224], [231, 232, 233, 234]],\n    ]\n)\nprint(t)\nprint(f\"The element at mathematical index (2,3,4) is: {t[1][2][3]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[[111, 112, 113, 114],\n         [121, 122, 123, 124],\n         [131, 132, 133, 134]],\n\n        [[211, 212, 213, 214],\n         [221, 222, 223, 224],\n         [231, 232, 233, 234]]])\nThe element at mathematical index (2,3,4) is: 234\n```\n:::\n:::\n\n\n## Linear neural nets\n\nWe'll get to know the simplest neural nets. Linear neural nets. But along the way we discuss sme stuff that is important for all neural nets.\n\nFor working with neural nets (NN) in general we need to import\n\n::: {#5983f256 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch.nn as nn\n```\n:::\n\n\nA linear neural net, has a bunch of input and output nodes and forwards each signal coming to an input node to an output node multiplied with the weight of that connection. Additionally each output node has a constant bias applied to it. \n<!-- this description is not really understandable if you don't know them already -->\n\nLet's look at a little example.\n\n::: {#ccd3618d .cell execution_count=10}\n``` {.python .cell-code}\n# === linear nn with custom parameters ===\n# create a linear layer\nlinear_nn = nn.Linear(3, 2)\n\n# manually set the weights\nweights = torch.FloatTensor([[1, 0, 1], [0, 1, -1]])\n\n# manually set the bias\nbias = torch.FloatTensor([2, -3])\n\n# assign the weights and bias to the linear layer\nlinear_nn.weight = nn.Parameter(weights)\nlinear_nn.bias = nn.Parameter(bias)\n```\n:::\n\n\n::: {.callout-note}\n- we have to make sure that the tensors for a neutral net are float tensors. That's why I used `FloatTensor' for creating the weights adn biases\n- a parameter is a tensor that is a parameter of neural net.\n<!-- is this all accurate? -->\n:::\n\nWhen we apply this network to to a vector $(x,y,z)$ it does this computation\n$$\n\\mathrm{linear\\_nn}(x,y,z) = \n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y \\\\ z\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ -3\n\\end{pmatrix}\n$$\n\nFor example for the input $(1,0,0)$ we should get $(3,-3)$. We can verify this by by using `forward` to plug a tensor into the net:\n\n::: {#b392c81c .cell execution_count=11}\n``` {.python .cell-code}\n# === applying a net to input ===\nv = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)\n```\n\n::: {.cell-output .cell-output-display execution_count=218}\n```\ntensor([ 3., -3.], grad_fn=<ViewBackward0>)\n```\n:::\n:::\n\n\n::: {.callout-note}\ninstead of writing `torch.FloatTensor`, we just can just use dedicated floats in the array by adding a decimal point.\n:::\n\nWe get the right tensor back, but also a bit extra information `grad_fn=<ViewBackward0>`. This reference to a gradient node, but we for that we have to talk about gradients first. And before we do even that, let's see why we need gradients in the first place.\nBut don't worry in @sec-viewBackward0 we discuss the mystery of the viewBackward.\n\n## Stochastic gradient descent\n\nAt its core, training a neural network means finding model parameters θ (weights and biases) that minimize a loss function\n$$\nL(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f(x_i;\\theta), y_i),\n$$\n\nwhere $\\ell$ measures prediction error on sample $(x_i, y_i)$, $L$ is the (mean) loss on the whole batch of samples.\n\nFor basic gradient descent we try to minimize the loss by walking the parameters against the gradient\n$$\n\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L(\\theta)\n$$\n\nWhere $f$ is represented as a neural net and $\\theta$ are it's parameters.\n\nLet's look at a very simple example.\nLet's pick a linear neural net of size $(1,1)$, i.e., one input and one output. \n<!-- is this called size? -->\nFor this net the parameters are $\\theta = (w,b)$ where $w$ is the single weight and $b$ the single bias of the net. For input $x$ the net produces $f(x) = xw + b$, a linear function.\nOur sample batches are just of size $1$ so we get one input $x$ and one desired output $y$.\nAs a loss function we use squarred error which is just $L(f(x;\\theta), y) = (f(x;\\theta) - y)^2$.\n\nFor the sample $(x,y) = (3,5)$ we want to see which way the gradient points for parameters $\\theta = (0,0)$:\n$$\nL((w,b)) = (3w + b - 5)^2 = 9w^2 + 6w(b-5) + (b-5)^2\n$$\nSo the gradient is\n$$\n\\nabla_\\theta L((w,b)) = \n\\begin{pmatrix}\n18w + 6b - 30 \\\\\n6w + 2b - 10\n\\end{pmatrix}\n$$\n\nAnd thus for $(w,b) = (0,0)$ the gradient \n$$\n\\nabla_\\theta L((0,0)) = \\begin{pmatrix}\n- 30 \\\\\n- 10\n\\end{pmatrix}\n$$\n\nUsing pyTorch we get the same result (and that feels kind of magical):\n\n::: {#739ff3e7 .cell execution_count=12}\n``` {.python .cell-code}\n# === one stochastic GD step ===\nimport torch.nn as nn\nimport torch\n\n# linear net representing f(x) = 0.0 x + 0.0\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\nx = torch.tensor([3.0])\ny = torch.tensor([5.0])\nf_x = model_net.forward(x)\n\nL = (f_x - y).pow(2)\n# this says: computes ∇L\nL.backward()\n\n# this is how we access the components of ∇L\nprint(f\"grad(w): {model_net.weight.grad}\")\nprint(f\"grad(b): {model_net.bias.grad}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngrad(w): tensor([[-30.]])\ngrad(b): tensor([-10.])\n```\n:::\n:::\n\n\n::: {.callout-note}\n- the code has some stuff about tensor shapes that can trip you up when you're new\n- the weight parameter is a tensor of shape (inputs, outputs)=(1,1), as a list this is `[[w]]`\n- the same is true for the shapes of the gradients \n- neutral nets like their arguments to be batched. So in our example even though the basic input shape for the nn is (1) it really would like tho have a batch of (1), i.e., a shape (b,1). Our batch has only size 1, so we want to just add an extra dimension at the beginning, which is done by `.unsqueeeze(0)` (new 0 dimension please)\n:::\n\nNote the nice terminology. `.forward` pushes a tensor `x` through our net. Abd with `.backward` we update our net according to how well it's result was for `x`.\n<!-- I'm also sure this can be a bit expanded on and less blunt -->\n\nWe can use this to create a proper little stochastic gradient descent.\nWe will extend the setup from above. We have our model function $f(x;\\theta_0)$ with $\\theta_0 = (0,0)$.\nWe want to approximate a target function $f(x;\\theta_*)$ with $\\theta_* = (2,1)$.\nOf course we should imagine here that we don't know our the parameters of the target function.\nWe will do stochastic gradient descent by sampling $x$ uniformly from $[0,1)$ (this region is chosen arbitrarily) and do our updates according to\n$$\n\\theta_{t+1} \\gets \\theta_t - \\eta \\nabla_{\\theta}L(f(x_t;\\theta_t),y_t),\n$$\nwhere $L(a,b) = (a - b)^2$ and $y_t = f(x_t, \\theta_*)$.\n\n::: {#4eb13f9c .cell execution_count=13}\n``` {.python .cell-code}\n# === stochastic GD ===\ntorch.manual_seed(7)\neta = 0.1\n\n# target function with (w,b) = (2,1)\ntarget_function = nn.Linear(1, 1)\ntarget_function.weight = nn.Parameter(torch.tensor([[2.0]]))\ntarget_function.bias = nn.Parameter(torch.tensor([1.0]))\n\n# model function initialized with (w,b) = (0,0)\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n\n# this is one step\ndef step(x, y):\n    o = model_net.forward(x)\n    loss = (o - y).pow(2)\n    loss.backward()\n\n    model_net.weight = nn.Parameter(model_net.weight - eta * model_net.weight.grad)\n    model_net.bias = nn.Parameter(model_net.bias - eta * model_net.bias.grad)\n\n\n# do 200 steps and record each θ = (w,b)\ntorch_path = []\nfor _ in range(200):\n    torch_path.append(\n        (model_net.weight.data[0].item(), model_net.bias.data.data[0].item())\n    )\n    x = torch.rand((1, 1))\n    y = target_function.forward(x)\n    step(x, y)\nprint(f\"the final θ = {torch_path[-1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthe final θ = (1.9013594388961792, 1.0614407062530518)\n```\n:::\n:::\n\n\nThis actually worked. And it will also work for much more complicated nets and other loss functions. Of course the theory behind it has to be sound.\nActually I want to go a little bit into this direction for our toy example. It might still be kind of very abstract and not really palpable why it does work actually. Especially when you look at the first couple of results it's nto really clear if they are going the right way.\n\n::: {#93abc500 .cell execution_count=14}\n``` {.python .cell-code}\ntorch_path[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=221}\n```\n[(0.0, 0.0),\n (0.0682234913110733, 0.2931046187877655),\n (0.19812387228012085, 0.5987083911895752),\n (0.23019880056381226, 0.7535954117774963),\n (0.4016020596027374, 1.0257779359817505)]\n```\n:::\n:::\n\n\nWhat we do basically, in each step take a sample of the expected loss\n$$\n\\begin{split}\nF(w,b) &= \\mathbb{E}_{X\\sim U(0,1)}\\big[(wX + b - (2X +1))^2\\big] \\\\\n&= \\int_0^1 ((w-2)x + (b-1))^2 \\mathrm{d}x\\\\\n&= \\frac{(w-2)^2}{3} + (w-2)(b-1) + (b-1)^2\n\\end{split}\n$$\nand the gradient\n$$\n\\nabla F(w,b) =\n\\begin{pmatrix}\n\\frac{2}{3}(w-2) + (b-1)\\\\\n(w-2) + 2(b-1)\n\\end{pmatrix}\n$$\nIn deterministic GD with step size $\\eta$, is then\n$$\n\\begin{pmatrix}\nw_{t+1}\\\\\nb_{t+1}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}\n- \\eta\n\\begin{pmatrix}\n\\frac{2}{3} & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\left(\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix} -\n\\begin{pmatrix}\n2\\\\\n1\n\\end{pmatrix}\n\\right)\n$$\n\nSo there is some theory behind this that this converges when when $\\eta A$ is a contraction and $I - A$ invertible, but let us rather plot the vector field given by the update rule $-A (\\theta - \\theta^*)$ and see how the deterministic GD and the stochastic GD we have computed earlier fare \n\n::: {#6562cb5f .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ── Settings ───────────────────────────────────\nw_star, b_star = 2.0, 1.0\nA = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\nlr = 0.1\nnum_steps = 200\n\n# ── Build grid for vector field ───────────────\nw_vals = np.linspace(0, 3, 15)\nb_vals = np.linspace(0, 2, 15)\nW, B = np.meshgrid(w_vals, b_vals)\nU = np.zeros_like(W)\nV = np.zeros_like(B)\n\n# Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\nfor i in range(W.shape[0]):\n    for j in range(W.shape[1]):\n        theta = np.array([W[i, j], B[i, j]])\n        grad = A.dot(theta - np.array([w_star, b_star]))\n        U[i, j] = -grad[0]\n        V[i, j] = -grad[1]\n\n# ── Deterministic GD trajectory ──────────────\ndet_path = [(0.0, 0.0)]\nfor _ in range(num_steps):\n    w, b = det_path[-1]\n    grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n    det_path.append((w - lr * grad[0], b - lr * grad[1]))\nw_det, b_det = zip(*det_path)\n\n\n# from pyTorch\nw_torch, b_torch = zip(*torch_path)\n\n# ── Plot vector field + paths ────────────────\nplt.figure(figsize=(8, 6))\nplt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\nplt.plot(w_det, b_det, \"o-\", label=\"Deterministic GD\")\nplt.plot(w_torch, b_torch, \".-\", label=\"Stochastic GD\")\nplt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\nplt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\nplt.xlabel(\"Weight $w$\")\nplt.ylabel(\"Bias $b$\")\nplt.title(\"Gradient Vector Field\\nand GD Trajectories\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-deep-learning-with-pytorch_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nIt's also so interesting to see that the even though the stochastic GD bumbles along the 2D-plane and the end it's not much worse that deterministic GD.\n\n### Batches\n\nActually nets want batches of data. Until now pyTorch just applied some behind the scenes magic so that we didn't notice.\n\nSo in our model_net we gave it one vector of size 1, i.e., a 1-Dimensional tensor. The basic function actually expects a batch a tensor of shape (b,1) where $b$ is the lengtht of the batch and it returns all the results as a shape (b,1) tensor again. Just to make it general. If we have a nn that is can process tensors of shape s_in and produces tensors of shape s_out we actually should feed the network tensors of shape (b,s_in) and the forward result is a tensor of shape (b,s_out).\nAnd now let's go back again to what happens when we have only one tensor. Then behind the scenes pyTorch takes our shape (1) tensor and transforms it into a shape (1,1) tensor (so [x] became [[x]], which looks pointless without context). For that we can use `unsqueeze`\n\n::: {#3da92af4 .cell execution_count=16}\n``` {.python .cell-code}\nt = torch.tensor([1, 2])\nt.unsqueeze(0)\n```\n\n::: {.cell-output .cell-output-display execution_count=223}\n```\ntensor([[1, 2]])\n```\n:::\n:::\n\n\nIt basically wraps an extra dimension around the dimension that we have specified (here it was the most outer one).\n\nIt basically introduces another dimension of size 1.\nActually `squeeze` does get rid of (pointless) dimensions of size 1\n\n::: {#92fc543d .cell execution_count=17}\n``` {.python .cell-code}\nt = torch.randn(2, 1, 2, 1)\nprint(f\"before squeeze: {t.shape}\")\nt = t.squeeze()\nprint(f\"after squeeze: {t.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbefore squeeze: torch.Size([2, 1, 2, 1])\nafter squeeze: torch.Size([2, 2])\n```\n:::\n:::\n\n\nBut batches are great they can reduce the bumbleness of our updates and pyTorch can compute them also much faster internally then feeding the tensors 1 by 1 in python loops.\n\nLet's look at the linear layer example from above again and introduce some batches.\n\n::: {#0f4f2720 .cell execution_count=18}\n``` {.python .cell-code}\n# === Simple example with linear layers ===\ntorch.manual_seed(0)\nBATCH_SIZE = 20\nSTEP_SIZE = 0.1\nSTEPS = 100\n\n# target function with (w,b) = (2,1)\ntarget_function = nn.Linear(1, 1)\ntarget_function.weight = nn.Parameter(torch.tensor([[2.0]]))\ntarget_function.bias = nn.Parameter(torch.tensor([1.0]))\n\n# model function initialized with (w,b) = (0,0)\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n\n# this is one step\ndef step(x_batch, y_batch):\n    loss = (model_net.forward(x_batch) - y_batch).pow(2).mean()\n    loss.backward()\n\n    model_net.weight = nn.Parameter(\n        model_net.weight - STEP_SIZE * model_net.weight.grad\n    )\n    model_net.bias = nn.Parameter(model_net.bias - STEP_SIZE * model_net.bias.grad)\n\n\n# do steps and record each θ = (w,b)\ntorch_path = []\nfor _ in range(STEPS):\n    torch_path.append(\n        (model_net.weight.data[0].item(), model_net.bias.data.data[0].item())\n    )\n    x_batch = torch.rand((BATCH_SIZE, 1))\n    y_batch = target_function.forward(x_batch)\n    step(x_batch, y_batch)\nprint(f\"the final θ = {torch_path[-1]}\")\nprint((7 / 30, 0.4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthe final θ = (1.693508267402649, 1.1575511693954468)\n(0.23333333333333334, 0.4)\n```\n:::\n:::\n\n\n::: {#b0a0b804 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ── Settings ───────────────────────────────────\nw_star, b_star = 2.0, 1.0\nA = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\nlr = 0.1\nnum_steps = 100\n\n# ── Build grid for vector field ───────────────\nw_vals = np.linspace(0, 3, 15)\nb_vals = np.linspace(0, 2, 15)\nW, B = np.meshgrid(w_vals, b_vals)\nU = np.zeros_like(W)\nV = np.zeros_like(B)\n\n# Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\nfor i in range(W.shape[0]):\n    for j in range(W.shape[1]):\n        theta = np.array([W[i, j], B[i, j]])\n        grad = A.dot(theta - np.array([w_star, b_star]))\n        U[i, j] = -grad[0]\n        V[i, j] = -grad[1]\n\n# ── Deterministic GD trajectory ──────────────\ndet_path = [(0.0, 0.0)]\nfor _ in range(num_steps):\n    w, b = det_path[-1]\n    grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n    det_path.append((w - lr * grad[0], b - lr * grad[1]))\nw_det, b_det = zip(*det_path)\n\n\n# from pyTorch\nw_torch, b_torch = zip(*torch_path)\n\n# ── Plot vector field + paths ────────────────\nplt.figure(figsize=(8, 6))\nplt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\nplt.plot(w_det, b_det, \"o-\", label=\"Deterministic GD\")\nplt.plot(w_torch, b_torch, \".-\", label=\"Stochastic GD\")\nplt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\nplt.scatter([7 / 30], [0.4], marker=\"x\", s=100, label=\"first step\")\nplt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\nplt.xlabel(\"Weight $w$\")\nplt.ylabel(\"Bias $b$\")\nplt.title(\"Gradient Vector Field\\nand GD Trajectories\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-deep-learning-with-pytorch_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\nNow we can see that the batched stochastic GD follows the determinsitce GD much more closely.\n\n## Gradients\n\nNow we look at calculating gradients a more detailed.\n\nLet's take something super simple $f(x,y) = x + 2y$ and use pyTorch to compute it's gradient. For that we need the tensors to be float tensors, otherwise we get something like `RuntimeError: Only Tensors of floating point and complex dtype can require gradients` which means a tensor must be a float tensor so we can compute gradients (which is quite sensible, it's hard to compute gradients for integers). We also have to add `requires_grad=True` because for manually created tensors this is false by default.\n\n::: {#aaffda16 .cell execution_count=20}\n``` {.python .cell-code}\nimport numpy as np\n\nx = torch.tensor([1.0], requires_grad=True)\ny = torch.tensor([2.0], requires_grad=True)\n\nresult = x + 2 * y\nresult.backward()\n\n\nprint(f\"gradient for x:  {x.grad}\")\nprint(f\"gradient for y: {y.grad}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngradient for x:  tensor([1.])\ngradient for y: tensor([2.])\n```\n:::\n:::\n\n\nWhen we print plus now we see a function reference that pyTorch uses to determine these gradients.\n\n::: {#db0fdcb9 .cell execution_count=21}\n``` {.python .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-display execution_count=228}\n```\ntensor([5.], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nWe get a reference to the function `AddBackward`, because this function 'knows' how to handle the backward pass of computing the gradients for an addition (because result came from an addition).\n\nJust for fun we can check other operations\n\n::: {#5d0e71cb .cell execution_count=22}\n``` {.python .cell-code}\nprint(x * y)\nprint(x / y)\nprint(max(x, y))\nprint(x < y)\nprint(x.unsqueeze(0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([2.], grad_fn=<MulBackward0>)\ntensor([0.5000], grad_fn=<DivBackward0>)\ntensor([2.], requires_grad=True)\ntensor([True])\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\n```\n:::\n:::\n\n\n### The mysterious of the viewBackward0 {#sec-viewBackward0}\n\nBecause we didn't input a batch of tensors internally it created a batch (but just as a view) like this:\n\n::: {#1ba1ae53 .cell execution_count=23}\n``` {.python .cell-code}\nprint(x.view(-1, x.size(-1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1.]], grad_fn=<ViewBackward0>)\n```\n:::\n:::\n\n\nAnd that's where the view came from\n\n## NN building blocks\n\nWe can connect layers with `Sequential`.\n<!-- hm, actually i dont like this blunt way of introducing concepts with their code counterparts. I know im doing it all the time -->\n\n::: {#5c56c88e .cell execution_count=24}\n``` {.python .cell-code}\n# === combining networks ===\ninput_layer = nn.Linear(4, 125)\nhidden_layer = nn.Linear(125, 125)\noutput_layer = nn.Linear(125, 2)\n\nsequential_nn = nn.Sequential(input_layer, hidden_layer, output_layer)\n\n\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0]).unsqueeze(0)\n\n# Applying sequential_nn produces the same as...\nprint(f\"sequential_nn: {sequential_nn(tensor)}\")\n# ... applying the individual layers in order\nprint(f\"concatenated: {output_layer(hidden_layer(input_layer(tensor)))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsequential_nn: tensor([[-0.4515,  0.2871]], grad_fn=<AddmmBackward0>)\nconcatenated: tensor([[-0.4515,  0.2871]], grad_fn=<AddmmBackward0>)\n```\n:::\n:::\n\n\n::: {.callout-note}\n- I haven't used the forward function for the input but just applied the layer to the input directly. This does just use the forward function\n<!-- what python mechanic allows this?-->\n- And yes, we have to make manually that our layers do fit together othewise we might get something like this: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x126 and 125x2)\n- there are, of course, packages that take this work away from you, but we're learning here. So we don't use them\n<!-- is that really true. Can you give an example?-->\n:::\n\nWe combined 3 linear layers, which actually doesn't make much sense as this is basically multiplying three matrices together and this is just a single matrix again. And similarly we could have just done with a single layer from 4 inputs to 2 outputs.\n\nBut we can add other layers in between. For example a Rectified Linear Unit (ReLU) which is very easy\n$$\n\\mathrm{ReLU}(x) = \\max(0,x)\n$$\nand when we add as a layer it just 'rectifies' each input and forwards it (and it doesn't need any size specification).\n\nIn this example we can see that the third component gets clipped.\n\n::: {#d79306da .cell execution_count=25}\n``` {.python .cell-code}\n# === ReLU clips input ===\nlayer = nn.ReLU()\n\nlayer(torch.tensor([1, 0, -1]).unsqueeze(0))\n```\n\n::: {.cell-output .cell-output-display execution_count=232}\n```\ntensor([[1, 0, 0]])\n```\n:::\n:::\n\n\nSo and if we are for example using a net like this, it can't be simplified (or at least nobody knows how and if):\n\n::: {#cd77bc4d .cell execution_count=26}\n``` {.python .cell-code}\n# === combining linear with relu ===\nnn.Sequential(\n    nn.Linear(6, 125), nn.ReLU(), nn.Linear(125, 125), nn.ReLU(), nn.Linear(125, 2)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=233}\n```\nSequential(\n  (0): Linear(in_features=6, out_features=125, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=125, out_features=125, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=125, out_features=2, bias=True)\n)\n```\n:::\n:::\n\n\nWe will actually use this (or very similar) net to solve the cartpole environment.\n\n## Example - GAN on Atari images\n\nThis example from the book is quite big and I think as a beginner it's ok not to bother to understand it. However, it's fun to run it out of the box (when it works and it didn't for me at the beginning and that was quite demotivating). It's also nice to identify the things that we have already seen in a proper example and also teaser some things that are still to come.\n\nThe running file is under `chapter-03/03_atari_gan.py`. It looks a bit daunting at first but nearly half of it is \"just\" declaration of the neural networks. Of course there is quite some stuff to learn here as well, but for the beginning I would say \"there are two sufficently complicated nets\" is enough.\n\n## Discriminator vs Generator Loss\n\n::: {#f3087056 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# adjust these paths to wherever you saved your CSVs\ndisc_csv = \"quarto/data/atari_gan_dis_loss.csv\"\ngen_csv = \"quarto/data/atari_gan_gen_loss.csv\"\n\n# TensorBoard CSVs typically have columns: wall_time, step, value\ndf_disc = pd.read_csv(disc_csv)\ndf_gen = pd.read_csv(gen_csv)\n\n# Create plot\nfig, ax1 = plt.subplots(figsize=(8, 5))\n\n# Left y-axis: Discriminator loss\nax1.plot(\n    df_disc[\"Step\"], df_disc[\"Value\"], color=\"tab:blue\", label=\"Discriminator Loss\"\n)\nax1.set_xlabel(\"Training Step\")\nax1.set_ylabel(\"Discriminator Loss\", color=\"tab:blue\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\n# Right y-axis: Generator loss\nax2 = ax1.twinx()\nax2.plot(df_gen[\"Step\"], df_gen[\"Value\"], color=\"tab:orange\", label=\"Generator Loss\")\nax2.set_ylabel(\"Generator Loss\", color=\"tab:orange\")\nax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n\n# Title and layout\nplt.title(\"GAN Losses (Dual Axis)\")\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-deep-learning-with-pytorch_files/figure-html/cell-27-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "03-deep-learning-with-pytorch_files"
    ],
    "filters": [],
    "includes": {}
  }
}
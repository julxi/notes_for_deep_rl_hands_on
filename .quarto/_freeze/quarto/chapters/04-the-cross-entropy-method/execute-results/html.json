{
  "hash": "1f3bdb835435d295b22b6b18c379615e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: last-modified\n---\n\n# The cross entropy method\n\nWe will use the cross entropy method to solve the stick on a pole challenge. Solved means that we can balance the pole so long that the episode is clapped\n\n> Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n\nWhich is after 500 time steps.\n\nWe will solve it with a very simple net.\nIt has 4 observations: cart position, cart velocity, pole angle, pole angular velocity\nand two actions: move car left, move car right.\n\n::: {#6c1a7829 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch.nn as nn\n\n# the net\nobs_size = 4\nn_actions = 2\nhidden_layer = 128\ncartpole_net = nn.Sequential(\n    nn.Linear(obs_size, hidden_layer),\n    nn.ReLU(),\n    nn.Linear(hidden_layer, n_actions),\n    nn.Softmax(dim=1),\n)\n```\n:::\n\n\n## Softmax\n\nA function that takes $n$ inputs and makes them all positive and into a probability distribution.\n\"Slice along dim\" = fix all dimensions except dim, and let dim vary.\nA slice along dim means: fix all indices except for the one at position dim, and let that one vary.\n\n## The cross-entropy method in practice\n\nSo with softmax we can make a fully fledged cartpole agent: It gets 4 inputs. Internally it usses its net to produce a probability distribution for the outputs and then it randomly chooses an action according to that distribution.\n\nThis means our agent is a policy-based agent, because the output of the nn is directly a policy $\\pi(a|s)$.\n\nSampling\nWe repeatedly let the agent interact with the environment to generate a diverse set of episodes. Each episode is a sequence of state–action pairs and the cumulative reward obtained.\n\nSelection\nNot all episodes are equally informative. We focus on the top‑performing episodes—the “elite set”—which represent trajectories that achieved above‑average returns. By ranking episodes by total reward and choosing a threshold (for instance, the 70th percentile), we discard the lower‑performing ones.\n\nFitting\nWe treat the states and actions from elite episodes as supervised data: observations as inputs, the taken actions as “labels.” We then perform a gradient‑based update—typically via cross‑entropy (log‑likelihood) loss—to push the policy network toward reproducing these high‑reward behaviors.\n\nIteration\nAs the policy improves, more episodes exceed the threshold, shifting the boundary upward. Abrupt jumps in performance are smoothed out over many iterations, yielding a stable learning curve.\n\n::: {#7e94b422 .cell execution_count=3}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\nt = torch.FloatTensor([[[1,2,3], [4,5,6]], [[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]]])\nprint(t)\nprint(F.softmax(t, dim=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 7.,  8.,  9.],\n         [10., 11., 12.]],\n\n        [[13., 14., 15.],\n         [16., 17., 18.]]])\ntensor([[[6.1290e-06, 6.1290e-06, 6.1290e-06],\n         [6.1290e-06, 6.1290e-06, 6.1290e-06]],\n\n        [[2.4726e-03, 2.4726e-03, 2.4726e-03],\n         [2.4726e-03, 2.4726e-03, 2.4726e-03]],\n\n        [[9.9752e-01, 9.9752e-01, 9.9752e-01],\n         [9.9752e-01, 9.9752e-01, 9.9752e-01]]])\n```\n:::\n:::\n\n\n",
    "supporting": [
      "04-the-cross-entropy-method_files"
    ],
    "filters": [],
    "includes": {}
  }
}
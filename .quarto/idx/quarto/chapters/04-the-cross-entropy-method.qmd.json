{"title":"The cross entropy method","markdown":{"yaml":{"date":"last-modified"},"headingText":"The cross entropy method","containsRefs":false,"markdown":"\n\nWe will use the cross entropy method to solve the stick on a pole challenge. Solved means that we can balance the pole so long that the episode is clapped\n\n> Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n\nWhich is after 500 time steps.\n\nWe will solve it with a very simple net.\nIt has 4 observations: cart position, cart velocity, pole angle, pole angular velocity\nand two actions: move car left, move car right.\n```{python}\nimport torch.nn as nn\n\n# the net\nobs_size = 4\nn_actions = 2\nhidden_layer = 128\ncartpole_net = nn.Sequential(\n    nn.Linear(obs_size, hidden_layer),\n    nn.ReLU(),\n    nn.Linear(hidden_layer, n_actions),\n    nn.Softmax(dim=1),\n)\n```\n\n## Softmax\n\nA function that takes $n$ inputs and makes them all positive and into a probability distribution.\n\"Slice along dim\" = fix all dimensions except dim, and let dim vary.\nA slice along dim means: fix all indices except for the one at position dim, and let that one vary.\n\n## The cross-entropy method in practice\n\nSo with softmax we can make a fully fledged cartpole agent: It gets 4 inputs. Internally it usses its net to produce a probability distribution for the outputs and then it randomly chooses an action according to that distribution.\n\nThis means our agent is a policy-based agent, because the output of the nn is directly a policy $\\pi(a|s)$.\n\nSampling\nWe repeatedly let the agent interact with the environment to generate a diverse set of episodes. Each episode is a sequence of state–action pairs and the cumulative reward obtained.\n\nSelection\nNot all episodes are equally informative. We focus on the top‑performing episodes—the “elite set”—which represent trajectories that achieved above‑average returns. By ranking episodes by total reward and choosing a threshold (for instance, the 70th percentile), we discard the lower‑performing ones.\n\nFitting\nWe treat the states and actions from elite episodes as supervised data: observations as inputs, the taken actions as “labels.” We then perform a gradient‑based update—typically via cross‑entropy (log‑likelihood) loss—to push the policy network toward reproducing these high‑reward behaviors.\n\nIteration\nAs the policy improves, more episodes exceed the threshold, shifting the boundary upward. Abrupt jumps in performance are smoothed out over many iterations, yielding a stable learning curve.\n\n\n```{python}\nimport torch\nimport torch.nn.functional as F\n\nt = torch.FloatTensor([[[1,2,3], [4,5,6]], [[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]]])\nprint(t)\nprint(F.softmax(t, dim=0))\n```\n\n","srcMarkdownNoYaml":"\n# The cross entropy method\n\nWe will use the cross entropy method to solve the stick on a pole challenge. Solved means that we can balance the pole so long that the episode is clapped\n\n> Since the goal is to keep the pole upright for as long as possible, by default, a reward of +1 is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n\nWhich is after 500 time steps.\n\nWe will solve it with a very simple net.\nIt has 4 observations: cart position, cart velocity, pole angle, pole angular velocity\nand two actions: move car left, move car right.\n```{python}\nimport torch.nn as nn\n\n# the net\nobs_size = 4\nn_actions = 2\nhidden_layer = 128\ncartpole_net = nn.Sequential(\n    nn.Linear(obs_size, hidden_layer),\n    nn.ReLU(),\n    nn.Linear(hidden_layer, n_actions),\n    nn.Softmax(dim=1),\n)\n```\n\n## Softmax\n\nA function that takes $n$ inputs and makes them all positive and into a probability distribution.\n\"Slice along dim\" = fix all dimensions except dim, and let dim vary.\nA slice along dim means: fix all indices except for the one at position dim, and let that one vary.\n\n## The cross-entropy method in practice\n\nSo with softmax we can make a fully fledged cartpole agent: It gets 4 inputs. Internally it usses its net to produce a probability distribution for the outputs and then it randomly chooses an action according to that distribution.\n\nThis means our agent is a policy-based agent, because the output of the nn is directly a policy $\\pi(a|s)$.\n\nSampling\nWe repeatedly let the agent interact with the environment to generate a diverse set of episodes. Each episode is a sequence of state–action pairs and the cumulative reward obtained.\n\nSelection\nNot all episodes are equally informative. We focus on the top‑performing episodes—the “elite set”—which represent trajectories that achieved above‑average returns. By ranking episodes by total reward and choosing a threshold (for instance, the 70th percentile), we discard the lower‑performing ones.\n\nFitting\nWe treat the states and actions from elite episodes as supervised data: observations as inputs, the taken actions as “labels.” We then perform a gradient‑based update—typically via cross‑entropy (log‑likelihood) loss—to push the policy network toward reproducing these high‑reward behaviors.\n\nIteration\nAs the policy improves, more episodes exceed the threshold, shifting the boundary upward. Abrupt jumps in performance are smoothed out over many iterations, yielding a stable learning curve.\n\n\n```{python}\nimport torch\nimport torch.nn.functional as F\n\nt = torch.FloatTensor([[[1,2,3], [4,5,6]], [[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]]])\nprint(t)\nprint(F.softmax(t, dim=0))\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"dracula","number-sections":true,"output-file":"04-the-cross-entropy-method.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.29","bibliography":["../../quarto/references.bib"],"respect-user-color-scheme":true,"theme":{"dark":["darkly","../../quarto/theme-dark.scss"],"light":"flatly"},"theme-options":{"code-color":"#d63384","code-bg":"#ffffff"},"sidebar":{"style":"floating"},"code-copy":true,"date":"last-modified"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for “Deep Reinforcement Learning Hands-On” by Maxim Lapan",
    "section": "",
    "text": "Preface\nWelcome to my study notes for Deep Reinforcement Learning Hands-On by Maxim Lapan (Lapan 2024).\nI’m writing these notes as a companion to my own studies, and they serve two main purposes.\nFirst, I’ve expanded on parts of the book that I wanted to explore in more depth. While the overall structure of these notes follows the book’s chapter layout, the content within each chapter deviates somewhat - I’ve chosen to focus on areas that I found especially interesting or challenging, other parts I kind of omit. These notes are not intended as a stand-alone resource, so without the book, they may be hard to follow.\nSecond, I encountered issues with some of the code from the official GitHub repository - most notably, the first GAN example for Atari games didn’t work, which was a bit disheartening for beginner me. To address this, I’ve created my own version of the accompanying code. It mirrors the structure of the official GitHub repository, but only includes examples I’ve modified in some way. Sometimes, the changes are quite substantial - just to reflect my personal style more closely - and I explain the code here in these notes.\n\n\n\n\nLapan, Maxim. 2024. Deep Reinforcement Learning Hands-on. 3rd ed. Packt Publishing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "quarto/chapters/01-what-is-rl.html",
    "href": "quarto/chapters/01-what-is-rl.html",
    "title": "1  What Is Reinforcement Learning?",
    "section": "",
    "text": "I have nothing to add to this chapter and this page is just here to keep the chapter numbering aligned.\nHowever, I can really recommend Sutton and Barto (2018) for the theoretical side of reinforcement learning.\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html",
    "href": "quarto/chapters/02-gymnasium.html",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "",
    "text": "2.1 Creating an environment\nThe best starting point for working with environments in Gymnasium is the official documentation, currently1 maintained by the Farama Foundation. I highly recommend consulting it - it makes it much easier to understand the structure of the action and observation spaces.\nTo create an environment, use gym.make(), passing the name of the environment as a string. The available environments and their different versions can be found in the documentation.\n# === creating an environment ===\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nenv.action_space\n\nDiscrete(2)\nAccording to the documentation, the CartPole-v1 environment has 4 observations and 2 possible actions. Let’s confirm this by inspecting the action space:\n# === checking out the action_space ===\nenv.action_space\n\nDiscrete(2)\nThis confirms that we have two discrete actions {0, 1} (for CartPole and also generally the discrete actions are simply numbered starting from 0). Also note that in Gymnasium the action_space is usually fixed, i.e., independent of the state. We will see how to deal with changing action spaces when they matter.\nNow let’s check the observation space:\n# === checking out the observation_space ===\nenv.observation_space\n\nBox([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\nThe Box space represents a 4-dimensional continuous space, with lower and upper bounds for each dimension. The third component (the shape attribute) indicates that each observation is a vector of 4 values. The first and second components specify the lower and upper bounds for each of the 4 dimension of the observation, respectively. For more detail on what each dimension represents, refer to the documentation. According to Gymnasium, a Box is “a space that represents closed boxes in Euclidean space.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html#the-random-cartpole-agent",
    "href": "quarto/chapters/02-gymnasium.html#the-random-cartpole-agent",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "2.2 The random CartPole agent",
    "text": "2.2 The random CartPole agent\nLet’s take the first step towards building a real agent: a random agent that takes actions randomly at each time step.\nThe idea is simple: the agent randomly samples actions from the environment’s action space until the episode ends - either by failure (the pole falling, cart to far off) or by timeout (very unlikely for a random agent).\n\n# === the random agent ===\nimport gymnasium as gym\n\n\n1def run_random_episode(env: gym.Env) -&gt; float:\n2    env.reset()\n    total_reward = 0.0\n    done = False\n\n    while not done:\n3        action = env.action_space.sample()\n4        _, reward, terminated, truncated, _ = env.step(action)\n        total_reward += reward\n        done = terminated or truncated\n\n    return total_reward\n\n\n1\n\nuse type hints like env: gym.Env as lightweight documentation. You’ll see this used frequently\n\n2\n\nin this random agent, we don’t store the initial observation from env.reset() because the agent doesn’t need it. Normally you would see something like state, _ = env.reset()\n\n3\n\nreturns a sample (random) action for the current state of the environment\n\n4\n\nSimilarly to env.reset(), we ignore the new state returned by env.step(action)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure that you click on the code annotations for more infos.\nIf you’re feeling completely lost, check out this Gymnasium basics guide which explains more of the fundamentals\n\n\nLet’s see how well the random agent performs over a few episodes:\n\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nepisodes = 5\nfor episode in range(1, episodes + 1):\n1    print(f\"\\n=== Episode {episode}/{episodes} ===\")\n    reward = run_random_episode(env)\n    print(f\"Reward: {reward}\")\n\n\n1\n\nif this is your first time seeing Python’s f-strings, they are a great way to include code in text. For example, f”5+5 is {5+5}” evaluates to “5+5 is 10”\n\n\n\n\n\n=== Episode 1/5 ===\nReward: 18.0\n\n=== Episode 2/5 ===\nReward: 19.0\n\n=== Episode 3/5 ===\nReward: 13.0\n\n=== Episode 4/5 ===\nReward: 42.0\n\n=== Episode 5/5 ===\nReward: 19.0\n\n\nThe total reward in each episode corresponds to how long the pole stays balanced, because in CartPole-v1, the agent receives a reward of +1 for each step.\nObviously, it doesn’t perform well. Our overall goal is to create agents that improve these rewards using reinforcement learning techniques.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html#footnotes",
    "href": "quarto/chapters/02-gymnasium.html#footnotes",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "",
    "text": "This has been checked as of the summer of 2025↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html",
    "title": "3  Deep Learning with PyTorch",
    "section": "",
    "text": "3.1 PyTorch\nPyTorch is a library for deep learning. In this chapter, we’ll introduce its most core features, and we’ll cover everything else along the way later on.\nWe can import it with:\n# === importing PyTorch ===\nimport torch",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#tensors",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#tensors",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.2 Tensors",
    "text": "3.2 Tensors\nThe term ‘tensor’ can have different meanings depending on the field (see Wikipedia). In PyTorch, a tensor is essentially a multidimensional array with a lot of useful functionality built in. They are the fundamental data structure used for computations involving neural networks (NNs).\n\n3.2.1 Creating Tensors and Tensor Shapes\nWe can ‘lift’ a python list into the universe of tensors. For example, here we convert a list into a rank-1 tensor (aka a vector):\n\n# === a vector in PyTorch ===\ntorch.tensor([1, 2, 3])\n\ntensor([1, 2, 3])\n\n\nThe rank of a tensor indicates how many indices you need to specify to access a single element.\n\n# === ranks of tensors ===\n\n# a rank-1 tensor takes 1 index\nv = torch.tensor([1, 2, 3])\nprint(f\"Element at index 0: {v[0]}\")\n\n# a rank-1 tensor takes 2 indices\nm = torch.tensor([[1, 2], [3, 4]])\nprint(f\"Element at (0,1): {m[0][1]}\")\n\n# a rank-3 tensor takes 3 indices\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint(f\"Element at (0,1,0): {t[0][1][0]}\")\n\nElement at index 0: 1\nElement at (0,1): 2\nElement at (0,1,0): 3\n\n\nThe term ‘dimension’ is often used interchangeably with rank. This was a bit confusing for me initially! Let me briefly explain why the term ‘dimension’ is used:\nIt’s not the same as the ‘dimension’ used when discussing vectors (e.g., a 3-dimensional vector), which refers to the degrees of freedom within the underlying vector space. Instead, it refers to the tensor’s size as a discrete space - the rank represents the number of independent directions you can move between elements.\nI’ll stick to ‘rank’ when talking about tensors, and generally use the terminology from TensorFlow’s introduction to tensors.\nInstead of converting Python arrays into tensors, we can also create them directly:\n\n# === common tensor creation methods ===\nprint(\"\\n a rank-2 tensor full of 0s\")\nprint(torch.zeros((3, 2)))\n\nprint(\"\\n a rank-2 tensor full of 7s\")\nprint(torch.full((3, 2), 7))\n\nprint(\"\\n a rank-2 tensor of random integers [0-9]\")\nprint(torch.randint(0, 10, (3, 2)))\n\n\n a rank-2 tensor full of 0s\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\n a rank-2 tensor full of 7s\ntensor([[7, 7],\n        [7, 7],\n        [7, 7]])\n\n a rank-2 tensor of random integers [0-9]\ntensor([[7, 0],\n        [3, 5],\n        [5, 0]])\n\n\nThe tuple (3, 2) in the code above defines the tensor’s shape: how many axes it has and the size of each. So in this example, the first axis has size 3 and the second has size 2.\nWe can query the shape of a tensor with .shape.\n\n# === the shape of a tensor ===\nt = torch.zeros((3, 2))\nprint(t.shape)\n\ntorch.Size([3, 2])\n\n\nThe return type is torch.Size, which is PyTorch’s internal way of representing shape.\nA special case of tensors are scalars, rank-0 tensors, whose shape is \\(()\\).\n\n# === scalar tensor ===\nt = torch.tensor(2)\nprint(t)\nprint(t.shape)\n\ntensor(2)\ntorch.Size([])\n\n\n\n\n3.2.2 Indexing\nTo illustrate tensor indexing, we consider a tensor of shape (2, 3, 4) where each element’s value represents its coordinates in mathematical notation:\n\n# === index positions of tensors ===\n# each entry in this tensor shows it's coordinate in mathematical notation\nt = torch.tensor(\n    [\n        [[111, 112, 113, 114], [121, 122, 123, 124], [131, 132, 133, 134]],\n        [[211, 212, 213, 214], [221, 222, 223, 224], [231, 232, 233, 234]],\n    ]\n)\nprint(t)\nprint(f\"The element at mathematical coordinates (2,3,4) is: {t[1][2][3]}\")\n\ntensor([[[111, 112, 113, 114],\n         [121, 122, 123, 124],\n         [131, 132, 133, 134]],\n\n        [[211, 212, 213, 214],\n         [221, 222, 223, 224],\n         [231, 232, 233, 234]]])\nThe element at mathematical coordinates (2,3,4) is: 234\n\n\nSo, the order of the indices reflects the order of the axes in the shape. Therefore, t[0][1][2] refers to the element at index 0 along the first axis, index 1 along the second axis, and index 2 along the third axis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#linear-neural-nets",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#linear-neural-nets",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.3 Linear neural nets",
    "text": "3.3 Linear neural nets\nLet’s explore the simplest type of neural network: the linear neural network. The basic ideas used here apply to more complex ones too.\nTo work with neural networks in PyTorch, we import the nn module:\n\nimport torch.nn as nn\n\nA linear neural network consists of input and output nodes, where each input signal is multiplied by a weight and passed to an output node. Each output node also has a constant bias added to it.\nLet’s look at a little example.\n\n# === linear nn with custom parameters ===\n# create a linear layer\nlinear_nn = nn.Linear(3, 2)\n\n# manually set the weights\n1weights = torch.FloatTensor([[1, 0, 1], [0, 1, -1]])\n\n# manually set the bias\nbias = torch.FloatTensor([2, -3])\n\n# assign the weights and bias to the linear layer\n2linear_nn.weight = nn.Parameter(weights)\nlinear_nn.bias = nn.Parameter(bias)\n\n\n1\n\nneural networks in PyTorch require floating-point tensors. That’s why we used torch.FloatTensor to create the weights and biases\n\n2\n\na nn.Parameter is a tensor that is recognized as a learnable parameter of the neural network. We’ll come back to that later when we discuss stochastic gradient descent (Section 3.4)\n\n\n\n\nWhen we apply this network to to a vector \\((x,y,z)\\), the computation is equivalent to: \\[\n\\mathrm{linear\\_nn}(x,y,z) =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y \\\\ z\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ -3\n\\end{pmatrix}\n\\]\nFor example for the input \\((1,0,0)\\) we should get \\((3,-3)\\). We can check this by using .forward(x) to pass a tensor into the network:\n\n# === applying the network to an input ===\n1v = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)\n\n\n1\n\ninstead of explicitly using torch.FloatTensor, we can implicitly coerce PyTorch into creating the correct tensor type by specifying float literals (e.g., 1.0 instead of 1).\n\n\n\n\ntensor([ 3., -3.], grad_fn=&lt;ViewBackward0&gt;)\n\n\nWe get the expected result, but there’s an additional detail: grad_fn=&lt;ViewBackward0&gt;. This is the first indication of PyTorch’s internal gradient tracking mechanism - a core requirement for stochastic gradient descent (SGD), which we’ll explore in the next chapter. We’ll return uncover the mystery of ViewBackward0 in Section 3.5.5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#sec-stochastic-gradient-descent",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#sec-stochastic-gradient-descent",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.4 Stochastic Gradient Descent",
    "text": "3.4 Stochastic Gradient Descent\nWe can model the behaviour of a neural network as a function \\(f(x_i;\\theta)\\) that maps inputs to outputs, defined by a set of learnable parameters, denoted as \\(θ\\).\nAt its core, training a neural network means finding model parameters that minimize a loss function \\[\nL(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f(x_i;\\theta), y_i),\n\\]\nwhere:\n\n\\((x_i, y_i)\\) represents a training sample (input-output pair)\n\\(\\ell\\) measures prediction error for individual samples\n\\(L\\) represents the average loss across all \\(N\\) samples in a batch\n\\(f(x_i, \\theta)\\) is the network’s prediction given input \\(x_i\\) and parameters \\(\\theta\\).\n\nThis is called ‘stochastic’ gradient descent, because the \\(x_i,y_i\\) are sampled by some stochastic process.\nFor basic stochastic gradient descent, we try to minimise the loss by iteratively adjusting the parameters against the gradient: \\[\n\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L(\\theta),\n\\]\nwhere \\(\\eta\\) is the learning rate, which controls the step size of each update.\n\n3.4.1 Single Gradient Descent Step\nLet’s look at a simple example of such an update step, using a tiny linear neural network with just one input and one output. Its parameters are \\(\\theta = (w, b)\\). We’ll train it on a single sample \\((x,y)=(3,5)\\) - we ignore batches for now - using the squared error: \\[\nL(f(x;\\theta), y) = (f(x;\\theta) - y)^2\n\\]\nWe want to work out the gradient by hand to see which way it points for parameters \\(\\theta =(0,0)\\). The loss is: \\[\n\\begin{split}\nL((w,b)) &= (3w + b - 5)^2\\\\\n&= 9w^2 + 6w(b-5) + (b-5)^2\n\\end{split}\n\\]\nSo the gradient is \\[\n\\nabla_\\theta L =\n\\begin{pmatrix}\n18w + 6b - 30 \\\\\n6w + 2b - 10\n\\end{pmatrix}\n\\quad\n\\Rightarrow\n\\quad\n\\nabla_\\theta L(0,0) = \\begin{pmatrix}\n- 30 \\\\\n- 10\n\\end{pmatrix}\n\\]\nNow, letting PyTorch calculate the gradient, we get the same result (which feels a bit magical):\n\n# === one SGD step: calculating gradient ===\nimport torch.nn as nn\nimport torch\n\n# linear model: f(x) = 0.0 x + 0.0\nmodel_net = nn.Linear(1, 1)\n1model_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n# sample\nx = torch.tensor([3.0])\ny = torch.tensor([5.0])\n\n\n# loss function\ndef loss(prediction, target) -&gt; float:\n    return (prediction - target).pow(2)\n\n\n# gradient calculation\nL = loss(model_net.forward(x), y)\nL.backward()  # computes ∇L for each model parameter\n\n# this is how we access the components of ∇L\nprint(f\"∂L/∂w: {model_net.weight.grad}\")\nprint(f\"∂L/∂b: {model_net.bias.grad}\")\n\n\n1\n\nthe weight parameter is a tensor of shape (inputs, outputs) = (1, 1), which as a list is just [[w]].\n\n\n\n\n∂L/∂w: tensor([[-30.]])\n∂L/∂b: tensor([-10.])\n\n\nNote the nice terminology. We use .forward(x) to push a tensor x through our net. And with .backward() we propagate the loss through the net backwards: During the forward flow, the data enters the network and flows in one direction, from input layer to the output layer. During the backward pass, it calculates the gradients of the loss function with respect to all the learnable parameters (weights and biases) in the network. This backward pass is tracing the influence to the loss back through all the calculations that contributed to it. It’s in the opposite direction of the forward flow.\nTo complete the “one stochastic gradient descent step” we need to update the model parameters, by walking against the gradient:\n\n# === one SGD step: updating model parameters ===\n1learning_rate = 0.01  # η\n\n# θ ← θ - η ∇L(θ)\nnew_weight = model_net.weight - learning_rate * model_net.weight.grad\nnew_bias = model_net.bias - learning_rate * model_net.bias.grad\n\nprint(\"The new parameters are:\")\nprint(f\"weight: {new_weight}\")\nprint(f\"bias: {new_bias}\")\n\n\n1\n\nThe learning rate here is chosen somewhat arbitrarily. In practice, it has to be small enough to avoid overshooting but big enough to achieve progress.\n\n\n\n\nThe new parameters are:\nweight: tensor([[0.3000]], grad_fn=&lt;SubBackward0&gt;)\nbias: tensor([0.1000], grad_fn=&lt;SubBackward0&gt;)\n\n\nAnd the new parameters do perform better regarding the loss\n\n# === one SGD step: new vs old parameters ===\n\n# model with new parameters\nnew_model_net = nn.Linear(1, 1)\nnew_model_net.weight = nn.Parameter(new_weight)\nnew_model_net.bias = nn.Parameter(new_bias)\n\n# predictions\nold_prediction = model_net.forward(x)\nnew_prediction = new_model_net.forward(x)\n\n# losses\n1old_loss = loss(old_prediction, y).item()\nnew_loss = loss(new_prediction, y).item()\n\n\nprint(f\"loss old: {old_loss} &gt; loss new: {new_loss}\")\n\n\n1\n\nwe can use item() to get a number from a tensor that has one element\n\n\n\n\nloss old: 25.0 &gt; loss new: 16.0\n\n\n\n\n3.4.2 Stochastic GD and Deterministic GD\nLet’s extend our one-step example into a proper little stochastic gradient descent (SGD) routine. The goal is to train our model function \\(f(x;\\theta)\\) to approximate a target function \\(f(x;\\theta_*)\\), where the target parameters are \\(\\theta_* = (2,1)\\).\nThe premise is that we don’t know the target parameters - we can only draw samples from the target function. In our example, we sample the values of \\(x\\) uniformly from the interval \\([0,1)\\).\nNow, we set up a small program that performs gradient descent: \\[\n\\theta_{t+1} \\gets \\theta_t - \\eta \\nabla_{\\theta}L(f(x_t;\\theta_t),y_t),\n\\]\nwith \\(L(x,y) = (x - y)^2\\), \\(y_t = f(x_t, \\theta_*)\\), and \\(\\theta_0 = (0,0)\\).\n\n# === stochastic GD ===\ntype Parameter = tuple[float, float]\n\n\ndef stochastic_gradient_descent(steps, target_parameters, seed=0) -&gt; list[Parameter]:\n    \"\"\"performs SGD for a specified number of steps\"\"\"\n\n1    torch.manual_seed(seed)\n    learning_rate = 0.1  # η\n\n    # target function with given target_parameters\n    target_function = nn.Linear(1, 1)\n    target_function.weight = nn.Parameter(torch.tensor([[target_parameters[0]]]))\n    target_function.bias = nn.Parameter(torch.tensor(target_parameters[1]))\n\n    # model function initialized with (w,b) = (0,0)\n    model_net = nn.Linear(1, 1)\n    model_net.weight = nn.Parameter(torch.tensor([[0.0]]))\n    model_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n    def train(x, y):\n        # compute the loss gradient\n        prediction = model_net.forward(x)\n        loss = (prediction - y).pow(2)\n        loss.backward()\n\n        # walk the parameters against gradient\n        model_net.weight = nn.Parameter(\n            model_net.weight - learning_rate * model_net.weight.grad\n        )\n        model_net.bias = nn.Parameter(model_net.bias - learning_rate * model_net.bias.grad)\n\n    # perform training steps and record parameter trajectory\n    trajectory = []\n    for _ in range(steps):\n        # record θ\n        current_parameter = (\n            model_net.weight.data[0].item(),\n            model_net.bias.data.data[0].item(),\n        )\n        trajectory.append(current_parameter)\n\n        # generate sample\n        x = torch.rand((1, 1))\n        y = target_function.forward(x)\n\n        # train model network\n        train(x, y)\n\n    return trajectory\n\n\n# execute SGD\ntarget_parameters = (2.0, 1.0)\ntrajectory = stochastic_gradient_descent(200, target_parameters, seed=7)\n\n\nprint(f\"target parameters: {target_parameters}\")\n2print(f\"final θ = {trajectory[-1]}\")\n\n\n1\n\nit’s sometimes handy for debugging and demos to seed your random number generators (RNGs), as I’ve done here\n\n2\n\nin Python, a_list[-1] accesses the last element. Here, that’s our final parameter estimate\n\n\n\n\ntarget parameters: (2.0, 1.0)\nfinal θ = (1.9013594388961792, 1.0614407062530518)\n\n\nWe can see that the result of stochastic gradient descent gets us quite close to the target parameters.\nThis simple recipe:\n1. sample data\n2. compute loss\n3. update parameters\nis one of the core routines behind training neural networks, even in far more complex learning systems. Of course, there should be some theory justifying how we sample data and define the loss function.\nI want to push a little bit into this direction for our example - not too far, just enough to build some intuition. It might still be kind of abstract and not really palpable what is happening here.\nLet’s see what stochastic gradient descent does on average. To find out, we compute the expected loss: \\[\n\\begin{split}\nF(w,b) &= \\mathbb{E}_{X \\sim U(0,1)}[L(w,b)] \\\\\n&= \\mathbb{E}_{X\\sim U(0,1)}\\big[(wX + b - (2X +1))^2\\big] \\\\\n&= \\int_0^1 ((w-2)x + (b-1))^2 \\mathrm{d}x\\\\\n&= \\frac{(w-2)^2}{3} + (w-2)(b-1) + (b-1)^2\n\\end{split}\n\\]\nand its gradient \\[\n\\nabla F(w,b) =\n\\begin{pmatrix}\n\\frac{2}{3}(w-2) + (b-1)\\\\\n(w-2) + 2(b-1)\n\\end{pmatrix}\n\\]\nUsing this expected gradient removes the randomness - that’s why it’s called deterministic gradient descent. Of course, in practice, we don’t have access to this expectation.\nWe can write the deterministic GD update rule as: \\[\n\\begin{pmatrix}\nw_{t+1}\\\\\nb_{t+1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}\n- \\eta\n\\underbrace{\n\\begin{pmatrix}\n\\frac{2}{3} & 1 \\\\\n1 & 2\n\\end{pmatrix}\n}_{\\mathbf{A}}\n\\left(\n\\underbrace{\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}}_{\\mathbf{\\theta}} -\n\\underbrace{\\begin{pmatrix}\n2\\\\\n1\n\\end{pmatrix}}\n_{\\mathbf{\\theta}_*}\n\\right)\n\\]\nSo, there is some theory behind this that this converges when we let \\(\\eta\\) become sufficiently small, but we are aiming for intuition here. Let’s just visualise the vector field defined by the update rule \\(-\\mathbf{A} (\\theta - \\theta_*)\\), and see how both deterministic and stochastic GD move through parameter space.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(title, target_parameters, trajectory, label):\n    # ── Settings ───────────────────────────────────\n    w_star, b_star = target_parameters\n    A = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\n    lr = 0.1\n    num_steps = len(trajectory)\n\n    # ── Build grid for vector field ───────────────\n    w_vals = np.linspace(0, 3, 15)\n    b_vals = np.linspace(0, 2, 15)\n    W, B = np.meshgrid(w_vals, b_vals)\n    U = np.zeros_like(W)\n    V = np.zeros_like(B)\n\n    # Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\n    for i in range(W.shape[0]):\n        for j in range(W.shape[1]):\n            theta = np.array([W[i, j], B[i, j]])\n            grad = A.dot(theta - np.array([w_star, b_star]))\n            U[i, j] = -grad[0]\n            V[i, j] = -grad[1]\n\n    # ── Deterministic GD trajectory ──────────────\n    det_path = [(0.0, 0.0)]\n    for _ in range(num_steps):\n        w, b = det_path[-1]\n        grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n        det_path.append((w - lr * grad[0], b - lr * grad[1]))\n    w_det, b_det = zip(*det_path)\n\n    # from PyTorch\n    w_torch, b_torch = zip(*trajectory)\n\n    # ── Plot vector field + paths ────────────────\n    plt.figure(figsize=(8, 6))\n    plt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\n    plt.plot(w_det, b_det, \"o-\", label=\"deterministic GD\")\n    plt.plot(w_torch, b_torch, \".-\", label=label)\n    plt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\n    plt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\n    plt.xlabel(\"Weight $w$\")\n    plt.ylabel(\"Bias $b$\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_trajectory(\n    f\"SGD and GD Trajectories and Gradient Vector Field ({len(trajectory)} steps)\",\n    target_parameters,\n    trajectory,\n    \"stochastic GD\",\n)\n\n\n\n\n\n\n\n\n\nIt’s nice to see that even though the stochastic GD bumbles along the 2D-plane, it kind of follows the path of the deterministic GD.\n\n\n3.4.3 Batches\nSo far we’ve been training on one sample at a time. In reality, neural networks are designed to consume batches of inputs - and PyTorch has been quietly reshaping our single inputs into 1-element batches form behind the scenes.\nInstead of feeding tensors one by one, we can stack them and process them together:\n\n# === batched input ===\n\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[1.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([2.0]))\n\nbatch = torch.Tensor([[1.0], [2.0], [3]])\n\nmodel_net.forward(batch)\n\ntensor([[3.],\n        [4.],\n        [5.]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nThis forward call effectively maps the network across each row of the batch, giving the same result as looping over the elements.\nMore generally, if a network is designed for input shape s_in and returns outputs of shape s_out, then it actually consumes batches of inputs with shape (b, s_in) and return outputs with shape (b, s_out).\nSo, how is PyTorch treating unbatched samples as batches? It’s essentially reshaping a single-element tensor like [x] into [[x]] - a (1,1)-shaped tensor. This can be done manually using .unsqueeze(0):\n\n# === adding extra axis ===\nt = torch.tensor([1, 2])\nt.unsqueeze(0)\n\ntensor([[1, 2]])\n\n\nThis adds an extra axis of size 1. To remove such singleton dimensions, we can use .squeeze():\n\n# === removing axes of size 1 ==\nt = torch.randn(2, 1, 2, 1)\nprint(f\"before squeeze: {t.shape}\")\nt = t.squeeze()\nprint(f\"after squeeze: {t.shape}\")\n\nbefore squeeze: torch.Size([2, 1, 2, 1])\nafter squeeze: torch.Size([2, 2])\n\n\nBack to gradient descent. Our original loss function for gradient descent also included batching: \\[\nL(\\theta) = \\frac{1}{N} \\sum_{i = 1}^N \\ell(f(x_i);\\theta, y_i),\n\\]\nwhere \\(L\\) is the mean of \\(\\ell\\) over the batch. This reduces the noisiness of updates.\nLet’s modify our SGD to use batches:\n\n# === batch GD ===\ndef batch_gradient_descent(\n    batch_size, steps, target_parameters, seed=0\n) -&gt; list[Parameter]:\n    \"\"\"performs batch GD for a specified number of steps\"\"\"\n\n    torch.manual_seed(seed)\n    step_size = 0.1\n\n    # target function with given target_parameters\n    target_function = nn.Linear(1, 1)\n    target_function.weight = nn.Parameter(torch.tensor([[target_parameters[0]]]))\n    target_function.bias = nn.Parameter(torch.tensor([target_parameters[1]]))\n\n    # model function initialized with (w,b) = (0,0)\n    model_net = nn.Linear(1, 1)\n    model_net.weight = nn.Parameter(torch.tensor([[0.0]]))\n    model_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n    def train(x_batch, y_batch):\n        # compute the mean loss gradient\n        prediction_batch = model_net.forward(x_batch)\n1        mean_loss = (prediction_batch - y_batch).pow(2).mean()\n        mean_loss.backward()\n\n        # walk the parameters against gradient\n        model_net.weight = nn.Parameter(\n            model_net.weight - step_size * model_net.weight.grad\n        )\n        model_net.bias = nn.Parameter(model_net.bias - step_size * model_net.bias.grad)\n\n    # perform training steps and record parameter trajectory\n    trajectory = []\n    for _ in range(steps):\n        # record θ\n        current_parameter = (\n            model_net.weight.data[0].item(),\n            model_net.bias.data.data[0].item(),\n        )\n        trajectory.append(current_parameter)\n\n        # generate batch sample\n        x_batch = torch.rand((batch_size, 1))\n        y_batch = target_function.forward(x_batch)\n\n        # train model network\n        train(x_batch, y_batch)\n\n    return trajectory\n\n\n# execute batch GD\ntarget_parameters = (2.0, 1.0)\ntrajectory = batch_gradient_descent(20, 200, target_parameters)\n\n\nplot_trajectory(\n    f\"SGD and batch GD Trajectories and Gradient Vector Field ({len(trajectory)} steps)\",\n    target_parameters,\n    trajectory,\n    \"batch GD\",\n)\n\n\n1\n\nthe code for computing the squared errors is identical to the single-sample version (except the mean at the end). Many scalar operations naturally extend to tensors\n\n\n\n\n\n\n\n\n\n\n\nThe batched version stays closer to the deterministic path. That’s expected - averaging reduces the variance. However, we’ve now done 200 steps with batches of 20, so in effect, we’ve increased the amount of data processed by a factor of 20.\nBut are we paying for that in compute? Let’s benchmark:\n\n\nCode\nimport time\nimport statistics\n\n\ndef run_experiment(\n    gradient_descent_type, *args, target_parameters=(2.0, 1.0), iterations=20\n):\n    \"\"\"\n    times performance and evaluates accuracy for stochastic or batch GD.\n    \"\"\"\n    start = time.time()\n\n    results = [\n        gradient_descent_type(*args, target_parameters, seed=i)[-1]\n        for i in range(iterations)\n    ]\n\n    elapsed_time = time.time() - start\n    squared_errors = [\n        (w - target_parameters[0]) ** 2 + (b - target_parameters[1]) ** 2\n        for (w, b) in results\n    ]\n    mean_squared_error = statistics.mean(squared_errors)\n\n    print(f\"time: {elapsed_time:.2f} seconds\")\n    print(f\"mean squared error: {mean_squared_error:.4f}\")\n\n\nsteps = 200\niterations = 20\nbatch_size = 20\n\nprint(f\"=== Comparison between SGD and batch GD ===\")\nprint(f\"{steps} steps averaged over {iterations} iterations\")\n\n# Run plain stochastic gradient descent (200 steps total)\nprint(\"\\n--- stochastic GD ---\")\nrun_experiment(stochastic_gradient_descent, 200)\n\n# Run batched gradient descent (20 batches of 200 steps total)\nprint(f\"\\n--- batch GD (batch size: {batch_size}) ---\")\nrun_experiment(batch_gradient_descent, 20, 200)\n\n\n=== Comparison between SGD and batch GD ===\n200 steps averaged over 20 iterations\n\n--- stochastic GD ---\ntime: 0.46 seconds\nmean squared error: 0.0094\n\n--- batch GD (batch size: 20) ---\ntime: 0.51 seconds\nmean squared error: 0.0086\n\n\nThe non-batched version runs faster, but not 20× faster. Maybe 10% or so. On the other hand, the batched version gives a lower mean squared error.\nChoosing a good batch size is part of what’s known as hyperparameter tuning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#gradients",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#gradients",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.5 Gradients",
    "text": "3.5 Gradients\nAlright, it’s time to see how PyTorch computes all the gradients we’ve been happily using in gradient descent. Let’s look at a simple example that performs some operations on a scalar tensor and computes its gradient (more precisely, its derivative in this case):\n\n# === calculating derivative ===\na = torch.tensor(3.0)\na.requires_grad = True\nb = a.pow(2)\nc = b * 5\n\nc.backward()\na.grad\n\ntensor(30.)\n\n\nWe needed to set requires_grad = True so that PyTorch would track the gradient of a. We haven’t seen this before because for network parameters (nn.Parameter), it’s automatically set to True.\n\n3.5.1 Why Bother\nBefore diving deeper, I’ll admit I wasn’t originally keen on writing this. It sounded technical, and the buzzwordy explanation that ‘PyTorch backpropagates gradients using the chain rule’ felt like enough to get a rough intuition. But after watching Karpathy (2016), I found it really interesting. This presentation of PyTorch’s gradient mechanics is very much inspired by that video. The author also gives their own take on ‘why bother learning what PyTorch does automatically for you’.\n\n\n3.5.2 Backpropagation\nPyTorch uses backpropagation to compute gradients. We’ll unpack how this works using the simple example above. The computation has the structure:\n\\(a \\gets 3\\)\n\\(b \\gets f(a)\\)\n\\(c \\gets  g(b)\\)\nWhat we want is the derivative \\((g \\circ f)'(a)\\), and we’ll compute it in a way that mirrors how backpropagation works. First, let’s visualise the computation of \\(c\\) - known as the forward pass - using a circuit graph, where each function is a ‘gate’:\n\n\n\nBackpropagation works by computing derivatives in the reverse direction, using the chain rule: \\[\n(g \\circ f)'(a) = g'(f(a))\\; f'(a).\n\\]\nOr, in Leibniz notation and using \\(b = f(a)\\) and \\(c = g(b)\\): \\[\n\\frac{\\partial c}{\\partial a} = \\frac{\\partial c}{\\partial b} \\;\\frac{\\partial b}{\\partial a}.\n\\]\nTo compute \\(\\frac{\\partial c}{\\partial a}\\), each gate uses the ‘local’ derivatives of its forward pass function \\(f'(a) = \\frac{\\partial b}{\\partial a}\\) and \\(g'(b) = \\frac{\\partial c}{\\partial b}\\), and the process starts with the identity derivative \\(\\frac{\\partial c}{\\partial c} = 1\\). The backward pass then has this structure:\n\\(\\frac{\\partial c}{\\partial c} \\gets 1\\)\n\\(\\frac{\\partial c}{\\partial b} \\gets \\frac{\\partial c}{\\partial c} \\cdot g'(b)\\)\n\\(\\frac{\\partial c}{\\partial a} \\gets \\frac{\\partial c}{\\partial b} \\cdot f'(a)\\)\nEach step follows from the chain rule. This is how we traverse the computation graph in reverse - and we can now augment the forward pass diagram with the backward pass:\n\n\n\n\n\nThat’s, in a nutshell, how backpropagation works - and how PyTorch computes gradients.\nFinally, let’s verify that this produces the same result of \\(30\\) as in our original example. For \\(f(a) = a^2\\), \\(g(b) = 5b\\) with \\(f'(a) = 2a\\), \\(g'(b) = 5\\), we get:\n\n\n\n\n\nIn practice, it’s a bit more complicated of course. For example, we have talked about derivatives but we need to compute gradients - that is, partial derivatives with respect to multiple variables. For example, if we have a function \\(f(x, y)\\), with gradient \\(\\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial x})\\) the backward pass through its gate looks like this (forward pass not shown).\n\n\n\nIn general, each gate in the backward pass receives an upstream gradient, multiplies it by its local gradients, and sends the results downstream.\n\n\n3.5.3 grad_fn\nFor PyTorch to find it’s way ‘back’ in the backpropagation under the hood, it dynamically builds a computational graph during the forward pass, storing the sequence of operations. Also, each non-leaf tensor stores a reference to the local gradient functions in the grad_fn attribute.\nWhen we call .backward(), PyTorch traverses that graph in reverse order, applying those gradient functions to propagate gradients back to the leaves, just as we’ve discussed.\nFor example, when we compute x + y, the resulting tensor stores grad_fn = &lt;AddBackward0&gt; - the ‘backward’ function that computes the gradient of the addition:\n\n# === the grad_fn attribute ===\nx = torch.tensor(1.0, requires_grad=True)\ny = torch.tensor(2.0, requires_grad=True)\n\nx + y\n\ntensor(3., grad_fn=&lt;AddBackward0&gt;)\n\n\nWe haven’t called .backward() here, so no gradients have been computed yet. But PyTorch has already set up the computation graph, so it’s ready to go as soon as one needs gradients with respect to x and y.\n\n\n3.5.4 Gradients and Control Structures\nBecause backpropagation uses the computational graph, it handles control structures just fine - as long as operations are traceable.\nHere’s an example where we compute \\(b = 10 \\cdot a\\) using a loop. PyTorch still gets the correct gradient for \\(\\frac{\\partial b}{\\partial a}\\):\n\n# === gradients calculation and control structures ===\na = torch.tensor(0.0, requires_grad=True)\n\nb = torch.tensor(0.0)\nfor i in range(10):\n    b += a\n\nb.backward()\nprint(f\"∂b/∂a: {a.grad}\")\n\n∂b/∂a: 10.0\n\n\n\n\n3.5.5 The mysterious of the viewBackward0\nNow that we understand backpropagation and how neural networks process batches of inputs, we can revisit something we encountered earlier when introducing linear neural networks.\nWhen we applied a linear neural network to some date we saw the attribute grad_fn=&lt;ViewBackward0&gt; popping up, like here\n\n# === unbatched input ===\n1linear_nn = nn.Linear(3, 2)\n\nv = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)  # grad_fn=&lt;ViewBackward0&gt;\n\n\n1\n\nif we initialize a network it’s parameters are randomly distributed\n\n\n\n\ntensor([0.0356, 0.6774], grad_fn=&lt;ViewBackward0&gt;)\n\n\nSo what’s going on?\nWe know that grad_fn stores the backward function of the final operation. In this case, apparently a view, i.e., an operation that returns a tensor on the same data but a different shape.\nThe code for handling linear layers is written in C++ and lies under aten/src/ATen/native/Linear.cpp in the PyTorch repository (at least, I think that’s the relevant code; I’m not 100% sure). The relevant part, which is called when the input is not a rank-2 tensor with some comments by me (I’m not 100% sure about the comments either)::\nstatic inline Tensor _flatten_nd_linear(const Tensor& input, const Tensor& weight, const Tensor& bias) {\n    // get the sizes of the input tensor\n    const auto input_sizes = input.sym_sizes();\n    \n    // calculate the flattened rank size\n    c10::SymInt flattened_dim = 1;\n    for (int64_t i = 0, ndim = input_sizes.size(); i &lt; ndim - 1; ++i) {\n        flattened_dim = flattened_dim * input_sizes[i];\n    }\n    \n    // reshape the input tensor to flatten all but the last axis\n    auto inp_reshape = input.reshape_symint({flattened_dim, input_sizes.at(input_sizes.size() -1)});\n    \n    // perform the linear operation\n    const auto result = at::addmm(bias, inp_reshape, weight.t());\n    \n    // calculate the new size of the output tensor\n    auto new_size = input_sizes.slice(0, input_sizes.size() - 1);\n    c10::SymDimVector sizes_vec(new_size.begin(), new_size.end());\n    sizes_vec.push_back(result.sym_size(1));\n    \n    // reshape the output tensor to match the original input shape\n    return result.view_symint(sizes_vec);\n}\nSo basically, the linear networks expect inputs of dimension 2. If we provide a tensor without a batch dimension, PyTorch internally reshapes it, creating a view with an extra axis of size 1. Then it runs the computation, and finally reshapes the output back to having no batches.\nWhen we explicitly provide a batch, that internal reshaping isn’t necessary. The final computation is then matrix multiplication plus bias addition, whose gradient function shows up as grad_fn = &lt;AddmmBackward0&gt;:\n\n# === batched input ===\nlinear_nn = nn.Linear(3, 2)\nv = torch.tensor([1.0, 0.0, 0.0]).unsqueeze(0)\n\nlinear_nn.forward(v)  # grad_fn=&lt;AddmmBackward0&gt;\n\ntensor([[-0.1479, -0.1826]], grad_fn=&lt;AddmmBackward0&gt;)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#nn-building-blocks",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#nn-building-blocks",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.6 NN building blocks",
    "text": "3.6 NN building blocks\nUntil now, we have only worked with a single linear layer. We can connect layers so that they feed into each other using nn.Sequential\n\n# === combining networks ===\n1layer1 = nn.Linear(4, 125)\nlayer2 = nn.Linear(125, 125)\nlayer3 = nn.Linear(125, 2)\n\nsequential_nn = nn.Sequential(layer1, layer2, layer3)\n\n\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0]).unsqueeze(0)\n\n# Applying sequential_nn produces the same as...\nr_seq = sequential_nn(tensor)\n2print(f\"sequential_nn: {r_seq}\")\n# ... applying the individual layers in order\nr_con = layer3(layer2(layer1(tensor)))\nprint(f\"concatenated: {r_con}\")\n\n\n1\n\nWe have to make sure manually that our layers fit together. Otherwise, we might get something like this: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x126 and 125x2)\n\n2\n\nI haven’t used the forward function for the input but just applied the layer to the input directly. This is possible, because PyTorch’s neural networks are callable objects, which just invoke the the forward function\n\n\n\n\nsequential_nn: tensor([[ 0.4786, -0.5196]], grad_fn=&lt;AddmmBackward0&gt;)\nconcatenated: tensor([[ 0.4786, -0.5196]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nCombining three linear layers doesn’t make much sense actually, as it is essentially multiplying three matrices together, resulting in a single matrix. Therefore, in our example, we could have simply used a single layer with 4 inputs and 2 outputs.\nWith linear layers, we are essentially limited to affine linear transformations. [Very technically, this is not entirely true. I recall seeing a YouTube video where only linear layers were used, and the non-linearity came from floating-point imprecision. However, I can’t find it.]\n\n3.6.1 ReLU\nTo move beyond the linear world, we need to add some non-linear layers. A frequently used function used in layers is the ‘Rectified Linear Unit’ (ReLU), which is defined by this simple function: \\[\n\\mathrm{ReLU}(x) = \\max(0,x).\n\\]\nand get’s applied to each input.\nIn this example we can see that the third component gets clipped to 0.\n\n# === ReLU clips input ===\nlayer = nn.ReLU()\n\nlayer(torch.tensor([1, 0, -1]).unsqueeze(0))\n\ntensor([[1, 0, 0]])\n\n\nSo, if we are using a network like this, it cannot be simplified to just one layer:\n\n# === combining linear with relu ===\nsequential_nn = nn.Sequential(\n    nn.Linear(6, 125), nn.ReLU(), nn.Linear(125, 125), nn.ReLU(), nn.Linear(125, 2)\n)\n1print(sequential_nn)\n\n\n1\n\nPyTorch provides us with a quick overview of a neural network whenever we print it.\n\n\n\n\nSequential(\n  (0): Linear(in_features=6, out_features=125, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=125, out_features=125, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=125, out_features=2, bias=True)\n)\n\n\nWe will a network like this to solve the CartPole environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#example---gan-on-atari-images",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#example---gan-on-atari-images",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.7 Example - GAN on Atari images",
    "text": "3.7 Example - GAN on Atari images\nThis final example of in (Lapan 2024, chap. 3) is quite big and might look daunting. Although it may look intimidating initially, nearly half of it consists of neural network declarations, but there’s still plenty of new stuff. For starters, simply running the code, viewing the results in tensorboard, and identifying familiar elements from this chapter should be enough.\nI had to ‘repair’ the code, so it would run for me. You can that code under /code/chapter_03/03_atari_gan.py.\n\n3.7.1 Running\nI use my IDE to run the code, but you can also use the command:\npython3 code/chapter_03/03_atari_gan.py\nYou should see output similar to this in the console:\nA.L.E: Arcade Learning Environment (version 0.11.0+dfae0bd)\n[Powered by Stella]\nINFO:__main__:Iter 100 in 32.86s: gen_loss=5.530e+00, dis_loss=5.460e-02\nINFO:__main__:Iter 200 in 33.80s: gen_loss=7.217e+00, dis_loss=5.016e-03\nThe actual data produced will be saved in the /runs folder and can be viewed using TensorBoard.\n\n\n3.7.2 Viewing the Data\nIf TensorBoard is installed (it should if you installed the requirements.txt), you can run:\ntensorboard --logdir runs\nYou should see something like this in the console (don’t worry about the reduced feature set message):\nTensorFlow installation not found - running with reduced feature set.\n\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https://github.com/tensorflow/tensorboard/issues/4784\n\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\nClicking the link will open TensorBoard in your browser.\nIt liked watching the data flow into TensorBoard, observing the losses of the discriminator and generator fluctuate, and seeing the latest ‘phases’ the generator went through.\nHere are some of the final images created by the generator in my run (yes, they are generated at that size):\n\n\n\nSample images seen in tensorboard crated by of 03_atari_gan.py\n\n\n\n\n3.7.3 Discussing the code\nThe code defines two classes, Discriminator and Generator, which are the neural networks being trained. The Discriminator is presented with images from Atari games and images produced by the Generator. It is trained to distinguish between real and fake images. The Generator, on the other hand, creates images and is trained to convince the Discriminator that these images are real Atari game images.\nHere’s a little code snippet that I copied from 03_atari_gan.py with some comments. Just to show that it also follows the basic training recipe:\n# train generator\ngen_optimizer.zero_grad() # gradients have to reset before computing new ones\ndis_output_v = net_discr(gen_output_v) # sample data\ngen_loss_v = objective(dis_output_v, true_labels_v) # compute loss\ngen_loss_v.backward() # calculate gradient\ngen_optimizer.step() # update parameters\nThis snippet uses:\n\nOptimizer: This updates the parameters in the direction of the gradient.\nObjective: This calculates the loss function.\n\nWe will delve deeper into these in the next chapter.\n\n\n3.7.4 Discriminator vs Generator Loss\nFor fun, I’ve included the losses from my run here with some heavy smoothing to make the trends clearer. It seems that, in principle, when one loss goes up, the other goes down, which makes sense.\nNote that the losses are on completely different scales. I think this means that the discriminator is quite confident about identifying fake images. However, the generator keeps using that tiny bit of uncertainty in the discriminator to improve its output quality.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\n\n# adjust these paths to wherever you saved your CSVs\ndisc_csv = \"quarto/data/atari_gan_dis_loss.csv\"\ngen_csv = \"quarto/data/atari_gan_gen_loss.csv\"\n\n# load data\ndf_disc = pd.read_csv(disc_csv)\ndf_gen = pd.read_csv(gen_csv)\n\n# smooth the losses using Savitzky-Golay filter\nwindow_length = 31  # must be odd; larger = smoother\npolyorder = 3  # polynomial order for the filter\n\ndf_disc[\"Smoothed_Value\"] = savgol_filter(\n    df_disc[\"Value\"], window_length=window_length, polyorder=polyorder\n)\ndf_gen[\"Smoothed_Value\"] = savgol_filter(\n    df_gen[\"Value\"], window_length=window_length, polyorder=polyorder\n)\n\n\n# create plot\nfig, ax1 = plt.subplots(figsize=(8, 5))\n\n# left y-axis: Discriminator loss\nax1.plot(\n    df_disc[\"Step\"],\n    df_disc[\"Smoothed_Value\"],\n    color=\"tab:blue\",\n    label=\"Discriminator Loss\",\n)\nax1.set_xlabel(\"Training Step\")\nax1.set_ylabel(\"Discriminator Loss\", color=\"tab:blue\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\n# right y-axis: Generator loss\nax2 = ax1.twinx()\nax2.plot(\n    df_gen[\"Step\"], df_gen[\"Smoothed_Value\"], color=\"tab:orange\", label=\"Generator Loss\"\n)\nax2.set_ylabel(\"Generator Loss\", color=\"tab:orange\")\nax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n\n# title and layout\nplt.title(\"GAN Losses (Dual Axis)\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nKarpathy, Andrej. 2016. “CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1.” https://www.youtube.com/watch?v=i94OvYb6noo.\n\n\nLapan, Maxim. 2024. Deep Reinforcement Learning Hands-on. 3rd ed. Packt Publishing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html",
    "href": "quarto/chapters/04-the-cross-entropy-method.html",
    "title": "4  The Cross-Entropy Method",
    "section": "",
    "text": "4.1 Policies\nIn this chapter, we will explore the cross-entropy method. Although the name sounds sophisticated, the underlying idea is quite simple. The origin of the name is discussed in the appendix Section 4.14.2.\nWe will learn the method using the example of the CartPole environment.\nEpisodes in CartPole are truncated after 500 steps. We want to train an agent that can balance the pole for that long.\nIn CartPole, the state is represented by a 4-tuple of numbers.\n# === states in CartPole ===\nstate,_ = env.reset()\nstate\n\narray([ 0.02830805,  0.04183438, -0.02289853,  0.01228617], dtype=float32)\nWe don’t really need to know in detail what those numbers mean; the idea is to let the agent figure out what to do.\nThe action space consists of two discrete choices: left or right.\n# === actions of CartPole ===\nenv.action_space\n\nDiscrete(2)\nDiscrete(2) means that it expects 0 or 1 as actions (and again, we don’t really need to know which is which; the agent has to figure out which one to use in which situation)..\nWe want to build an agent with a neural network that models some kind of ‘internal belief’ about which action is best to take in each state. More precisely, the agent should produce a probability distribution over actions, depending on the current state. This distribution is the agent’s policy, written as \\[\n\\pi_\\theta(a\\mid s),\n\\]\nwhere \\(s\\) is the current state, \\(a\\) an action, and \\(\\theta\\) the neural network’s parameters (like weights and biases).\nConcretely, our agent’s neural network will output a policy vector for each state \\(s\\): \\[\n(\\pi_\\theta(a_1 \\mid s), \\pi_\\theta(a_2 \\mid s))\n\\]\nwhere \\(a_1\\) and \\(a_2\\) are the two possible actions in CartPole.\nTo create a neural network that returns a probability distribution, we need the softmax function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#softmax",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#softmax",
    "title": "4  The Cross-Entropy Method",
    "section": "4.2 Softmax",
    "text": "4.2 Softmax\nGiven an input vector \\(\\mathbf{x} \\in \\mathbb{R}^N\\)​, the softmax function returns a probability vector \\(\\mathrm{softmax}(\\mathbf{x})\\) defined by \\[\n\\mathrm{softmax}(\\mathbf{x})_i = \\frac{\\exp(\\mathbf{x}_i)}{\\sum_{j=1}^N \\exp(\\mathbf{x}_j)}.\n\\]\nSoftmax preserves the relative ordering of the components: if \\(\\mathbf{x}_i &lt; \\mathbf{x}_k\\)​, then \\(\\mathrm{softmax}(\\mathbf{x})_i &lt; \\mathrm{softmax}(\\mathbf{x})_k\\)\nPyTorch provides a Softmax layer:\n\n# === Softmax layer applied to a single vector ===\nimport torch.nn as nn\nimport torch\n\ntensor = torch.tensor([1.0, 2.0, 3.0])\n\nsoftmax = nn.Softmax()\nsoftmax(tensor)\n\n/home/jx/projects/notes_for_deep_rl_hands_on/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n\n\ntensor([0.0900, 0.2447, 0.6652])\n\n\nRunning this works, but we also get a warning\n\nImplicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument\n\nThat’s because softmax operates along a specific axis, and PyTorch wants us to be explicit about it. As we will be working with batches (which we almost always do when training), the data we feed into softmax will be a rank-2 tensor: the 0-axis for the batch and the 1-axis for the outputs of the previous layer. So, we want to apply softmax along this output, dim=1:\n\n# === Softmax layer applied to a batch ===\ntensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n\nsoftmax = nn.Softmax(dim=1)\n\nsoftmax(tensor)\n\ntensor([[0.0900, 0.2447, 0.6652],\n        [0.0900, 0.2447, 0.6652]])\n\n\nEach row is transformed independently, yielding a probability distribution.\nYou might notice that both rows give the same probabilities, even though the inputs are different. Softmax is invariant to the addition of a constant to all input elements. This is because adding a constant to each element results in the same factor appearing in both the numerator and the denominator of the softmax formula, which cancels out.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#design-of-the-agent",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#design-of-the-agent",
    "title": "4  The Cross-Entropy Method",
    "section": "4.3 Design of the Agent",
    "text": "4.3 Design of the Agent\nLet us design the agent for CartPole, which we will train later. It has two functions: policy, for getting the agent’s policy as calculated by its network, and sample_action, to sample an action for a single state according to the policy vector. For example, if the policy returns \\((0.6, 0.4)\\), the agent will choose action 0 with probability 0.6 and action 1 with probability 0.4. This stochasticity is kind of a feature of the agent; it helps ensure that it keeps exploring different actions.\nThe network architecture is as follows:\n\nInput layer: 4 units (CartPole state).\nHidden layer: 128 units with ReLU activation.\nOutput layer: 2 units (one score per action), followed by softmax to yield a probability vector.\n\n\n# === CartPole agent ===\nfrom torch.distributions import Categorical\nfrom numpy import ndarray\n\nState = ndarray  # a 4‑element NumPy array\nAction = int  # 0 or 1\n\n\nclass CartPoleAgent:\n    def __init__(self):\n        self.policy_network = nn.Sequential(\n            nn.Linear(4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2),\n            nn.Softmax(dim=1),\n        )\n\n    def policy(self, states: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Given a batch of states (shape [batch_size, 4]),\n        return a tensor of shape [batch_size, 2] giving\n        probability distributions over the two actions.\n        \"\"\"\n        return self.policy_network(states)\n\n    def sample_action(self, state: State) -&gt; Action:\n        \"\"\"\n        Sample an action according to the policy.\n        \"\"\"\n        # convert to tensor and add a batch axis\n1        state_tensor = torch.tensor(state, dtype=torch.float)\n        batch = state_tensor.unsqueeze(0)  # shape [1,4]\n\n        # compute action probabilities and remove batch axis\n        probs = self.policy(batch).squeeze(0)  # shape [2]\n\n        # create a categorical distribution and sample\n2        dist = Categorical(probs)\n        action = dist.sample()\n\n        return action.item()\n\n\n1\n\nWe can explicitly set the type of the elements in a tensor using dtype.\n\n2\n\nPyTorch’s Categorical distribution provides functionality to sample from a probability vector like [0.3,0.7] to obtain an action index according to its probability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#episodes-and-return",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#episodes-and-return",
    "title": "4  The Cross-Entropy Method",
    "section": "4.4 Episodes and Return",
    "text": "4.4 Episodes and Return\nCartPole is an episodic task, which means it has an end; either because the pole fell over or the time limit was reached.\nFormally, an episode \\(\\tau\\) is a sequence of states, actions and rewards: \\[\n\\tau = s_0, a_0, r_0, s_1, a_1,r_1 \\dots, s_T, a_T, r_T\n\\]\nessentially1 in the order of how they happen.\nLet’s write some code to generate an episode by running an agent in the environment.\n\n# === generating episodes ===\nfrom dataclasses import dataclass\n\nReward = float\n\n\n1@dataclass(frozen=True)\n2class Episode:\n    states: list[State]\n    actions: list[Action]\n    rewards: list[Reward]\n\n\ndef generate_episode(env, agent) -&gt; Episode:\n    states, actions, rewards = [], [], []\n\n    state, _ = env.reset()\n    states.append(state)\n\n3    while True:\n        action = agent.sample_action(state)\n        actions.append(action)\n\n        state, reward, terminated, truncated, _ = env.step(action)\n        rewards.append(reward)\n\n        if terminated or truncated:\n            return Episode(states, actions, rewards)\n\n        states.append(state)\n\n\n1\n\nDataclasses are very similar to a dictionaries. I them here because I think they are more readable. The argument frozen=True makes the dataclass immutable (though the lists themselves remain mutable). This helps with code clarity, conveying that Episode is a plain data object.\n\n2\n\nGenerally, I try to keep the code similar to the theoretical description. However, we split the states, actions, and rewards into three lists instead of one tuple \\((s_0, a_0, r_0, \\dots, s_T, a_T, r_T)\\). Anything else would just be mad.\n\n3\n\nInitially, I didn’t like ‘while True-loops’ when I started learning reinforcement learning. I was told they are poor practice. However, they do have an advantage: they allow us to write loops that are conditioned on variables that do not exist at the beginning of the loop, such as terminated here.\n\n\n\n\nNow we can instantiate an agent and let it run:\n\n# === run a single episode ===\nagent = CartPoleAgent()\nepisode = generate_episode(env, agent)\n\nprint(f\"The episode's states: {episode.states}\")\nprint(f\"The episode's actions: {episode.actions}\")\nprint(f\"The episode's rewards: {episode.rewards}\")\nprint(f\"Balanced pole for {len(episode.states)} steps\")\n\nThe episode's states: [array([-0.00352081,  0.02557276,  0.03008931, -0.03682047], dtype=float32), array([-0.00300935,  0.22025059,  0.0293529 , -0.3198601 ], dtype=float32), array([ 0.00139566,  0.02472317,  0.0229557 , -0.01806681], dtype=float32), array([ 0.00189012,  0.21950851,  0.02259437, -0.30341947], dtype=float32), array([ 0.00628029,  0.4143013 ,  0.01652598, -0.58889186], dtype=float32), array([ 0.01456632,  0.21895188,  0.00474814, -0.29104933], dtype=float32), array([ 0.01894536,  0.41400582, -0.00107285, -0.582231  ], dtype=float32), array([ 0.02722547,  0.6091428 , -0.01271747, -0.8752517 ], dtype=float32), array([ 0.03940833,  0.80443525, -0.0302225 , -1.1719056 ], dtype=float32), array([ 0.05549704,  0.9999368 , -0.05366061, -1.4739081 ], dtype=float32), array([ 0.07549577,  0.8055102 , -0.08313878, -1.1984566 ], dtype=float32), array([ 0.09160598,  0.61155665, -0.10710791, -0.9329457 ], dtype=float32), array([ 0.10383711,  0.80794793, -0.12576683, -1.2572742 ], dtype=float32), array([ 0.11999607,  1.004435  , -0.1509123 , -1.5865549 ], dtype=float32), array([ 0.14008477,  0.81139463, -0.1826434 , -1.3444854 ], dtype=float32)]\nThe episode's actions: [1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0]\nThe episode's rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nBalanced pole for 15 steps\n\n\nWhen we create a new agent like above, a neural network with random (but sensible) weights is generated. Thus, the agent’s policy is more or less uniform: \\[\n\\pi(a\\mid s)\\approx \\frac{1}{2} \\quad \\text{for }a \\in \\{0,1\\}\n\\]\nFor training we need to evaluate the agents performance on an episode.\nFor this particular problem, performance is easy to calculate; it is simply \\(T+1\\), the number of time steps the pole was balanced. More generally, in reinforcement learning, we use the rewards received after each action to measure performance. We define the return of an episode \\(\\tau\\) as: \\[\nR(\\tau) = r_0 + r_1 + \\dots + r_T,\n\\]\nThis form can be used for all kinds of reinforcement learning problems. Since in our case, the reward for each action is just 1.0, the return is just number of steps, \\(R(\\tau)=T+1\\). Here is a function that calculates the return:\n\n# === calculating return ==\nimport numpy as np\n\nReturn = float\n\n\ndef calculate_return(episode) -&gt; Return:\n    return float(np.sum(episode.rewards))\n\n\nprint(f\"Length: {len(episode.states)}\")\nprint(f\"Return: {calculate_return(episode)}\")\n\nLength: 15\nReturn: 15.0\n\n\nThe return is stochastic, even if the environment is deterministic (CartPole is deterministic up to the starting state), because our agent itself is stochastic as it samples actions from its policy distribution. For example, seeding the environment so that the initial states are the same…\n\n# === return is stochastic ===\ndeterministic_env = gym.make(\"CartPole-v1\")\ndeterministic_env.reset(seed=0)\nepisode1 = generate_episode(deterministic_env, agent)\ndeterministic_env.reset(seed=0)\nepisode2 = generate_episode(deterministic_env, agent)\n\nprint(episode1.states)\nprint(episode2.states)\n\nprint(episode1.actions)\nprint(episode2.actions)\n\n[array([0.03132702, 0.04127556, 0.01066358, 0.02294966], dtype=float32), array([ 0.03215253, -0.15399769,  0.01112257,  0.3189779 ], dtype=float32), array([ 0.02907258, -0.34927627,  0.01750213,  0.6151476 ], dtype=float32), array([ 0.02208706, -0.54463834,  0.02980508,  0.9132912 ], dtype=float32), array([ 0.01119429, -0.7401505 ,  0.0480709 ,  1.2151906 ], dtype=float32), array([-0.00360872, -0.5456805 ,  0.07237472,  0.93795   ], dtype=float32), array([-0.01452233, -0.74169976,  0.09113372,  1.2524687 ], dtype=float32), array([-0.02935633, -0.5478558 ,  0.11618309,  0.9896656 ], dtype=float32), array([-0.04031344, -0.74432504,  0.1359764 ,  1.3164638 ], dtype=float32), array([-0.05519994, -0.9408797 ,  0.16230568,  1.6484282 ], dtype=float32), array([-0.07401754, -0.74798495,  0.19527425,  1.4103975 ], dtype=float32)]\n[array([0.03132702, 0.04127556, 0.01066358, 0.02294966], dtype=float32), array([ 0.03215253, -0.15399769,  0.01112257,  0.3189779 ], dtype=float32), array([ 0.02907258, -0.34927627,  0.01750213,  0.6151476 ], dtype=float32), array([ 0.02208706, -0.15440318,  0.02980508,  0.3280281 ], dtype=float32), array([0.01899899, 0.04028206, 0.03636564, 0.04489136], dtype=float32), array([ 0.01980463, -0.15534198,  0.03726347,  0.34882256], dtype=float32), array([ 0.01669779, -0.35097355,  0.04423992,  0.65301913], dtype=float32), array([ 0.00967832, -0.15649468,  0.0573003 ,  0.37458852], dtype=float32), array([0.00654843, 0.03776852, 0.06479207, 0.10050905], dtype=float32), array([ 0.0073038 , -0.1582193 ,  0.06680226,  0.41290945], dtype=float32), array([ 0.00413941, -0.35422143,  0.07506044,  0.7258822 ], dtype=float32), array([-0.00294501, -0.5502966 ,  0.08957808,  1.0412139 ], dtype=float32), array([-0.01395095, -0.74648684,  0.11040237,  1.36062   ], dtype=float32), array([-0.02888068, -0.55290836,  0.13761477,  1.1044124 ], dtype=float32), array([-0.03993885, -0.74954504,  0.15970302,  1.4369103 ], dtype=float32), array([-0.05492975, -0.94623435,  0.18844122,  1.7749431 ], dtype=float32)]\n[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0]\n[0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n\n\n…shows that, although the initial states are identical, the agent’s random action sampling produces different episodes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#distribution-on-trajectories",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#distribution-on-trajectories",
    "title": "4  The Cross-Entropy Method",
    "section": "4.5 Distribution on Trajectories",
    "text": "4.5 Distribution on Trajectories\nThe agent’s intrinsic randomness can be beneficial; even an untrained policy will occasionally stumble upon sequences of good actions. We can visualise this by sampling many episodes and plotting the empirical distribution of returns:\n\n# === empirical distribution of untrained agent ===\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef benchmark_agent(env, agent, num_samples=250):\n    returns = []\n    for _ in range(num_samples):\n        episode = generate_episode(env, agent)\n        returns.append(calculate_return(episode))\n    return returns\n\n\nreturns = benchmark_agent(env, agent)\nax = sns.histplot(returns)\nax.set_xlabel(\"Returns\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWe see a tail of higher returns corresponding to episodes in which the agent did a better job of balancing.\nAs the number of samples grows, the empirical distribution approaches the true trajectory distribution (trajectory is a more general term than episode, but here they mean the same): \\[\np_\\theta(\\tau) := \\mathrm{Pr}_{\\mathrm{Env}, \\pi_\\theta}(\\tau)\n\\]\nwhich assigns each possible trajectory \\(\\tau\\) the probability that the environment \\(\\mathrm{Env}\\), under policy \\(\\pi_\\theta\\)​, will generate it. Thus, with sufficient sampling, the upper tail of our sampled returns reflects the best behaviour of our agent.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#basic-idea-behind-cross-entropy-method",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#basic-idea-behind-cross-entropy-method",
    "title": "4  The Cross-Entropy Method",
    "section": "4.6 Basic Idea Behind Cross-Entropy Method",
    "text": "4.6 Basic Idea Behind Cross-Entropy Method\nThe idea behind the training procedure for the cross-entropy method is to use the tail of the trajectory distribution for training:\n\nsample a batch of episodes to obtain an approximation of \\(p_\\theta(\\tau)\\)\nSelect the elite episodes, those whose returns is at least as good as the top fraction q of the sample. That is, we select episodes whose returns are at or above the q-quantile2 of the return distribution.\nperform stochastic gradient descent using a suitable loss function (which we still need to discuss) to update \\(\\theta \\to \\theta'\\), so that the new policy \\(\\pi_{\\theta'}(a \\mid s)\\) assigns higher probability to the state-action pairs that occurred in the elite episodes.\n\nWe now implement a function for the first two steps: generate & pick out those elite episodes.\n\n# === sampling elite episodes ==\ndef sample_elite_episodes(\n    env,\n    agent,\n    num_episodes: int,\n    quantile_threshold: float,\n) -&gt; tuple[list[State], list[Action]]:\n    \"\"\"\n    - Generate num_episodes episodes\n    - return 'elite' episodes  \n    (elite: return at or above the quantile_threshold)\n    \"\"\"\n    episodes = []\n    returns = []\n\n    for _ in range(num_episodes):\n        ep = generate_episode(env, agent)\n        episodes.append(ep)\n        returns.append(calculate_return(ep))\n\n    cutoff_return = np.quantile(returns, quantile_threshold)\n\n    elite_episodes = []\n\n1    for ep, ret in zip(episodes, returns):\n        if ret &gt;= cutoff_return:\n            elite_episodes.append(ep)\n\n    return elite_episodes\n\n\n1\n\nThe zip function produces the pointwise pairs of two lists. So, this loop goes through episodes and their respective returns.\n\n\n\n\nThe last remaining point is to figure out how to do the gradient descent. We can think of this as a classification problem: instead of classifying images to labels, here we want to classify states to actions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#classification",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#classification",
    "title": "4  The Cross-Entropy Method",
    "section": "4.7 Classification",
    "text": "4.7 Classification\nClassification is a branch of supervised learning, sometimes memetically referred to as “glorified curve fitting”.\nWe are given a dataset of N examples \\((\\mathbf{x}_n, y_n)_{n=1}^N\\). Each input vector \\(\\mathbf{x}_n \\in \\mathbb{R}^d\\) consists of \\(d\\) features, and each label \\(y_n \\in \\{1, \\dots ,K\\}\\) indicates the class index.\nOur goal is to learn a parametrised probabilistic model \\[\nf_\\theta(y \\mid \\mathbf{x})\n\\]\nwhich, given a new feature vector \\(\\mathbf{x}\\), outputs a probability distribution over the \\(K\\) possible classes.\nSo in our case the inputs \\(\\mathbf{x}\\) correspond to the states \\(s\\), the labels \\(y\\) to actions \\(a\\), and the conditional distribution \\(f_\\theta(y \\mid \\mathbf{x})\\) is the policy \\(\\pi_\\theta(a \\mid s)\\) that we want to improve.\nTo get an idea of classification more generally, let’s look at the classic Iris dataset, which contains measurements of three Iris species: Setosa, Versicolor, and Virginica. Each sample has four numerical features (sepal3 length, sepal width, petal length, petal width) and one of three target classes corresponding to the different species. Check out wikipedia if you’re curious what irises are.\nHere is some code for loading and displaying the dataset. There are no explanations for this code, as it isn’t central to the cross-entropy method. Also, for later code snippets in this section, I’ll only explain the parts relevant to this method.\n\n# === load the iris data set ===\nfrom sklearn import datasets\nimport pandas as pd\n\niris_df = pd.concat(datasets.load_iris(return_X_y=True, as_frame=True), axis=1)\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2\n\n\n\n\n150 rows × 5 columns\n\n\n\nThe dataset contains 150 entries and 5 columns. The first four columns are the features (sepal/petal × length/width), and the last column target contains the class labels: 0 for Setosa, 1 for Versicolor, and 2 for Virginica.\nFor training we need to split the date into a feature matrix X and label vector y. By convention, we use uppercase for the \\(N \\times d\\) feature matrix.\n\n# === split dataset into features and labels ===\nfeature_columns = [\n    \"sepal length (cm)\",\n    \"sepal width (cm)\",\n    \"petal length (cm)\",\n    \"petal width (cm)\",\n]\nX = torch.tensor(\n    iris_df[feature_columns].values,\n    dtype=torch.float32,\n)\ny = torch.tensor(iris_df[\"target\"].values, dtype=torch.long)\n\nWith that, we’re ready to set up a small neural network model for \\(f_\\theta(y \\mid \\mathbf{x})\\), which will classify the target labels. The model takes 4 inputs (one per feature) and outputs a probability vector over 3 classes:\n\n# === the iris-classifier model ===\nnet = nn.Sequential(\n    nn.Linear(4, 125),\n    nn.ReLU(),\n    nn.Linear(125, 3),\n    nn.Softmax(dim=1),\n)\n\nUntrained, its accuracy is roughly \\(\\frac{1}{3}\\), i.e., it gets about a third of the classifications right.\n\n# === accuracy of the untrained model ===\ndef check_accuracy(net, X, y):\n1    with torch.no_grad():\n        preds = torch.argmax(net(X), dim=1)\n        accuracy = (preds == y).float().mean().item()\n        return accuracy\n\naccuracy = check_accuracy(net, X, y)\nprint(f\"Accuracy: {accuracy*100:.2f}%\")\n\n\n1\n\nthis disables PyTorch’s automatic gradient tracking and makes calculations faster (I suppose). The ‘with’ syntax automatically turns it back on after the with-block.\n\n\n\n\nAccuracy: 33.33%\n\n\nWe optimise the network parameters using simple full-batch gradient descent (as opposed to stochastic gradient descent, which would use mini-batches). Each training epoch consists of four steps:\n\nforward pass: get the predictions of the model\ncompute mean loss: compute the mean loss \\(L\\) according to \\[\n\\ell(f_\\theta(\\cdot \\mid \\mathbf{x}_i), y_i) = - \\ln f_\\theta(y_i \\mid \\mathbf{x}_i),\n\\] which is called negative log likelihood (NLL). See Section 4.14.1 in the appendix for a discussion of why this loss makes sense.\nbackward pass: compute gradients of the loss with respect to \\(\\theta\\)\nparameter update: update parameters via gradient descent with learning rate \\(\\eta\\): \\(\\theta \\gets \\theta - \\eta \\nabla_\\theta L(\\theta)\\).\n\nLet’s train the model for a fixed number of epochs and then re-check the accuracy:\n\n# === train the iris-classifier ===\nn_epochs = 10\nlearning_rate = 0.1\n\nfor epoch in range(n_epochs):\n    # 1. forward pass\n    probs = net(X)  # batch of the probability vectors\n\n    # 2. compute mean loss\n    true_probs = probs[\n        torch.arange(probs.size(0)), y\n    ]  # probabilities for true classes\n    loss = -torch.log(true_probs).mean()\n\n    # 3. backward pass\n    loss.backward()\n\n    # 4. parameter update\n    with torch.no_grad():\n        for param in net.parameters():\n            param.data -= learning_rate * param.grad\n\n1    # zero gradients\n    net.zero_grad()\n\naccuracy = check_accuracy(net, X, y)\nprint(f\"Accuracy after training: {accuracy*100:.2f}%\")\n\n\n1\n\nThis is a technical step required by PyTorch. After calling backward(), the gradients are stored and will accumulate. Therefore, we need to reset them before the next update.\n\n\n\n\nAccuracy after training: 86.67%\n\n\nWe see that training has worked: the network’s accuracy improves compared to its untrained baseline.\nNote that in the code we have to actively zero the gradients. I think it’s actually safer to zero them at the beginning of the gradient descent step – just to rule out any strange bugs. So, that’s what we are going to do from now on.\nThat’s neural classification in a nutshell. Of course, there’s much more to real-world classification, such as how to detect and mitigate overfitting or underfitting. But in our setting, this is less of a concern; the environment always lets us generate fresh data, and it provides objective feedback on performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#optimizers",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#optimizers",
    "title": "4  The Cross-Entropy Method",
    "section": "4.8 Optimizers",
    "text": "4.8 Optimizers\nNow that we have the basic algorithmic structure for the cross-entropy method:\n\nSample episodes & filter for elite ones\ndo ‘classification’ on this data:\n\nforward pass\ncalculate loss\nbackward pass\nparameter update, according to learning rate \\(\\eta\\)\n\n\nlet’s talk about the parameter update step.\nKeeping a constant learning rate and manually updating parameters is a bit simplistic. There are many possible improvements, and much I don’t know. So, it’s best to use a good optimizer that handles gradient stepping for us. We will use Adam. You can check out Adam in the wikipedia article about stochastic gradient descent.\nSo, we get rid of this:\n    with torch.no_grad():\n        for param in net.parameters():\n            param.data -= learning_rate * param.grad\nand use Adam instead:\noptimizer = optim.Adam(\n    params=agent.policy_network.parameters(),\n    lr=LEARNING_RATE,\n)\n\n# main learning loop\n    # zero the gradient before learning\n    optimizer.zero_grad()\n    # 1. forward ...\n    # 2. compute loss ...\n    # 3. backward pass ...\n    # 4. update parameters using Adam\n    optimizer.step()\nIt would also make sense to use a prebuild loss function for the negative log likelihood, but there is some interaction between softmax and NLL that we have to discuss first. We do this below in Section 4.11. But before that, let’s finally finish implementing the cross-entropy method.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#implementation-of-cross-entropy-method",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#implementation-of-cross-entropy-method",
    "title": "4  The Cross-Entropy Method",
    "section": "4.9 Implementation of Cross-Entropy Method",
    "text": "4.9 Implementation of Cross-Entropy Method\nWe now have all the ingredients necessary to implement the cross-entropy method.\nFirst, we encapsulate all tuneable hyperparameters in a simple dataclass:\n\n# === hyperparameters for cross-entropy method ===\n@dataclass(frozen=True)\nclass TrainingParameters:\n    iterations: int = 20\n    batch_size: int = 10\n    quantile_threshold: float = 0.9\n    learning_rate: float = 0.005\n\nAnd here’s the cross-entropy method itself:\n\n# === implementation of cross-entropy method ===\nimport torch.optim as optim\n\n\ndef cross_entropy_method(\n    env,\n    agent,\n    config: TrainingParameters,\n):\n    # set up optimizer\n    optimizer = optim.Adam(\n        params=agent.policy_network.parameters(),\n        lr=config.learning_rate,\n    )\n\n1    # key metrics\n    loss_series = []\n    num_elite_episodes = []\n\n    # cross-entrpy method main loop\n    for i in range(config.iterations):\n        # zero gradient\n        optimizer.zero_grad()\n\n        # 1. sample a batch of episodes and select elite ones\n        episodes = sample_elite_episodes(\n            env,\n            agent,\n            config.batch_size,\n            config.quantile_threshold,\n        )\n\n        # 2. perform 'classification'\n        # split and flatten episodes into input and label datasets\n2        states = [state for ep in episodes for state in ep.states]\n        actions = [action for ep in episodes for action in ep.actions]\n3        states_np = np.array(states)\n        actions_np = np.array(actions)\n        states_tensor = torch.tensor(states_np, dtype=torch.float)\n        actions_tensor = torch.tensor(actions_np, dtype=torch.long)\n\n        # 2.1 forward pass\n        action_probs = agent.policy(states_tensor)\n\n        # 2.2 compute negative log-likelihood\n        elite_action_probs = action_probs[\n            torch.arange(len(actions)),\n            actions_tensor,\n        ]\n        loss = -torch.log(elite_action_probs).mean()\n\n        # 2.3 backward pass\n        loss.backward()\n\n        # 2.4 parameter update\n        optimizer.step()\n\n        # record statistics\n        loss_series.append(loss.item())\n        num_elite_episodes.append(len(episodes))\n\n    return loss_series, num_elite_episodes\n\n\n1\n\nIt’s generally a good idea to log metrics during training so you can monitor progress. Ideally, you’d report them live (e.g. via TensorBoard), rather than just returning them at the end like I’ve done here.\n\n2\n\nA pythonic way to flatten the list of episodes into a single list of states and actions using list comprehensions. These are essentially nested loops over the episodes and their respective states or actions.\n\n3\n\nConverting to NumPy first speeds up the conversion to tensors. If you pass PyTorch a raw list directly, it complains that the internal conversion is slow.\n\n\n\n\nAfter training, we want to benchmark the agent. For that, exploration is no longer necessary, so we switch to greedy action selection. For each state \\(s\\), we select the action \\[\n\\arg \\max_a \\pi_\\theta(a \\mid s).\n\\]\nThe following code trains a CartPole agent and benchmarks it. The code is somewhat messy and contains no new relevant bits, so you don’t need to focus on it – rather, look at the promising results:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef generate_greedy_episode(env, agent) -&gt; Episode:\n    states, actions, rewards = [], [], []\n\n    state, _ = env.reset()\n    states.append(state)\n\n    while True:\n        state_tensor = torch.tensor(state, dtype=torch.float32)\n        batch = state_tensor.unsqueeze(0)\n        probs = agent.policy(batch).squeeze(0)\n        action = int(torch.argmax(probs))\n\n        state, reward, terminated, truncated, _ = env.step(action)\n\n        actions.append(action)\n        rewards.append(reward)\n\n        if terminated or truncated:\n            return Episode(states, actions, rewards)\n\n        states.append(state)\n\n\ndef benchmark_greedy_agent(env, agent, num_samples=250):\n    returns = []\n    for _ in range(num_samples):\n        episode = generate_greedy_episode(env, agent)\n        returns.append(calculate_return(episode))\n    return returns\n\n\ndef run_experiments(env, agent, iterations, num_samples=50):\n    returns = []\n    total_losses, total_elites = [], []\n    running_total = 0\n\n    # initial benchmark before any training\n    returns.append((running_total, benchmark_greedy_agent(env, agent, num_samples)))\n\n    for it in iterations:\n        losses, elites = cross_entropy_method(\n            env, agent, TrainingParameters(iterations=it)\n        )\n        total_losses.extend(losses)\n        total_elites.extend(elites)\n\n        running_total += it\n\n        returns.append((running_total, benchmark_agent(env, agent, num_samples)))\n\n    return returns, total_losses, total_elites\n\n\nagent = CartPoleAgent()\niterations = [50, 75]\nnum_samples = 250\n\nreturns, total_losses, total_elites = run_experiments(\n    env, agent, iterations, num_samples=num_samples\n)\n\nfor total, ret in returns:\n    sns.histplot(ret, label=f\"After {total} training iterations\", alpha=0.5, kde=True)\n\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Returns (greedy actions) for {num_samples} samples\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe histograms of the returns gradually shift rightward, eventually piling up at the environment’s maximum return. You can train your own CartPole agent using the standalone code code/chapter_04/01_cartpole.py, but more on that later. Using this code, I found that the agent reaches optimal performance (reward = 500 for 250 episodes) after around 150 iterations, but sometimes it also takes considerably longer.\nEarlier, when motivating the cross-entropy method, I said we should take a sufficiently large sample to estimate the trajectory distribution \\(p_\\theta\\)​ in order to find elite episodes. Here, we used a batch size of just 10 episodes. Apparently, small batch sizes can work too; they lead to noisier updates but more frequent ones. I’m actually a big believer in ‘more learning, less sampling’.\nThat said, I’m not entirely sure under which conditions this procedure works reliably. We’ll later look at its limitations in Section 4.13. One key issue, I suspect, is that the method does not scale well when the reward signal is sparse.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#key-metrics",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#key-metrics",
    "title": "4  The Cross-Entropy Method",
    "section": "4.10 Key Metrics",
    "text": "4.10 Key Metrics\nDuring training, we usually want to monitor diagnostic metrics to spot any problems. Loss is usually one of them – although in reinforcement learning it isn’t always that informative. Another useful metric in our case is the number of elite episodes per batch.\nHere’s a plot of the loss and elite count over 12,800 iterations of training (parameters as before: batch size 10, threshold 0.9). I ran this separately, as it took a few hours.\n\n\n\nPlot of losses and elites over 12,800 iterations – much longer than actually needed for normal training. We see that in the long run, the loss keeps going down and overall, the number of elite episodes used for training reaches its maximum. (Ignore the word ‘total’ in the y-axis label; it shouldn’t be there, but I don’t want to run it again.)\n\n\nLet’s try to explain the behaviour of both graphs a bit.\n\nThe loss starts off at around 0.7 because, at the beginning, the agent assigns each action a probability of roughly 0.5, which results in a negative log-likelihood loss of \\(−\\ln⁡(0.5)=0.69\\).\nIt’s hard to see here, but for the first ~100 episodes there’s usually only one elite episode. That’s because the return distribution has a long tail at this stage.\nLater, as the agent improves, all episodes often achieve the same (maximal) return, so they all count as elite, and the elite count is 10. At this point, the agent is not learning any new behaviour any more. It’s just reinforcing its current behaviour.\nOccasionally, the elite count drops again. I’m not sure why. Perhaps the agent unlearns good behaviour, or maybe it’s just due to the exploratory behaviour from softmax sampling.\nGenerally, the loss decreases. Interpreted as classification, this means the agent is getting better at predicting which actions were taken in elite episodes. From an RL perspective, it means it’s favouring actions that tend to lead to good returns.\nThat said, the learning trajectory is a bit bumpy. That’s due to the small batch size. This results in a sort of random walk in the loss (though I’m not entirely sure why).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#sec-softmax-and-nll",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#sec-softmax-and-nll",
    "title": "4  The Cross-Entropy Method",
    "section": "4.11 Softmax and NLL",
    "text": "4.11 Softmax and NLL\nWe designed our network so that it produces probabilites via the final softmax-layer: \\[\n\\mathrm{Softmax}(\\mathbf{x})_i = \\frac{\\exp(\\mathbf{x}_i)}{\\sum_j \\exp(\\mathbf{x}_j)}.\n\\]\nOften it is beneficial to work with the pure \\(\\mathbf{x}_i\\) comming from the penultimate layer. These are called ‘logits’.\nFor our loss function, we have to compute the negative logarithm of the softmax. The composition is called log-softmax: \\[\n\\mathrm{LogSoftmax}(\\mathbf{x})_i := \\ln(\\mathrm{Softmax}(\\mathbf{x})_i) = \\mathbf{x}_i - \\ln(\\sum_j \\exp(\\mathbf{x}_j))\n\\]\nThe implementation of this in PyTorch is ‘numerical stable’ – I have never actually seen it. That just means we should use this instead of doing our own thing, which is more likely to run into overflows/underflows.\nTo benefit from that, we have to change our network architecture to return pure logits, and in the training loop, we don’t calculate the negative log likelihood by hand any more but use the CrossEntropyLoss as the loss function, which computes \\(-\\mathrm{LogSoftmax}(\\mathbf{x})\\) for us. So it will look like this:\nloss_fn = nn.CrossEntropyLoss()\n\n# main learning loop\n    # zero gradients ...\n    # forward ...\n    # loss\n    loss_v = loss_fn(action_probs, actions_tensor)\n    # backward ...\n    # step ...\nUsing this architecture, we also have to keep in mind that whenever we are sampling actions from the network, we have to use the softmax to turn the logits into probabilities.\nThese changes are part of the final standalone implementation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#the-final-implementation",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#the-final-implementation",
    "title": "4  The Cross-Entropy Method",
    "section": "4.12 The Final Implementation",
    "text": "4.12 The Final Implementation\nYou can find the final implementation under code/chapter_04/01_cartpole.py. It is based on the code from Lapan (2024) but looks quite different at this point as it is more similar to the codebase that we have developed in these notes so far.\nHere are some key points:\n\nthere is no distinction between agent and network any more. Basically the agent is the policy and the policy is the network.\nwe log some key metrics loss, mean_return, the return_cutoff (where to cut of the ‘elite’ episodes) during training, and fraction_elites, which is the fraction of the sample that are elites\nwe stop training when the evaluation (using greedy action selection) shows that the agent can keep the pole balanced for the full 500 steps for 250 episodes in a row.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#sec-limitations-of-cross-entropy",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#sec-limitations-of-cross-entropy",
    "title": "4  The Cross-Entropy Method",
    "section": "4.13 Limitations of Cross-Entropy Method",
    "text": "4.13 Limitations of Cross-Entropy Method\nAs I mentioned earlier, it’s not clear to me when the cross-entropy method can be used effectively. However, the FrozenLake environment is a good example of where it struggles.\n\n4.13.1 The FrozenLake Environment\nIf you want an intuitive overview of the FrozenLake task, check out the the description in the official FrozenLake docs. Here, I’ll just explain what we need to know for training.\nLet’s set up the environment:\n\n# === create the FrozenLake environment ===\nenv = gym.make(\"FrozenLake-v1\")\n\nBoth the state (observation) space and the action space in FrozenLake are discrete:\n\n# === observations and action spaces of FrozenLake ===\nprint(\"observation space: \", env.observation_space)\nprint(\"action space: \", env.action_space)\n\nobservation space:  Discrete(16)\naction space:  Discrete(4)\n\n\nTo train a neural network on this environment, we’ll need to encode these observations in a suitable format. It’s not a good idea to feed states like 5, 9, or 12 directly into a neural network. They represent positions on a grid and should be treated as categorical values, not numeric ones.\nTo understand how to encode them, we first need to better understand how Gymnasium uses spaces to represent observations and actions.\n\n\n4.13.2 Spaces\nGymnasium represents observations and actions via spaces. For us important for now are:\n\nDiscrete(n): an integer in \\([0, n-1]\\)\nBox(low, high, shape, dtype): a continuous space where each element is a real number with a lower and upper bound. low and high are arrays (of the same shape) that define these bounds component-wise.\n\nFor example, in FrozenLake, observations and actions are represented by discrete spaces:\n\nobservation space – Discrete(16): The agent’s position on the 4×4 grid is encoded as an integer from 0 to 15.\naction space – Discrete(4): Four possible moves: left, down, right, up.\n\nFor CartPole, we had both discrete and continuous (Box) spaces:\n\nobservation space – Box([-4.8 -inf -0.41887903 -inf], [4.8 inf 0.41887903 inf], (4,), float32): The cart poles measurements:\n\nCart position (-4.8, 4.8)\nCart velocity (-inf, inf)\nPole angle (-0.42, 0.42)\nPole angular velocity (-inf, inf)\n\naction space – Discrete(4): Two possible moves: left, right\n\nIn CartPole, the observation is already a continuous vector and can be fed directly into a neural network. But in FrozenLake, the state is just a single integer (0 to 15), and each value represents a distinct grid cell. These values are categorical, so they are typically encoded as one-hot vectors before being passed to a neural network.\n\n\n4.13.3 One-Hot Encoding and Wrappers\nWhen observations are discrete labels, it is common to use one-hot encoding: converting a label \\(i\\) from a set of size \\(k\\) into a \\(k\\)-dimensional vector with a 1 in position \\(i\\) and 0s elsewhere. (Check out the wikipedia page on one-hot encoding for more background)\nGymnasium’s wrappers let us transform environments by intercepting and modifying what goes in or out of them (observations, actions, rewards, etc.). In this case, we use ObservationWrapper, which lets us modify observations returned by the environment. The official documentationsays about them:\n\nIf you would like to apply a function to only the observation before passing it to the learning code, you can simply inherit from ObservationWrapper and overwrite the method observation()to implement that transformation. The transformation defined in that method must be reflected by the env observation space. Otherwise, you need to specify the new observation space of the wrapper by setting self.observation_space in the __init__() method of your wrapper.\n\nThis is exactly what is done in DiscreteOneHotWrapper:\n\nSet self.observation_space to a Box to represent one-hot vectors.\nIn observation, convert each integer into a one-hot vector.\n\n\n# === wrapper for one-hot encoding ===\nimport gymnasium as gym\nimport numpy as np\n\n\n1class DiscreteOneHotWrapper(gym.ObservationWrapper):\n    def __init__(self, env: gym.Env):\n2        super(DiscreteOneHotWrapper, self).__init__(env)\n        assert isinstance(env.observation_space, gym.spaces.Discrete)\n3        shape = (env.observation_space.n,)\n        self.observation_space = gym.spaces.Box(\n4            0.0, 1.0, shape, dtype=np.float32\n        )\n\n    def observation(self, observation):\n        res = np.zeros(self.observation_space.shape, dtype=np.float32)\n        res[observation] = 1.0\n        return res\n\n\n1\n\nWe subclass ObservationWrapper\n\n2\n\nWe call the parent constructor using super(); this is standard inheritance in Python\n\n3\n\nWe set the Box shape to be (n,) where n is the size of the discrete observation space\n\n4\n\nIf we provide high and low as scalars, they will be used as unified bounds for all components of elements of the space.\n\n\n\n\nNow, if we wrap the environment, we get observations as one-hot vectors:\n\n# === observations and action spaces of wrapped FrozenLake ===\nenv = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\nprint(\"observation space: \", env.observation_space)\nstate, _ = env.reset()\nprint(\"starting state:\", state)\n\nobservation space:  Box(0.0, 1.0, (16,), float32)\nstarting state: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\nSo each observation is now a 16-dimensional float vector with a single 1 and the rest 0.\n\n\n4.13.4 Cross-Entropy Fails on FrozenLake\nLet’s see how the basic cross-entropy method does not improve the agent’s performance on FrozenLake. First, we define FrozenLakeAgent suited to our environment’s observation and action space:\n\n# === agent for FrozenLake ===\nclass FrozenLakeAgent:\n    def __init__(self):\n        self.policy_network = nn.Sequential(\n            nn.Linear(16, 128),\n            nn.ReLU(),\n            nn.Linear(128, 4),\n            nn.Softmax(dim=1),\n        )\n\n    def policy(self, states) -&gt; list[float]:\n        return self.policy_network(states)\n\n    def sample_action(self, state) -&gt; int:\n        state_tensor = torch.tensor(state)\n        states = state_tensor.unsqueeze(0)\n        probs = self.policy(states).squeeze()\n        dist = Categorical(probs)\n        action = dist.sample()\n        return action.item()\n\nNow let’s train and evaluate (batch size=10 and elites threashold=0.9):\n\n\nCode\nagent = FrozenLakeAgent()\niterations = [100, 1000, 2000]\nnum_samples = 250\n\nreturns, _, _ = run_experiments(env, agent, iterations, num_samples=num_samples)\n\nfor total, ret in returns:\n    values, counts = np.unique(ret, return_counts=True)\n    freqs = dict(zip(values, counts))\n    freq_0 = freqs.get(0, 0)\n    freq_1 = freqs.get(1, 0)\n    plt.bar(\n        [0, 1], [freq_0, freq_1], alpha=0.5, label=f\"After {total} training iterations\"\n    )\n\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Greedy Returns for {num_samples} samples on FrozenLake\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can’t see any real progress here. No bar really sticks out; they all have a success rate of ~1% plus or minus some statistical uncertainty.\nWe can also already see one issue here. Cross-entropy relies on the tail of the elite episodes in the reward distribution. Here, this tail is so thin that often the sample batch might not contain any elites.\n\n\n4.13.5 Problems for Cross-Entropy Method\nSo the basic cross-entropy method failed hard on FrozenLake. Here’s my take on why:\n\nNo progress signal: The agent only gets a reward of 1 for reaching the goal, with nothing to indicate making progress towards the goal. This leads to…\nToo few elite episodes: The success rate for a random agent is about 1%, so in a batch of 10 or 100, we often end up with all episodes having a return of 0. This is a problem if it happens too often because every time all episodes have the same return, the agent is trained on all the episodes, thus simply reinforcing its current behaviour.\nNoisy environment: The environment is highly stochastic. The agent only moves in the intended direction about a third of the time4. So elite episodes can be polluted by dumb but lucky behaviour.\n\nIn principle, I’d say the Cross-Entropy method just isn’t a good fit for environments like this and try something else.\n\n\n4.13.6 Tweaking Cross-Entropy Method\nHere are some ideas on how we can work around the problems from before:\n\nCreate a progress signal. For that we could do a couple of things\n\nReward shaping: We could give a positive reward to the agent after an episode is terminated (by falling into a hole or reaching the goal), which gets bigger the closer the agent has been to the goal. Reward shaping is not without its issues, though. This would create a preference in the agent for the holes. This is harmless on this map, but it could create behaviour where the agent prefers jumping into a hole close to the goal instead of trying to reach the goal if there is a high probability of falling into a less valued hole on the way.\nAdd discounting: The agent’s final reward is scaled by \\(\\gamma^{T−1}\\) for some \\(\\gamma &lt; 1\\) where \\(T\\) is the index of the last reward.[^This is not a general description of discounting.] This discourages long episodes. But again, discounting may shift the optimal behaviour. It can favour shorter but riskier paths.\nBoth?: We can try doing both. This also increases the likelihood of unwanted behaviour (maybe we will train an agent that tries to kill itself as quickly as possible to maximise local reward).\n\nEnsure elite episodes: We can increase the batch size so much that we can be quite sure that there will be an elite episode, or we can skip training when all episodes have the same return.\nSample out the noise: If we train the agent slowly for a long time, suboptimal but lucky episodes should be less represented and thus not learned in the long run. So just decreasing the learning rate should help.\n\n\n\n4.13.7 Making Cross-Entropy work on FrozenLake\nDespite these problems, we can make the cross-entropy method still work on Frozen Lake, especially since it’s such a small problem. We use a simple approach of ensuring elite episodes and a small learning rate.\nWhen should we actually consider Frozen Lake to be solved? Since we have a stochastic environment, we can check if it is solved by checking the average return. A blog post about the Q-learning algorithm on FrozenLake shows that the optimal policy has a success rate of 82.4%. However, this is assuming unrestricted episode lengths. The Frozen Lake implementation has a limit on the number of steps after which an episode gets truncated. Basically, this makes our formulation of Frozen Lake not a Markov Decision Problem (MDP) any more, but these are details we don’t worry about here. As we will see in the next chapter, with this constraint, the ‘optimal’ policy has a success rate of around ~74%.\nWe can actually achieve this performance with the simplest approach: keep basic cross-entropy but ensure that we have elites by skipping training if there are none. As this approach is quite different from the approach used by Lapan (2024) in 03_frozen_lake_tweaked.py, I decided to include the code as an extra file under code/chapter_04/x01_frozen_lake_solved.py.\nFor me, it was a surprise how well x01_frozen_lake_solved.py worked. With the right choice of batch size and learning rate, it can train an optimal agent in 5-20 minutes. In my experience, it feels like small batch sizes of 10 are faster than big batches. It also beats the more complex approach used by Lapan (2024) in speed and performance. Theirs reaches a final success rate of ~55%.\nHowever, when playing around with batch sizes and learning rates, it sometimes fails to get up to optimal performance and gets stuck at a lower success rate. That’s why it’s good to have an eye on the metrics in TensorBoard. You can start it with tensorboard --logdir=./runs/chapter_04 and see your runs there(old and current ones).\nLet’s look at some metrics that I recorded during a ‘typical’ run. Here I plot the training loss, and the mean return during evaluation. As mentioned, evaluation uses greedy action selection and here each evaluation step runs 2000 iterations.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom cycler import cycler\n\n\ndef plot_eval_metrics(\n    ret_csv: str,\n    loss_csv: str,\n    title: str,\n    ret_ylim=(0.0, 0.8),\n    loss_ylim=(1e-2, 2),\n    figsize=(10, 6),\n):\n    \"\"\"Plot mean return (left axis) and loss (right log-axis) for a single run.\"\"\"\n    # load\n    df_ret = pd.read_csv(ret_csv)\n    df_loss = pd.read_csv(loss_csv)\n\n    # make figure\n    fig, ax1 = plt.subplots(figsize=figsize)\n\n    # left axis: mean return\n    (l1,) = ax1.plot(df_ret.Step, df_ret.Value, label=\"Mean Return\")\n    ax1.set_xlabel(\"Step\")\n    ax1.set_ylabel(\"Mean Return\")\n    ax1.set_ylim(*ret_ylim)\n    ax1.tick_params(axis=\"y\")\n\n    # right axis: loss (log scale)\n    ax2 = ax1.twinx()\n    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n    # shift cycle so loss is a different color\n    ax2.set_prop_cycle(cycler(color=colors[1:] + colors[:1]))\n    (l2,) = ax2.plot(df_loss.Step, df_loss.Value, label=\"Loss (log)\")\n    ax2.set_yscale(\"log\")\n    ax2.set_ylabel(\"Loss (log scale)\")\n    ax2.set_ylim(*loss_ylim)\n    ax2.tick_params(axis=\"y\")\n\n    # combine legends\n    ax1.legend([l1, l2], [l1.get_label(), l2.get_label()], loc=\"best\")\n\n    plt.title(title)\n    plt.grid(True)\n    fig.tight_layout()\n    plt.show()\n\n\nplot_eval_metrics(\n    ret_csv=\"quarto/data/frozen_lake_20250715-12_40_44_lr0.0005_batch100_eval_ret_mean.csv\",\n    loss_csv=\"quarto/data/frozen_lake_20250715-12_40_44_lr0.0005_batch100_loss.csv\",\n    title=\"Loss ond Evaluation Mean Return for batch=100, lr=0.0005\",\n    ret_ylim=(0.0, 0.8),\n    loss_ylim=(1e-1, 2),\n)\n\n\n\n\n\n\n\n\n\nThe agent reaches optimal greedy performance at around ~2250 steps. After that, the greedy policy is optimal, and any fluctuations in evaluation are due to stochasticity. Also, note that often the loss stagnates or even rises before improvements in the policy.\nIf we increase the learning rate, we can get faster results. One lucky run with the same batch size as before but a \\(\\eta = 0.005\\) was this, which needed about 2-3 minutes.\n\n\nCode\nplot_eval_metrics(\n    ret_csv=\"quarto/data/frozen_lake_20250715-12_32_47_lr0.005_batch100_eval_ret_mean.csv\",\n    loss_csv=\"quarto/data/frozen_lake_20250715-12_32_47_lr0.005_batch100_loss.csv\",\n    title=\"Loss ond Evaluation Mean Return (lucky run) for batch=100, lr=0.005\",\n    ret_ylim=(0.0, 0.8),\n    loss_ylim=(1e-2, 2),\n)\n\n\n\n\n\n\n\n\n\nHowever, it also increases the chance of ending up with suboptimal behaviour. This is another run with the same parameters as before but the agent too quickly learned suboptimal behaviour and didn’t recover from that.\n\n\nCode\nplot_eval_metrics(\n    ret_csv=\"quarto/data/frozen_lake_20250715-12_11_39_lr0.005_batch100_eval_ret_mean.csv\",\n    loss_csv=\"quarto/data/frozen_lake_20250715-12_11_39_lr0.005_batch100_loss.csv\",\n    title=\"Loss ond Evaluation Mean Return (suboptimal run) for batch=100, lr=0.005\",\n    ret_ylim=(0.0, 0.8),\n    loss_ylim=(1e-8, 2),\n)\n\n\n\n\n\n\n\n\n\nHere we can also see that not all spikes in loss need to lead to an improvement in the greedy policy. The last one does not translate to any improvement in the greedy policy.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#appendix",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#appendix",
    "title": "4  The Cross-Entropy Method",
    "section": "4.14 Appendix",
    "text": "4.14 Appendix\n\n4.14.1 Negative Log Likelihood and Maximum Likelihood Estimation\nIn classification problems, we are given a dataset of \\(N\\) examples \\({(\\mathbf{x}_n, y_n)}_{n=1}^N\\), where each \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) is a feature vector and \\(y_n \\in \\{1, \\dots, K\\}\\) is the corresponding class label. Our goal is to model the relationship between features and labels using a parametric model that defines a conditional probability distribution: \\[\nf_\\theta(y \\mid x),\n\\]\nwhere \\(\\theta\\) denotes the model’s parameters.\nWe now need to choose values for \\(\\theta\\) that make the model “fit” the data well. A standard approach is maximum likelihood estimation (MLE), which essentially is selecting the parameters that make the observed labels as probable as possible under the model.\nTo formalise this, we define the likelihood function: \\[\n\\mathcal{L}(\\theta) := \\prod_{n=1}^N f_\\theta(y_n \\mid x_n),\n\\]\nwhich quantifies how likely the entire dataset is in our model for parameters \\(\\theta\\). The maximum likelihood estimator (MLE) is the parameter value that maximises this likelihood: \\[\n\\hat{\\theta}_{\\mathrm{mle}} := \\arg \\max_\\theta \\mathcal{L}(\\theta) =  \\arg \\max_\\theta \\prod f_\\theta(y\\mid x)\n\\]\nThe hat on \\(\\hat{\\theta}\\) indicates that this is an estimate derived from data.\nIn practice, it’s more convenient to work with the log-likelihood, since taking logarithms turns the product into a sum: \\[\n\\log \\mathcal{L}(\\theta) = \\sum_{n=1}^N \\log f_\\theta(y_n \\mid \\mathbf{x}_n)\n\\]\nBecause the logarithm is strictly increasing, maximising the log-likelihood yields the same solution as maximising the original likelihood.\nIn machine learning we want to minimise a loss function. Therefore, we use the negative log-likelihood (NLL) instead: \\[\n\\mathrm{NLL}(\\theta) =\n- \\log \\mathcal{L}(\\theta)\n= -\\sum_{n=1}^N \\log p(\\mathbf{y}_n \\mid \\mathbf{x}_n, \\boldsymbol{\\theta})\n\\]\nSo the maximum likelihood estimator can also be written as: \\[\n\\arg \\min_\\theta \\mathrm{NLL}(\\theta) = \\hat{\\theta}_{\\mathrm{mle}}.\n\\]\nMinimising the NLL is equivalent to maximising the likelihood.\n\n\n4.14.2 Motivating Cross-Entropy Method using Cross-Entropy\nHere, I attempt to motivate the Cross-Entropy method with some theoretical machinery and one bit of unsatisfying hand-waving.\nThe formal derivation in Lapan (2024) is not understandable to me. The paper referenced in the derivation from Boer, Kroese, and Mannor (2005) focuses primarily on the cross-entropy method in stochastic optimization, with a final section on its application to reinforcement learning (RL) in tabular settings, i.e., a matrix of state-action probabilities. I don’t know how to adapt this approach for parameterized policies. In the corresponding paper, Mannor, Rubinstein, and Gat (2003) include a brief section on the cross-entropy method for parameterized policies, which does not contain any methodological details on how to do so. However, they suggest that their method can replace traditional gradient-based approaches, which we do employ in our formulation of the cross-entropy method. So this seems to go in another direction.\nIn summary, I cannot find in any of the cited sources a formal derivation of the cross-entropy method we are using that I understand. Therefore, what follows is a largely theoretical motivation with a crucial bit of hand waving. Due to this, the presentation is not entirely satisfying, as it does not provide guarantees on convergence or deeper insights into the method.\nThe goal of the cross-entropy method is to improve the performance of our current policy \\(\\pi_\\theta(a \\mid s)\\). Its trajectory distribution is \\(p_\\theta(\\tau)\\) and its average return is \\[\nJ(\\theta) := \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)]\n\\]\nFor some \\(\\gamma \\in \\mathbb{R}\\) consider the distribution that focuses on the ‘elite’ trajectories \\[\ng(\\tau) := \\frac{\\mathbb{I}(R(\\tau) \\geq \\gamma) p_\\theta(\\tau)}{\\ell},\n\\]\nwhere \\(\\mathbb{I}\\) is the indicator function and \\(\\ell\\) a normalisation factor ensuring \\(g(\\tau)\\) is a valid distribution: \\[\n\\ell = \\sum_{\\tau \\geq \\gamma} p_\\theta(\\tau).\n\\]\nWe assume that \\(\\ell &lt; 1\\), which means the current policy \\(\\pi_\\theta\\) produces at least one episode whose reward is less than \\(\\gamma\\).\nIt is intuitively clear that the average reward under \\(g\\) is better than under \\(p_\\theta\\): \\[\n\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)] &lt; \\mathbb{E}_{\\tau \\sim g(\\tau)}[R(\\tau)].\n\\]\nFor completeness sake, here is a derivation of this inequality: \\[\n\\begin{split}\nJ(\\theta) &= \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)]\n= \\sum_\\tau R(\\tau) p_\\theta(\\tau) \\\\\n&= \\sum_{\\tau \\geq \\gamma} R(\\tau) p_\\theta(\\tau) + \\sum_{\\tau &lt; \\gamma} R(\\tau) p_\\theta(\\tau) \\\\\n&&lt; \\sum_{\\tau \\geq \\gamma} R(\\tau) p_\\theta(\\tau) + \\gamma\\sum_{\\tau &lt; \\gamma} p_\\theta(\\tau) \\\\\n&= \\sum_{\\tau \\geq \\gamma} R(\\tau) p_\\theta(\\tau) + \\gamma (1 - \\sum_{\\tau \\geq \\gamma} p_\\theta(\\tau)) \\\\\n&= \\sum_{\\tau \\geq \\gamma} R(\\tau) p_\\theta(\\tau) + \\gamma (\\frac{1}{\\ell}\\sum_{\\tau \\geq \\gamma} p_\\theta(\\tau) - \\sum_{\\tau \\geq \\gamma} p_\\theta(\\tau)) \\\\\n&= \\sum_{\\tau \\geq \\gamma} R(\\tau) p_\\theta(\\tau) + \\sum_{\\tau \\geq \\gamma} \\gamma \\big(\\frac{1}{\\ell} - 1\\big) p_\\theta(\\tau) \\\\\n&\\leq \\sum_{\\tau \\geq \\gamma} R(\\tau) p_\\theta(\\tau) + \\sum_{\\tau \\geq \\gamma} R(\\tau) \\big(\\frac{1}{\\ell} - 1\\big) p_\\theta(\\tau) \\\\\n&= \\sum_{\\tau \\geq \\gamma} R(\\tau) \\frac{1}{\\ell} p_\\theta(\\tau)\n= \\mathbb{E}_{\\tau \\sim g(\\tau)}[R(\\tau)]\n\\end{split}\n\\]\nOf course, \\(g(\\tau)\\) does not necessarily lie within our parameterized family or may not even be achievable by any policy. However, we can attempt to approximate it as closely as possible within our model family.\nOne such proximity measure (not a measure or metric in the mathematical sense) between distributions is the Kullback-Leibler divergence: \\[\n\\begin{split}\n\\mathrm{KL}(p_1(x)\\parallel p_2(x)) &= \\mathbb{E}_{x \\sim p_1(x)} \\log \\frac{p_1(x)}{p_2(x)} \\\\\n&= \\mathbb{E}_{x \\sim p_1(x)}[\\log p_1(x)] - \\mathbb{E}_{x \\sim p_1(x)}[\\log p_2(x)]\n\\end{split}\n\\]\nThe goal is to find parameters \\(\\theta_\\mathrm{CE}\\) such that the distribution \\(p_{\\theta_\\mathrm{CE}}\\) is as close as possible to \\(g(τ)\\) in terms of KL divergence: \\[\n\\theta_{\\mathrm{CE}} = \\arg \\min_{\\theta'} \\mathrm{KL}(g \\parallel p_{\\theta'}).\n\\]\nThis step involves the hand-waving as it is not really motivated: Does this guarantee that we improve our policy if it was not optimal? Does it at least ensure that we do not deteriorate performance? I think the general answers to these questions are no, but I’m not sure.\nWe will just accept this step as an ‘educated guess’ and proceed.\nIf we expand \\(\\mathrm{KL}(g \\parallel p_{\\theta'})\\), we see that minimizing the KL divergence is equivalent to maximizing the right expectation of the KL: \\[\n\\theta_{\\mathrm{CE}} = \\arg \\max_{\\theta'} \\mathbb{E}_{\\tau \\sim g}[\\log p_{\\theta'}(\\tau)]\n\\]\nThis expectation \\(\\mathbb{E}_{x \\sim p_1(x)}[\\ln p_2(x)]\\) of the KL divergence is also called the cross entropy. So this is where the cross-entropy method gets its name from.\nNow, \\[\n\\begin{split}\n\\mathbb{E}_{\\tau \\sim g}[\\log p_{\\theta'}(\\tau)]\n&= \\sum_{\\tau} p_{\\theta'}(\\tau) \\cdot \\frac{\\mathbb{I}(R(\\tau) \\geq \\gamma) p_\\theta(\\tau)}{\\ell} \\\\\n&= \\frac{1}{\\ell}\\sum_{\\tau} \\big( \\mathbb{I}(R(\\tau) \\geq \\gamma) p_{\\theta'}(\\tau) \\big) \\cdot  p_\\theta(\\tau) \\\\\n&= \\frac{1}{\\ell}\\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[\\mathbb{I}(R(\\tau) \\geq \\gamma) \\log p_{\\theta'}(\\tau)]\n\\end{split}\n\\]\nyields \\[\n\\theta_{\\mathrm{CE}} = \\arg \\max_{\\theta'} \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}[\\mathbb{I}(R(\\tau) \\geq \\gamma) \\log p_{\\theta'}(\\tau)]\n\\]\nWe can’t calculate this expectation. But we can estimate it by sampling from \\(p_\\theta(\\tau)\\), ,i.e., generate a set of \\(N\\) sample episodes. Then we can estimate: \\[\n\\hat{\\theta}_{\\mathrm{CE}} = \\arg \\max_{\\theta'} \\frac{1}{N} \\sum_{R(\\tau) \\geq \\gamma} \\log p_{\\theta'}(\\tau)\n\\]\nWe can ignore the factor \\(\\frac{1}{N}\\). We can also factor \\(p_{\\theta'}(\\tau)\\) into the environment contribution and the policy contribution (I’m not doing the details here. It’s not particularly hard, but kind of verbose) which yields \\[\n\\hat{\\theta}_{\\mathrm{CE}} = \\arg \\max_{\\theta'} \\sum_{(s,a) \\in \\tau, R(\\tau) \\geq \\gamma} \\log \\pi_{\\theta'}(a \\mid s)\n\\]\nOr as we like to minimize something in RL: \\[\n\\hat{\\theta}_{\\mathrm{CE}} = -\\arg \\min_{\\theta'} \\sum_{(s,a) \\in \\tau, R(\\tau) \\geq \\gamma} - \\log \\pi_{\\theta'}(a \\mid s)\n\\]\nSo this concludes the motivation for the cross-entropy method. It would be nice to have some understanding of when this procedure produces a \\(\\hat{\\theta}_{\\mathrm{CE}}\\) that performs better than \\(\\theta\\), but this is sadly part of the hand-wavy bits. I hope that in the future, I will find a better theoretical analysis.\n\n\n\n\nBoer, P. T. de, D. P. Kroese, and S. Mannor. 2005. “A Tutorial on the Cross-Entropy Method.” Ann Oper Res 134: 19–67. https://doi.org/10.1007/s10479-005-5724-z.\n\n\nLapan, Maxim. 2024. Deep Reinforcement Learning Hands-on. 3rd ed. Packt Publishing.\n\n\nMannor, Shie, Reuven Rubinstein, and Yohai Gat. 2003. “The Cross Entropy Method for Fast Policy Search.” In Proceedings of the Twentieth International Conference on International Conference on Machine Learning, 512–19. ICML’03. Washington, DC, USA: AAAI Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#footnotes",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#footnotes",
    "title": "4  The Cross-Entropy Method",
    "section": "",
    "text": "I use the term essentially here because the reward and state are received simultaneously, so they could be ordered either way.↩︎\nThe q-quantile of a dataset is the value below which a fraction q of the data lies.↩︎\nSepals are the leaves that usually cover the flower petals. (In case, like me, you were wondering what they are.)↩︎\nIt’s a bit more nuanced than that. A good policy can also avoid going into bad direction which gives a 2/3 survival rate, but it’s still very stochastic overall.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Cross-Entropy Method</span>"
    ]
  }
]
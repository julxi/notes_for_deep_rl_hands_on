[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for “Deep Reinforcement Learning Hands-On” by Maxim Lapan",
    "section": "",
    "text": "Preface\nWelcome to my study notes for Deep Reinforcement Learning Hands-On by Maxim Lapan (Lapan 2024).\nI’m writing these notes as a companion to my own studies, and they serve two main purposes.\nFirst, I’ve expanded on parts of the book that I wanted to explore in more depth. While the overall structure of these notes follows the book’s chapter layout, the content within each chapter deviates somewhat - I’ve chosen to focus on areas that I found especially interesting or challenging, other parts I kind of omit. These notes are not intended as a stand-alone resource, so without the book, they may be hard to follow.\nSecond, I encountered issues with some of the code from the official GitHub repository - most notably, the first GAN example for Atari games didn’t work, which was a bit disheartening for beginner me. To address this, I’ve created my own version of the accompanying code. It mirrors the structure of the official GitHub repository, but only includes examples I’ve modified in some way. Sometimes, the changes are quite substantial - just to reflect my personal style more closely - and I explain the code here in these notes.\n\n\n\n\nLapan, Maxim. 2024. Deep Reinforcement Learning Hands-on. 3rd ed. Packt Publishing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "quarto/chapters/01-what-is-rl.html",
    "href": "quarto/chapters/01-what-is-rl.html",
    "title": "1  What Is Reinforcement Learning?",
    "section": "",
    "text": "I have nothing to add to this chapter and this page is just here to keep the chapter numbering aligned.\nHowever, I can really recommend Sutton and Barto (2018) for the theoretical side of reinforcement learning.\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html",
    "href": "quarto/chapters/02-gymnasium.html",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "",
    "text": "2.1 Creating an environment\nThe best starting point for working with environments in Gymnasium is the official documentation, currently1 maintained by the Farama Foundation. I highly recommend consulting it - it makes it much easier to understand the structure of the action and observation spaces.\nTo create an environment, use gym.make(), passing the name of the environment as a string. The available environments and their different versions can be found in the documentation.\n# === creating an environment ===\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nenv.action_space\n\nDiscrete(2)\nAccording to the documentation, the CartPole-v1 environment has 4 observations and 2 possible actions. Let’s confirm this by inspecting the action space:\n# === checking out the action_space ===\nenv.action_space\n\nDiscrete(2)\nThis confirms that we have two discrete actions {0, 1} (for CartPole and also generally the discrete actions are simply numbered starting from 0). Also note that in Gymnasium the action_space is usually fixed, i.e., independent of the state. We will see how to deal with changing action spaces when they matter.\nNow let’s check the observation space:\n# === checking out the observation_space ===\nenv.observation_space\n\nBox([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\nThe Box space represents a 4-dimensional continuous space, with lower and upper bounds for each dimension. The third component (the shape attribute) indicates that each observation is a vector of 4 values. The first and second components specify the lower and upper bounds for each of the 4 dimension of the observation, respectively. For more detail on what each dimension represents, refer to the documentation. According to Gymnasium, a Box is “a space that represents closed boxes in Euclidean space.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html#the-random-cartpole-agent",
    "href": "quarto/chapters/02-gymnasium.html#the-random-cartpole-agent",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "2.2 The random CartPole agent",
    "text": "2.2 The random CartPole agent\nLet’s take the first step towards building a real agent: a random agent that takes actions randomly at each time step.\nThe idea is simple: the agent randomly samples actions from the environment’s action space until the episode ends - either by failure (the pole falling, cart to far off) or by timeout (very unlikely for a random agent).\n\n# === the random agent ===\nimport gymnasium as gym\n\n\n1def run_random_episode(env: gym.Env) -&gt; float:\n2    env.reset()\n    total_reward = 0.0\n    done = False\n\n    while not done:\n3        action = env.action_space.sample()\n4        _, reward, terminated, truncated, _ = env.step(action)\n        total_reward += reward\n        done = terminated or truncated\n\n    return total_reward\n\n\n1\n\nuse type hints like env: gym.Env as lightweight documentation. You’ll see this used frequently\n\n2\n\nin this random agent, we don’t store the initial observation from env.reset() because the agent doesn’t need it. Normally you would see something like state, _ = env.reset()\n\n3\n\nreturns a sample (random) action for the current state of the environment\n\n4\n\nSimilarly to env.reset(), we ignore the new state returned by env.step(action)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure that you click on the code annotations for more infos.\nIf you’re feeling completely lost, check out this Gymnasium basics guide which explains more of the fundamentals\n\n\nLet’s see how well the random agent performs over a few episodes:\n\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nepisodes = 5\nfor episode in range(1, episodes + 1):\n1    print(f\"\\n=== Episode {episode}/{episodes} ===\")\n    reward = run_random_episode(env)\n    print(f\"Reward: {reward}\")\n\n\n1\n\nif this is your first time seeing Python’s f-strings, they are a great way to include code in text. For example, f”5+5 is {5+5}” evaluates to “5+5 is 10”\n\n\n\n\n\n=== Episode 1/5 ===\nReward: 18.0\n\n=== Episode 2/5 ===\nReward: 19.0\n\n=== Episode 3/5 ===\nReward: 13.0\n\n=== Episode 4/5 ===\nReward: 42.0\n\n=== Episode 5/5 ===\nReward: 19.0\n\n\nThe total reward in each episode corresponds to how long the pole stays balanced, because in CartPole-v1, the agent receives a reward of +1 for each step.\nObviously, it doesn’t perform well. Our overall goal is to create agents that improve these rewards using reinforcement learning techniques.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html#footnotes",
    "href": "quarto/chapters/02-gymnasium.html#footnotes",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "",
    "text": "This has been checked as of the summer of 2025↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html",
    "title": "3  Deep Learning with PyTorch",
    "section": "",
    "text": "3.1 PyTorch\nPyTorch is a library for deep learning. In this chapter, we’ll introduce its most core features, and we’ll cover everything else along the way later on.\nWe can import it with:\n# === importing PyTorch ===\nimport torch",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#tensors",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#tensors",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.2 Tensors",
    "text": "3.2 Tensors\nThe term ‘tensor’ can have different meanings depending on the field (see Wikipedia). In PyTorch, a tensor is essentially a multidimensional array with a lot of useful functionality built in. They are the fundamental data structure used for computations involving neural networks (NNs).\n\n3.2.1 Creating Tensors and Tensor Shapes\nWe can ‘lift’ a python list into the universe of tensors. For example, here we convert a list into a rank-1 tensor (aka a vector):\n\n# === a vector in PyTorch ===\ntorch.tensor([1, 2, 3])\n\ntensor([1, 2, 3])\n\n\nThe rank of a tensor indicates how many indices you need to specify to access a single element.\n\n# === ranks of tensors ===\n\n# a rank-1 tensor takes 1 index\nv = torch.tensor([1, 2, 3])\nprint(f\"Element at index 0: {v[0]}\")\n\n# a rank-1 tensor takes 2 indices\nm = torch.tensor([[1, 2], [3, 4]])\nprint(f\"Element at (0,1): {m[0][1]}\")\n\n# a rank-3 tensor takes 3 indices\nt = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint(f\"Element at (0,1,0): {t[0][1][0]}\")\n\nElement at index 0: 1\nElement at (0,1): 2\nElement at (0,1,0): 3\n\n\nThe term ‘dimension’ is often used interchangeably with rank. This was a bit confusing for me initially! Let me briefly explain why the term ‘dimension’ is used:\nIt’s not the same as the ‘dimension’ used when discussing vectors (e.g., a 3-dimensional vector), which refers to the degrees of freedom within the underlying vector space. Instead, it refers to the tensor’s size as a discrete space - the rank represents the number of independent directions you can move between elements.\nI’ll stick to ‘rank’ when talking about tensors, and generally use the terminology from TensorFlow’s introduction to tensors.\nInstead of converting Python arrays into tensors, we can also create them directly:\n\n# === common tensor creation methods ===\nprint(\"\\n a rank-2 tensor full of 0s\")\nprint(torch.zeros((3, 2)))\n\nprint(\"\\n a rank-2 tensor full of 7s\")\nprint(torch.full((3, 2), 7))\n\nprint(\"\\n a rank-2 tensor of random integers [0-9]\")\nprint(torch.randint(0, 10, (3, 2)))\n\n\n a rank-2 tensor full of 0s\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\n a rank-2 tensor full of 7s\ntensor([[7, 7],\n        [7, 7],\n        [7, 7]])\n\n a rank-2 tensor of random integers [0-9]\ntensor([[7, 0],\n        [3, 5],\n        [5, 0]])\n\n\nThe tuple (3, 2) in the code above defines the tensor’s shape: how many axes it has and the size of each. So in this example, the first axis has size 3 and the second has size 2.\nWe can query the shape of a tensor with .shape.\n\n# === the shape of a tensor ===\nt = torch.zeros((3, 2))\nprint(t.shape)\n\ntorch.Size([3, 2])\n\n\nThe return type is torch.Size, which is PyTorch’s internal way of representing shape.\nA special case of tensors are scalars, rank-0 tensors, whose shape is \\(()\\).\n\n# === scalar tensor ===\nt = torch.tensor(2)\nprint(t)\nprint(t.shape)\n\ntensor(2)\ntorch.Size([])\n\n\n\n\n3.2.2 Indexing\nTo illustrate tensor indexing, we consider a tensor of shape (2, 3, 4) where each element’s value represents its coordinates in mathematical notation:\n\n# === index positions of tensors ===\n# each entry in this tensor shows it's coordinate in mathematical notation\nt = torch.tensor(\n    [\n        [[111, 112, 113, 114], [121, 122, 123, 124], [131, 132, 133, 134]],\n        [[211, 212, 213, 214], [221, 222, 223, 224], [231, 232, 233, 234]],\n    ]\n)\nprint(t)\nprint(f\"The element at mathematical coordinates (2,3,4) is: {t[1][2][3]}\")\n\ntensor([[[111, 112, 113, 114],\n         [121, 122, 123, 124],\n         [131, 132, 133, 134]],\n\n        [[211, 212, 213, 214],\n         [221, 222, 223, 224],\n         [231, 232, 233, 234]]])\nThe element at mathematical coordinates (2,3,4) is: 234\n\n\nSo, the order of the indices reflects the order of the axes in the shape. Therefore, t[0][1][2] refers to the element at index 0 along the first axis, index 1 along the second axis, and index 2 along the third axis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#linear-neural-nets",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#linear-neural-nets",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.3 Linear neural nets",
    "text": "3.3 Linear neural nets\nLet’s explore the simplest type of neural network: the linear neural network. The basic ideas used here apply to more complex ones too.\nTo work with neural networks in PyTorch, we import the nn module:\n\nimport torch.nn as nn\n\nA linear neural network consists of input and output nodes, where each input signal is multiplied by a weight and passed to an output node. Each output node also has a constant bias added to it.\nLet’s look at a little example.\n\n# === linear nn with custom parameters ===\n# create a linear layer\nlinear_nn = nn.Linear(3, 2)\n\n# manually set the weights\n1weights = torch.FloatTensor([[1, 0, 1], [0, 1, -1]])\n\n# manually set the bias\nbias = torch.FloatTensor([2, -3])\n\n# assign the weights and bias to the linear layer\n2linear_nn.weight = nn.Parameter(weights)\nlinear_nn.bias = nn.Parameter(bias)\n\n\n1\n\nneural networks in PyTorch require floating-point tensors. That’s why we used torch.FloatTensor to create the weights and biases\n\n2\n\na nn.Parameter is a tensor that is recognized as a learnable parameter of the neural network. We’ll come back to that later when we discuss stochastic gradient descent (Section 3.4)\n\n\n\n\nWhen we apply this network to to a vector \\((x,y,z)\\), the computation is equivalent to: \\[\n\\mathrm{linear\\_nn}(x,y,z) =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y \\\\ z\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ -3\n\\end{pmatrix}\n\\]\nFor example for the input \\((1,0,0)\\) we should get \\((3,-3)\\). We can check this by using .forward(x) to pass a tensor into the network:\n\n# === applying the network to an input ===\n1v = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)\n\n\n1\n\ninstead of explicitly using torch.FloatTensor, we can implicitly coerce PyTorch into creating the correct tensor type by specifying float literals (e.g., 1.0 instead of 1).\n\n\n\n\ntensor([ 3., -3.], grad_fn=&lt;ViewBackward0&gt;)\n\n\nWe get the expected result, but there’s an additional detail: grad_fn=&lt;ViewBackward0&gt;. This is the first indication of PyTorch’s internal gradient tracking mechanism - a core requirement for stochastic gradient descent (SGD), which we’ll explore in the next chapter. We’ll return uncover the mystery of ViewBackward0 in Section 3.5.5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#sec-stochastic-gradient-descent",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#sec-stochastic-gradient-descent",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.4 Stochastic Gradient Descent",
    "text": "3.4 Stochastic Gradient Descent\nWe can model the behaviour of a neural network as a function \\(f(x_i;\\theta)\\) that maps inputs to outputs, defined by a set of learnable parameters, denoted as \\(θ\\).\nAt its core, training a neural network means finding model parameters that minimize a loss function \\[\nL(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f(x_i;\\theta), y_i),\n\\]\nwhere:\n\n\\((x_i, y_i)\\) represents a training sample (input-output pair)\n\\(\\ell\\) measures prediction error for individual samples\n\\(L\\) represents the average loss across all \\(N\\) samples in a batch\n\\(f(x_i, \\theta)\\) is the network’s prediction given input \\(x_i\\) and parameters \\(\\theta\\).\n\nThis is called ‘stochastic’ gradient descent, because the \\(x_i,y_i\\) are sampled by some stochastic process.\nFor basic stochastic gradient descent, we try to minimise the loss by iteratively adjusting the parameters against the gradient: \\[\n\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L(\\theta),\n\\]\nwhere \\(\\eta\\) is the learning rate, which controls the step size of each update.\n\n3.4.1 Single Gradient Descent Step\nLet’s look at a simple example of such an update step, using a tiny linear neural network with just one input and one output. Its parameters are \\(\\theta = (w, b)\\). We’ll train it on a single sample \\((x,y)=(3,5)\\) - we ignore batches for now - using the squared error: \\[\nL(f(x;\\theta), y) = (f(x;\\theta) - y)^2\n\\]\nWe want to work out the gradient by hand to see which way it points for parameters \\(\\theta =(0,0)\\). The loss is: \\[\n\\begin{split}\nL((w,b)) &= (3w + b - 5)^2\\\\\n&= 9w^2 + 6w(b-5) + (b-5)^2\n\\end{split}\n\\]\nSo the gradient is \\[\n\\nabla_\\theta L =\n\\begin{pmatrix}\n18w + 6b - 30 \\\\\n6w + 2b - 10\n\\end{pmatrix}\n\\quad\n\\Rightarrow\n\\quad\n\\nabla_\\theta L(0,0) = \\begin{pmatrix}\n- 30 \\\\\n- 10\n\\end{pmatrix}\n\\]\nNow, letting PyTorch calculate the gradient, we get the same result (which feels a bit magical):\n\n# === one SGD step: calculating gradient ===\nimport torch.nn as nn\nimport torch\n\n# linear model: f(x) = 0.0 x + 0.0\nmodel_net = nn.Linear(1, 1)\n1model_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n# sample\nx = torch.tensor([3.0])\ny = torch.tensor([5.0])\n\n\n# loss function\ndef loss(prediction, target) -&gt; float:\n    return (prediction - target).pow(2)\n\n\n# gradient calculation\nL = loss(model_net.forward(x), y)\nL.backward()  # computes ∇L for each model parameter\n\n# this is how we access the components of ∇L\nprint(f\"∂L/∂w: {model_net.weight.grad}\")\nprint(f\"∂L/∂b: {model_net.bias.grad}\")\n\n\n1\n\nthe weight parameter is a tensor of shape (inputs, outputs) = (1, 1), which as a list is just [[w]].\n\n\n\n\n∂L/∂w: tensor([[-30.]])\n∂L/∂b: tensor([-10.])\n\n\nNote the nice terminology. We use .forward(x) to push a tensor x through our net. And with .backward() we propagate the loss through the net backwards: During the forward flow, the data enters the network and flows in one direction, from input layer to the output layer. During the backward pass, it calculates the gradients of the loss function with respect to all the learnable parameters (weights and biases) in the network. This backward pass is tracing the influence to the loss back through all the calculations that contributed to it. It’s in the opposite direction of the forward flow.\nTo complete the “one stochastic gradient descent step” we need to update the model parameters, by walking against the gradient:\n\n# === one SGD step: updating model parameters ===\n1learning_rate = 0.01  # η\n\n# θ ← θ - η ∇L(θ)\nnew_weight = model_net.weight - learning_rate * model_net.weight.grad\nnew_bias = model_net.bias - learning_rate * model_net.bias.grad\n\nprint(\"The new parameters are:\")\nprint(f\"weight: {new_weight}\")\nprint(f\"bias: {new_bias}\")\n\n\n1\n\nThe learning rate here is chosen somewhat arbitrarily. In practice, it has to be small enough to avoid overshooting but big enough to achieve progress.\n\n\n\n\nThe new parameters are:\nweight: tensor([[0.3000]], grad_fn=&lt;SubBackward0&gt;)\nbias: tensor([0.1000], grad_fn=&lt;SubBackward0&gt;)\n\n\nAnd the new parameters do perform better regarding the loss\n\n# === one SGD step: new vs old parameters ===\n\n# model with new parameters\nnew_model_net = nn.Linear(1, 1)\nnew_model_net.weight = nn.Parameter(new_weight)\nnew_model_net.bias = nn.Parameter(new_bias)\n\n# predictions\nold_prediction = model_net.forward(x)\nnew_prediction = new_model_net.forward(x)\n\n# losses\n1old_loss = loss(old_prediction, y).item()\nnew_loss = loss(new_prediction, y).item()\n\n\nprint(f\"loss old: {old_loss} &gt; loss new: {new_loss}\")\n\n\n1\n\nwe can use item() to get a number from a tensor that has one element\n\n\n\n\nloss old: 25.0 &gt; loss new: 16.0\n\n\n\n\n3.4.2 Stochastic GD and Deterministic GD\nLet’s extend our one-step example into a proper little stochastic gradient descent (SGD) routine. The goal is to train our model function \\(f(x;\\theta)\\) to approximate a target function \\(f(x;\\theta_*)\\), where the target parameters are \\(\\theta_* = (2,1)\\).\nThe premise is that we don’t know the target parameters - we can only draw samples from the target function. In our example, we sample the values of \\(x\\) uniformly from the interval \\([0,1)\\).\nNow, we set up a small program that performs gradient descent: \\[\n\\theta_{t+1} \\gets \\theta_t - \\eta \\nabla_{\\theta}L(f(x_t;\\theta_t),y_t),\n\\]\nwith \\(L(x,y) = (x - y)^2\\), \\(y_t = f(x_t, \\theta_*)\\), and \\(\\theta_0 = (0,0)\\).\n\n# === stochastic GD ===\ntype Parameter = tuple[float, float]\n\n\ndef stochastic_gradient_descent(steps, target_parameters, seed=0) -&gt; list[Parameter]:\n    \"\"\"performs SGD for a specified number of steps\"\"\"\n\n1    torch.manual_seed(seed)\n    learning_rate = 0.1  # η\n\n    # target function with given target_parameters\n    target_function = nn.Linear(1, 1)\n    target_function.weight = nn.Parameter(torch.tensor([[target_parameters[0]]]))\n    target_function.bias = nn.Parameter(torch.tensor(target_parameters[1]))\n\n    # model function initialized with (w,b) = (0,0)\n    model_net = nn.Linear(1, 1)\n    model_net.weight = nn.Parameter(torch.tensor([[0.0]]))\n    model_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n    def train(x, y):\n        # compute the loss gradient\n        prediction = model_net.forward(x)\n        loss = (prediction - y).pow(2)\n        loss.backward()\n\n        # walk the parameters against gradient\n        model_net.weight = nn.Parameter(\n            model_net.weight - learning_rate * model_net.weight.grad\n        )\n        model_net.bias = nn.Parameter(model_net.bias - learning_rate * model_net.bias.grad)\n\n    # perform training steps and record parameter trajectory\n    trajectory = []\n    for _ in range(steps):\n        # record θ\n        current_parameter = (\n            model_net.weight.data[0].item(),\n            model_net.bias.data.data[0].item(),\n        )\n        trajectory.append(current_parameter)\n\n        # generate sample\n        x = torch.rand((1, 1))\n        y = target_function.forward(x)\n\n        # train model network\n        train(x, y)\n\n    return trajectory\n\n\n# execute SGD\ntarget_parameters = (2.0, 1.0)\ntrajectory = stochastic_gradient_descent(200, target_parameters, seed=7)\n\n\nprint(f\"target parameters: {target_parameters}\")\n2print(f\"final θ = {trajectory[-1]}\")\n\n\n1\n\nit’s sometimes handy for debugging and demos to seed your random number generators (RNGs), as I’ve done here\n\n2\n\nin Python, a_list[-1] accesses the last element. Here, that’s our final parameter estimate\n\n\n\n\ntarget parameters: (2.0, 1.0)\nfinal θ = (1.9013594388961792, 1.0614407062530518)\n\n\nWe can see that the result of stochastic gradient descent gets us quite close to the target parameters.\nThis simple recipe:\n1. sample data\n2. compute loss\n3. update parameters\nis one of the core routines behind training neural networks, even in far more complex learning systems. Of course, there should be some theory justifying how we sample data and define the loss function.\nI want to push a little bit into this direction for our example - not too far, just enough to build some intuition. It might still be kind of abstract and not really palpable what is happening here.\nLet’s see what stochastic gradient descent does on average. To find out, we compute the expected loss: \\[\n\\begin{split}\nF(w,b) &= \\mathbb{E}_{X \\sim U(0,1)}[L(w,b)] \\\\\n&= \\mathbb{E}_{X\\sim U(0,1)}\\big[(wX + b - (2X +1))^2\\big] \\\\\n&= \\int_0^1 ((w-2)x + (b-1))^2 \\mathrm{d}x\\\\\n&= \\frac{(w-2)^2}{3} + (w-2)(b-1) + (b-1)^2\n\\end{split}\n\\]\nand its gradient \\[\n\\nabla F(w,b) =\n\\begin{pmatrix}\n\\frac{2}{3}(w-2) + (b-1)\\\\\n(w-2) + 2(b-1)\n\\end{pmatrix}\n\\]\nUsing this expected gradient removes the randomness - that’s why it’s called deterministic gradient descent. Of course, in practice, we don’t have access to this expectation.\nWe can write the deterministic GD update rule as: \\[\n\\begin{pmatrix}\nw_{t+1}\\\\\nb_{t+1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}\n- \\eta\n\\underbrace{\n\\begin{pmatrix}\n\\frac{2}{3} & 1 \\\\\n1 & 2\n\\end{pmatrix}\n}_{\\mathbf{A}}\n\\left(\n\\underbrace{\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}}_{\\mathbf{\\theta}} -\n\\underbrace{\\begin{pmatrix}\n2\\\\\n1\n\\end{pmatrix}}\n_{\\mathbf{\\theta}_*}\n\\right)\n\\]\nSo, there is some theory behind this that this converges when we let \\(\\eta\\) become sufficiently small, but we are aiming for intuition here. Let’s just visualise the vector field defined by the update rule \\(-\\mathbf{A} (\\theta - \\theta_*)\\), and see how both deterministic and stochastic GD move through parameter space.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(title, target_parameters, trajectory, label):\n    # ── Settings ───────────────────────────────────\n    w_star, b_star = target_parameters\n    A = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\n    lr = 0.1\n    num_steps = len(trajectory)\n\n    # ── Build grid for vector field ───────────────\n    w_vals = np.linspace(0, 3, 15)\n    b_vals = np.linspace(0, 2, 15)\n    W, B = np.meshgrid(w_vals, b_vals)\n    U = np.zeros_like(W)\n    V = np.zeros_like(B)\n\n    # Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\n    for i in range(W.shape[0]):\n        for j in range(W.shape[1]):\n            theta = np.array([W[i, j], B[i, j]])\n            grad = A.dot(theta - np.array([w_star, b_star]))\n            U[i, j] = -grad[0]\n            V[i, j] = -grad[1]\n\n    # ── Deterministic GD trajectory ──────────────\n    det_path = [(0.0, 0.0)]\n    for _ in range(num_steps):\n        w, b = det_path[-1]\n        grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n        det_path.append((w - lr * grad[0], b - lr * grad[1]))\n    w_det, b_det = zip(*det_path)\n\n    # from PyTorch\n    w_torch, b_torch = zip(*trajectory)\n\n    # ── Plot vector field + paths ────────────────\n    plt.figure(figsize=(8, 6))\n    plt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\n    plt.plot(w_det, b_det, \"o-\", label=\"deterministic GD\")\n    plt.plot(w_torch, b_torch, \".-\", label=label)\n    plt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\n    plt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\n    plt.xlabel(\"Weight $w$\")\n    plt.ylabel(\"Bias $b$\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_trajectory(\n    f\"SGD and GD Trajectories and Gradient Vector Field ({len(trajectory)} steps)\",\n    target_parameters,\n    trajectory,\n    \"stochastic GD\",\n)\n\n\n\n\n\n\n\n\n\nIt’s nice to see that even though the stochastic GD bumbles along the 2D-plane, it kind of follows the path of the deterministic GD.\n\n\n3.4.3 Batches\nSo far we’ve been training on one sample at a time. In reality, neural networks are designed to consume batches of inputs - and PyTorch has been quietly reshaping our single inputs into 1-element batches form behind the scenes.\nInstead of feeding tensors one by one, we can stack them and process them together:\n\n# === batched input ===\n\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[1.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([2.0]))\n\nbatch = torch.Tensor([[1.0], [2.0], [3]])\n\nmodel_net.forward(batch)\n\ntensor([[3.],\n        [4.],\n        [5.]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nThis forward call effectively maps the network across each row of the batch, giving the same result as looping over the elements.\nMore generally, if a network is designed for input shape s_in and returns outputs of shape s_out, then it actually consumes batches of inputs with shape (b, s_in) and return outputs with shape (b, s_out).\nSo, how is PyTorch treating unbatched samples as batches? It’s essentially reshaping a single-element tensor like [x] into [[x]] - a (1,1)-shaped tensor. This can be done manually using .unsqueeze(0):\n\n# === adding extra axis ===\nt = torch.tensor([1, 2])\nt.unsqueeze(0)\n\ntensor([[1, 2]])\n\n\nThis adds an extra axis of size 1. To remove such singleton dimensions, we can use .squeeze():\n\n# === removing axes of size 1 ==\nt = torch.randn(2, 1, 2, 1)\nprint(f\"before squeeze: {t.shape}\")\nt = t.squeeze()\nprint(f\"after squeeze: {t.shape}\")\n\nbefore squeeze: torch.Size([2, 1, 2, 1])\nafter squeeze: torch.Size([2, 2])\n\n\nBack to gradient descent. Our original loss function for gradient descent also included batching: \\[\nL(\\theta) = \\frac{1}{N} \\sum_{i = 1}^N \\ell(f(x_i);\\theta, y_i),\n\\]\nwhere \\(L\\) is the mean of \\(\\ell\\) over the batch. This reduces the noisiness of updates.\nLet’s modify our SGD to use batches:\n\n# === batch GD ===\ndef batch_gradient_descent(\n    batch_size, steps, target_parameters, seed=0\n) -&gt; list[Parameter]:\n    \"\"\"performs batch GD for a specified number of steps\"\"\"\n\n    torch.manual_seed(seed)\n    step_size = 0.1\n\n    # target function with given target_parameters\n    target_function = nn.Linear(1, 1)\n    target_function.weight = nn.Parameter(torch.tensor([[target_parameters[0]]]))\n    target_function.bias = nn.Parameter(torch.tensor([target_parameters[1]]))\n\n    # model function initialized with (w,b) = (0,0)\n    model_net = nn.Linear(1, 1)\n    model_net.weight = nn.Parameter(torch.tensor([[0.0]]))\n    model_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n    def train(x_batch, y_batch):\n        # compute the mean loss gradient\n        prediction_batch = model_net.forward(x_batch)\n1        mean_loss = (prediction_batch - y_batch).pow(2).mean()\n        mean_loss.backward()\n\n        # walk the parameters against gradient\n        model_net.weight = nn.Parameter(\n            model_net.weight - step_size * model_net.weight.grad\n        )\n        model_net.bias = nn.Parameter(model_net.bias - step_size * model_net.bias.grad)\n\n    # perform training steps and record parameter trajectory\n    trajectory = []\n    for _ in range(steps):\n        # record θ\n        current_parameter = (\n            model_net.weight.data[0].item(),\n            model_net.bias.data.data[0].item(),\n        )\n        trajectory.append(current_parameter)\n\n        # generate batch sample\n        x_batch = torch.rand((batch_size, 1))\n        y_batch = target_function.forward(x_batch)\n\n        # train model network\n        train(x_batch, y_batch)\n\n    return trajectory\n\n\n# execute batch GD\ntarget_parameters = (2.0, 1.0)\ntrajectory = batch_gradient_descent(20, 200, target_parameters)\n\n\nplot_trajectory(\n    f\"SGD and batch GD Trajectories and Gradient Vector Field ({len(trajectory)} steps)\",\n    target_parameters,\n    trajectory,\n    \"batch GD\",\n)\n\n\n1\n\nthe code for computing the squared errors is identical to the single-sample version (except the mean at the end). Many scalar operations naturally extend to tensors\n\n\n\n\n\n\n\n\n\n\n\nThe batched version stays closer to the deterministic path. That’s expected - averaging reduces the variance. However, we’ve now done 200 steps with batches of 20, so in effect, we’ve increased the amount of data processed by a factor of 20.\nBut are we paying for that in compute? Let’s benchmark:\n\n\nCode\nimport time\nimport statistics\n\n\ndef run_experiment(\n    gradient_descent_type, *args, target_parameters=(2.0, 1.0), iterations=20\n):\n    \"\"\"\n    times performance and evaluates accuracy for stochastic or batch GD.\n    \"\"\"\n    start = time.time()\n\n    results = [\n        gradient_descent_type(*args, target_parameters, seed=i)[-1]\n        for i in range(iterations)\n    ]\n\n    elapsed_time = time.time() - start\n    squared_errors = [\n        (w - target_parameters[0]) ** 2 + (b - target_parameters[1]) ** 2\n        for (w, b) in results\n    ]\n    mean_squared_error = statistics.mean(squared_errors)\n\n    print(f\"time: {elapsed_time:.2f} seconds\")\n    print(f\"mean squared error: {mean_squared_error:.4f}\")\n\n\nsteps = 200\niterations = 20\nbatch_size = 20\n\nprint(f\"=== Comparison between SGD and batch GD ===\")\nprint(f\"{steps} steps averaged over {iterations} iterations\")\n\n# Run plain stochastic gradient descent (200 steps total)\nprint(\"\\n--- stochastic GD ---\")\nrun_experiment(stochastic_gradient_descent, 200)\n\n# Run batched gradient descent (20 batches of 200 steps total)\nprint(f\"\\n--- batch GD (batch size: {batch_size}) ---\")\nrun_experiment(batch_gradient_descent, 20, 200)\n\n\n=== Comparison between SGD and batch GD ===\n200 steps averaged over 20 iterations\n\n--- stochastic GD ---\ntime: 0.46 seconds\nmean squared error: 0.0094\n\n--- batch GD (batch size: 20) ---\ntime: 0.51 seconds\nmean squared error: 0.0086\n\n\nThe non-batched version runs faster, but not 20× faster. Maybe 10% or so. On the other hand, the batched version gives a lower mean squared error.\nChoosing a good batch size is part of what’s known as hyperparameter tuning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#gradients",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#gradients",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.5 Gradients",
    "text": "3.5 Gradients\nAlright, it’s time to see how PyTorch computes all the gradients we’ve been happily using in gradient descent. Let’s look at a simple example that performs some operations on a scalar tensor and computes its gradient (more precisely, its derivative in this case):\n\n# === calculating derivative ===\na = torch.tensor(3.0)\na.requires_grad = True\nb = a.pow(2)\nc = b * 5\n\nc.backward()\na.grad\n\ntensor(30.)\n\n\nWe needed to set requires_grad = True so that PyTorch would track the gradient of a. We haven’t seen this before because for network parameters (nn.Parameter), it’s automatically set to True.\n\n3.5.1 Why Bother\nBefore diving deeper, I’ll admit I wasn’t originally keen on writing this. It sounded technical, and the buzzwordy explanation that ‘PyTorch backpropagates gradients using the chain rule’ felt like enough to get a rough intuition. But after watching Karpathy (2016), I found it really interesting. This presentation of PyTorch’s gradient mechanics is very much inspired by that video. The author also gives their own take on ‘why bother learning what PyTorch does automatically for you’.\n\n\n3.5.2 Backpropagation\nPyTorch uses backpropagation to compute gradients. We’ll unpack how this works using the simple example above. The computation has the structure:\n\\(a \\gets 3\\)\n\\(b \\gets f(a)\\)\n\\(c \\gets  g(b)\\)\nWhat we want is the derivative \\((g \\circ f)'(a)\\), and we’ll compute it in a way that mirrors how backpropagation works. First, let’s visualise the computation of \\(c\\) - known as the forward pass - using a circuit graph, where each function is a ‘gate’:\n\n\n\nBackpropagation works by computing derivatives in the reverse direction, using the chain rule: \\[\n(g \\circ f)'(a) = g'(f(a))\\; f'(a).\n\\]\nOr, in Leibniz notation and using \\(b = f(a)\\) and \\(c = g(b)\\): \\[\n\\frac{\\partial c}{\\partial a} = \\frac{\\partial c}{\\partial b} \\;\\frac{\\partial b}{\\partial a}.\n\\]\nTo compute \\(\\frac{\\partial c}{\\partial a}\\), each gate uses the ‘local’ derivatives of its forward pass function \\(f'(a) = \\frac{\\partial b}{\\partial a}\\) and \\(g'(b) = \\frac{\\partial c}{\\partial b}\\), and the process starts with the identity derivative \\(\\frac{\\partial c}{\\partial c} = 1\\). The backward pass then has this structure:\n\\(\\frac{\\partial c}{\\partial c} \\gets 1\\)\n\\(\\frac{\\partial c}{\\partial b} \\gets \\frac{\\partial c}{\\partial c} \\cdot g'(b)\\)\n\\(\\frac{\\partial c}{\\partial a} \\gets \\frac{\\partial c}{\\partial b} \\cdot f'(a)\\)\nEach step follows from the chain rule. This is how we traverse the computation graph in reverse - and we can now augment the forward pass diagram with the backward pass:\n\n\n\n\n\nThat’s, in a nutshell, how backpropagation works - and how PyTorch computes gradients.\nFinally, let’s verify that this produces the same result of \\(30\\) as in our original example. For \\(f(a) = a^2\\), \\(g(b) = 5b\\) with \\(f'(a) = 2a\\), \\(g'(b) = 5\\), we get:\n\n\n\n\n\nIn practice, it’s a bit more complicated of course. For example, we have talked about derivatives but we need to compute gradients - that is, partial derivatives with respect to multiple variables. For example, if we have a function \\(f(x, y)\\), with gradient \\(\\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial x})\\) the backward pass through its gate looks like this (forward pass not shown).\n\n\n\nIn general, each gate in the backward pass receives an upstream gradient, multiplies it by its local gradients, and sends the results downstream.\n\n\n3.5.3 grad_fn\nFor PyTorch to find it’s way ‘back’ in the backpropagation under the hood, it dynamically builds a computational graph during the forward pass, storing the sequence of operations. Also, each non-leaf tensor stores a reference to the local gradient functions in the grad_fn attribute.\nWhen we call .backward(), PyTorch traverses that graph in reverse order, applying those gradient functions to propagate gradients back to the leaves, just as we’ve discussed.\nFor example, when we compute x + y, the resulting tensor stores grad_fn = &lt;AddBackward0&gt; - the ‘backward’ function that computes the gradient of the addition:\n\n# === the grad_fn attribute ===\nx = torch.tensor(1.0, requires_grad=True)\ny = torch.tensor(2.0, requires_grad=True)\n\nx + y\n\ntensor(3., grad_fn=&lt;AddBackward0&gt;)\n\n\nWe haven’t called .backward() here, so no gradients have been computed yet. But PyTorch has already set up the computation graph, so it’s ready to go as soon as one needs gradients with respect to x and y.\n\n\n3.5.4 Gradients and Control Structures\nBecause backpropagation uses the computational graph, it handles control structures just fine - as long as operations are traceable.\nHere’s an example where we compute \\(b = 10 \\cdot a\\) using a loop. PyTorch still gets the correct gradient for \\(\\frac{\\partial b}{\\partial a}\\):\n\n# === gradients calculation and control structures ===\na = torch.tensor(0.0, requires_grad=True)\n\nb = torch.tensor(0.0)\nfor i in range(10):\n    b += a\n\nb.backward()\nprint(f\"∂b/∂a: {a.grad}\")\n\n∂b/∂a: 10.0\n\n\n\n\n3.5.5 The mysterious of the viewBackward0\nNow that we understand backpropagation and how neural networks process batches of inputs, we can revisit something we encountered earlier when introducing linear neural networks.\nWhen we applied a linear neural network to some date we saw the attribute grad_fn=&lt;ViewBackward0&gt; popping up, like here\n\n# === unbatched input ===\n1linear_nn = nn.Linear(3, 2)\n\nv = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)  # grad_fn=&lt;ViewBackward0&gt;\n\n\n1\n\nif we initialize a network it’s parameters are randomly distributed\n\n\n\n\ntensor([0.0356, 0.6774], grad_fn=&lt;ViewBackward0&gt;)\n\n\nSo what’s going on?\nWe know that grad_fn stores the backward function of the final operation. In this case, apparently a view, i.e., an operation that returns a tensor on the same data but a different shape.\nThe code for handling linear layers is written in C++ and lies under aten/src/ATen/native/Linear.cpp in the PyTorch repository (at least, I think that’s the relevant code; I’m not 100% sure). The relevant part, which is called when the input is not a rank-2 tensor with some comments by me (I’m not 100% sure about the comments either)::\nstatic inline Tensor _flatten_nd_linear(const Tensor& input, const Tensor& weight, const Tensor& bias) {\n    // get the sizes of the input tensor\n    const auto input_sizes = input.sym_sizes();\n    \n    // calculate the flattened rank size\n    c10::SymInt flattened_dim = 1;\n    for (int64_t i = 0, ndim = input_sizes.size(); i &lt; ndim - 1; ++i) {\n        flattened_dim = flattened_dim * input_sizes[i];\n    }\n    \n    // reshape the input tensor to flatten all but the last axis\n    auto inp_reshape = input.reshape_symint({flattened_dim, input_sizes.at(input_sizes.size() -1)});\n    \n    // perform the linear operation\n    const auto result = at::addmm(bias, inp_reshape, weight.t());\n    \n    // calculate the new size of the output tensor\n    auto new_size = input_sizes.slice(0, input_sizes.size() - 1);\n    c10::SymDimVector sizes_vec(new_size.begin(), new_size.end());\n    sizes_vec.push_back(result.sym_size(1));\n    \n    // reshape the output tensor to match the original input shape\n    return result.view_symint(sizes_vec);\n}\nSo basically, the linear networks expect inputs of dimension 2. If we provide a tensor without a batch dimension, PyTorch internally reshapes it, creating a view with an extra axis of size 1. Then it runs the computation, and finally reshapes the output back to having no batches.\nWhen we explicitly provide a batch, that internal reshaping isn’t necessary. The final computation is then matrix multiplication plus bias addition, whose gradient function shows up as grad_fn = &lt;AddmmBackward0&gt;:\n\n# === batched input ===\nlinear_nn = nn.Linear(3, 2)\nv = torch.tensor([1.0, 0.0, 0.0]).unsqueeze(0)\n\nlinear_nn.forward(v)  # grad_fn=&lt;AddmmBackward0&gt;\n\ntensor([[-0.1479, -0.1826]], grad_fn=&lt;AddmmBackward0&gt;)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#nn-building-blocks",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#nn-building-blocks",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.6 NN building blocks",
    "text": "3.6 NN building blocks\nUntil now, we have only worked with a single linear layer. We can connect layers so that they feed into each other using nn.Sequential\n\n# === combining networks ===\n1layer1 = nn.Linear(4, 125)\nlayer2 = nn.Linear(125, 125)\nlayer3 = nn.Linear(125, 2)\n\nsequential_nn = nn.Sequential(layer1, layer2, layer3)\n\n\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0]).unsqueeze(0)\n\n# Applying sequential_nn produces the same as...\nr_seq = sequential_nn(tensor)\n2print(f\"sequential_nn: {r_seq}\")\n# ... applying the individual layers in order\nr_con = layer3(layer2(layer1(tensor)))\nprint(f\"concatenated: {r_con}\")\n\n\n1\n\nWe have to make sure manually that our layers fit together. Otherwise, we might get something like this: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x126 and 125x2)\n\n2\n\nI haven’t used the forward function for the input but just applied the layer to the input directly. This is possible, because PyTorch’s neural networks are callable objects, which just invoke the the forward function\n\n\n\n\nsequential_nn: tensor([[ 0.4786, -0.5196]], grad_fn=&lt;AddmmBackward0&gt;)\nconcatenated: tensor([[ 0.4786, -0.5196]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nCombining three linear layers doesn’t make much sense actually, as it is essentially multiplying three matrices together, resulting in a single matrix. Therefore, in our example, we could have simply used a single layer with 4 inputs and 2 outputs.\nWith linear layers, we are essentially limited to affine linear transformations. [Very technically, this is not entirely true. I recall seeing a YouTube video where only linear layers were used, and the non-linearity came from floating-point imprecision. However, I can’t find it.]\n\n3.6.1 ReLU\nTo move beyond the linear world, we need to add some non-linear layers. A frequently used function used in layers is the ‘Rectified Linear Unit’ (ReLU), which is defined by this simple function: \\[\n\\mathrm{ReLU}(x) = \\max(0,x).\n\\]\nand get’s applied to each input.\nIn this example we can see that the third component gets clipped to 0.\n\n# === ReLU clips input ===\nlayer = nn.ReLU()\n\nlayer(torch.tensor([1, 0, -1]).unsqueeze(0))\n\ntensor([[1, 0, 0]])\n\n\nSo, if we are using a network like this, it cannot be simplified to just one layer:\n\n# === combining linear with relu ===\nsequential_nn = nn.Sequential(\n    nn.Linear(6, 125), nn.ReLU(), nn.Linear(125, 125), nn.ReLU(), nn.Linear(125, 2)\n)\n1print(sequential_nn)\n\n\n1\n\nPyTorch provides us with a quick overview of a neural network whenever we print it.\n\n\n\n\nSequential(\n  (0): Linear(in_features=6, out_features=125, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=125, out_features=125, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=125, out_features=2, bias=True)\n)\n\n\nWe will a network like this to solve the CartPole environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#example---gan-on-atari-images",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#example---gan-on-atari-images",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.7 Example - GAN on Atari images",
    "text": "3.7 Example - GAN on Atari images\nThis final example of in (Lapan 2024, chap. 3) is quite big and might look daunting. Although it may look intimidating initially, nearly half of it consists of neural network declarations, but there’s still plenty of new stuff. For starters, simply running the code, viewing the results in tensorboard, and identifying familiar elements from this chapter should be enough.\nI had to ‘repair’ the code, so it would run for me. You can that code under /code/chapter_03/03_atari_gan.py.\n\n3.7.1 Running\nI use my IDE to run the code, but you can also use the command:\npython3 code/chapter_03/03_atari_gan.py\nYou should see output similar to this in the console:\nA.L.E: Arcade Learning Environment (version 0.11.0+dfae0bd)\n[Powered by Stella]\nINFO:__main__:Iter 100 in 32.86s: gen_loss=5.530e+00, dis_loss=5.460e-02\nINFO:__main__:Iter 200 in 33.80s: gen_loss=7.217e+00, dis_loss=5.016e-03\nThe actual data produced will be saved in the /runs folder and can be viewed using TensorBoard.\n\n\n3.7.2 Viewing the Data\nIf TensorBoard is installed (it should if you installed the requirements.txt), you can run:\ntensorboard --logdir runs\nYou should see something like this in the console (don’t worry about the reduced feature set message):\nTensorFlow installation not found - running with reduced feature set.\n\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https://github.com/tensorflow/tensorboard/issues/4784\n\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\nClicking the link will open TensorBoard in your browser.\nIt liked watching the data flow into TensorBoard, observing the losses of the discriminator and generator fluctuate, and seeing the latest ‘phases’ the generator went through.\nHere are some of the final images created by the generator in my run (yes, they are generated at that size):\n\n\n\nSample images seen in tensorboard crated by of 03_atari_gan.py\n\n\n\n\n3.7.3 Discussing the code\nThe code defines two classes, Discriminator and Generator, which are the neural networks being trained. The Discriminator is presented with images from Atari games and images produced by the Generator. It is trained to distinguish between real and fake images. The Generator, on the other hand, creates images and is trained to convince the Discriminator that these images are real Atari game images.\nHere’s a little code snippet that I copied from 03_atari_gan.py with some comments. Just to show that it also follows the basic training recipe:\n# train generator\ngen_optimizer.zero_grad() # gradients have to reset before computing new ones\ndis_output_v = net_discr(gen_output_v) # sample data\ngen_loss_v = objective(dis_output_v, true_labels_v) # compute loss\ngen_loss_v.backward() # calculate gradient\ngen_optimizer.step() # update parameters\nThis snippet uses:\n\nOptimizer: This updates the parameters in the direction of the gradient.\nObjective: This calculates the loss function.\n\nWe will delve deeper into these in the next chapter.\n\n\n3.7.4 Discriminator vs Generator Loss\nFor fun, I’ve included the losses from my run here with some heavy smoothing to make the trends clearer. It seems that, in principle, when one loss goes up, the other goes down, which makes sense.\nNote that the losses are on completely different scales. I think this means that the discriminator is quite confident about identifying fake images. However, the generator keeps using that tiny bit of uncertainty in the discriminator to improve its output quality.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\n\n# adjust these paths to wherever you saved your CSVs\ndisc_csv = \"quarto/data/atari_gan_dis_loss.csv\"\ngen_csv = \"quarto/data/atari_gan_gen_loss.csv\"\n\n# load data\ndf_disc = pd.read_csv(disc_csv)\ndf_gen = pd.read_csv(gen_csv)\n\n# smooth the losses using Savitzky-Golay filter\nwindow_length = 31  # must be odd; larger = smoother\npolyorder = 3  # polynomial order for the filter\n\ndf_disc[\"Smoothed_Value\"] = savgol_filter(\n    df_disc[\"Value\"], window_length=window_length, polyorder=polyorder\n)\ndf_gen[\"Smoothed_Value\"] = savgol_filter(\n    df_gen[\"Value\"], window_length=window_length, polyorder=polyorder\n)\n\n\n# create plot\nfig, ax1 = plt.subplots(figsize=(8, 5))\n\n# left y-axis: Discriminator loss\nax1.plot(\n    df_disc[\"Step\"],\n    df_disc[\"Smoothed_Value\"],\n    color=\"tab:blue\",\n    label=\"Discriminator Loss\",\n)\nax1.set_xlabel(\"Training Step\")\nax1.set_ylabel(\"Discriminator Loss\", color=\"tab:blue\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\n# right y-axis: Generator loss\nax2 = ax1.twinx()\nax2.plot(\n    df_gen[\"Step\"], df_gen[\"Smoothed_Value\"], color=\"tab:orange\", label=\"Generator Loss\"\n)\nax2.set_ylabel(\"Generator Loss\", color=\"tab:orange\")\nax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n\n# title and layout\nplt.title(\"GAN Losses (Dual Axis)\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nKarpathy, Andrej. 2016. “CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1.” https://www.youtube.com/watch?v=i94OvYb6noo.\n\n\nLapan, Maxim. 2024. Deep Reinforcement Learning Hands-on. 3rd ed. Packt Publishing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html",
    "href": "quarto/chapters/04-the-cross-entropy-method.html",
    "title": "4  The cross entropy method",
    "section": "",
    "text": "4.1 Softmax\nWe will use the cross entropy method to solve the stick on a pole challenge. Solved means that we can balance the pole so long that the episode is clapped\nWhich is after 500 time steps.\nWe will solve it with a very simple net. It has 4 observations: cart position, cart velocity, pole angle, pole angular velocity and two actions: move car left, move car right.\nA function that takes \\(n\\) inputs and makes them all positive and into a probability distribution. “Slice along dim” = fix all dimensions except dim, and let dim vary. A slice along dim means: fix all indices except for the one at position dim, and let that one vary.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The cross entropy method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#the-cross-entropy-method-in-practice",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#the-cross-entropy-method-in-practice",
    "title": "4  The cross entropy method",
    "section": "4.2 The cross-entropy method in practice",
    "text": "4.2 The cross-entropy method in practice\nSo with softmax we can make a fully fledged cartpole agent: It gets 4 inputs. Internally it usses its net to produce a probability distribution for the outputs and then it randomly chooses an action according to that distribution.\nThis means our agent is a policy-based agent, because the output of the nn is directly a policy \\(\\pi(a|s)\\).\nSampling We repeatedly let the agent interact with the environment to generate a diverse set of episodes. Each episode is a sequence of state–action pairs and the cumulative reward obtained.\nSelection Not all episodes are equally informative. We focus on the top‑performing episodes—the “elite set”—which represent trajectories that achieved above‑average returns. By ranking episodes by total reward and choosing a threshold (for instance, the 70th percentile), we discard the lower‑performing ones.\nFitting We treat the states and actions from elite episodes as supervised data: observations as inputs, the taken actions as “labels.” We then perform a gradient‑based update—typically via cross‑entropy (log‑likelihood) loss—to push the policy network toward reproducing these high‑reward behaviors.\nIteration As the policy improves, more episodes exceed the threshold, shifting the boundary upward. Abrupt jumps in performance are smoothed out over many iterations, yielding a stable learning curve.\n\nimport torch\nimport torch.nn.functional as F\n\nt = torch.FloatTensor([[[1,2,3], [4,5,6]], [[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]]])\nprint(t)\nprint(F.softmax(t, dim=0))\n\ntensor([[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 7.,  8.,  9.],\n         [10., 11., 12.]],\n\n        [[13., 14., 15.],\n         [16., 17., 18.]]])\ntensor([[[6.1290e-06, 6.1290e-06, 6.1290e-06],\n         [6.1290e-06, 6.1290e-06, 6.1290e-06]],\n\n        [[2.4726e-03, 2.4726e-03, 2.4726e-03],\n         [2.4726e-03, 2.4726e-03, 2.4726e-03]],\n\n        [[9.9752e-01, 9.9752e-01, 9.9752e-01],\n         [9.9752e-01, 9.9752e-01, 9.9752e-01]]])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The cross entropy method</span>"
    ]
  }
]
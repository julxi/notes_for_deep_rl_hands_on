[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for “Deep Reinforcement Learning Hands-On” by Maxim Lapan",
    "section": "",
    "text": "Preface\nWelcome to my study notes for Deep Reinforcement Learning Hands-On by Maxim Lapan (Lapan 2024).\nI’m writing these notes as a companion to my own studies, and they serve two main purposes.\nFirst, I’ve expanded on parts of the book that I wanted to explore in more depth. While the overall structure of these notes follows the book’s chapter layout, the content within each chapter deviates somewhat - I’ve chosen to focus on areas that I found especially interesting or challenging, other parts I kind of omit. These notes are not intended as a stand-alone resource, so without the book, they may be hard to follow.\nSecond, I encountered issues with some of the code from the official GitHub repository - most notably, the first GAN example for Atari games didn’t work, which was a bit disheartening for beginner me. To address this, I’ve created my own version of the accompanying code [TBD: include GitHub link]. It mirrors the structure of the official GitHub repository, but only includes examples I’ve modified in some way. Sometimes, the changes are quite substantial - just to reflect my personal style more closely - and I’ve explained them where relevant throughout the notes.\n\n\n\n\nLapan, Maxim. 2024. Deep Reinforcement Learning Hands-on. 3rd ed. Packt Publishing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "quarto/chapters/01-what-is-rl.html",
    "href": "quarto/chapters/01-what-is-rl.html",
    "title": "1  What Is Reinforcement Learning?",
    "section": "",
    "text": "I have nothing to add to this chapter and this page is just here to keep the chapter numbering aligned.\nHowever, I can really recommend Sutton and Barto (2018) for the theoretical side of reinforcement learning.\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, MA: MIT Press. https://mitpress.mit.edu/9780262039246/reinforcement-learning/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Reinforcement Learning?</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html",
    "href": "quarto/chapters/02-gymnasium.html",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "",
    "text": "2.1 Creating an environment\nThe best starting point for working with environments in Gymnasium is the official documentation, currently1 maintained by the Farama Foundation. I highly recommend consulting it - it makes it much easier to understand the structure of the action and observation spaces.\nTo create an environment, use gym.make(), passing the name of the environment as a string. The available environments and their different versions can be found in the documentation.\n# === creating an environment ===\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nenv.action_space\n\nDiscrete(2)\nAccording to the documentation, the CartPole-v1 environment has 4 observations and 2 possible actions. Let’s confirm this by inspecting the action space:\n# === checking out the action_space ===\nenv.action_space\n\nDiscrete(2)\nThis confirms that we have two discrete actions {0, 1} (for CartPole and also generally the discrete actions are simply numbered starting from 0). Also note that in Gymnasium the action_space is usually fixed, i.e., independent of the state. We will see how to deal with changing action spaces when they matter.\nNow let’s check the observation space:\n# === checking out the observation_space ===\nenv.observation_space\n\nBox([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\nThe Box space represents a 4-dimensional continuous space, with lower and upper bounds for each dimension. The third component (the shape attribute) indicates that each observation is a vector of 4 values. The first and second components specify the lower and upper bounds for each of the 4 dimension of the observation, respectively. For more detail on what each dimension represents, refer to the documentation. According to Gymnasium, a Box is “a space that represents closed boxes in Euclidean space.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html#the-random-cartpole-agent",
    "href": "quarto/chapters/02-gymnasium.html#the-random-cartpole-agent",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "2.2 The random CartPole agent",
    "text": "2.2 The random CartPole agent\nLet’s take the first step towards building a real agent: a random agent that takes actions randomly at each time step.\nThe idea is simple: the agent randomly samples actions from the environment’s action space until the episode ends - either by failure (the pole falling, cart to far off) or by timeout (very unlikely for a random agent).\n\n# === the random agent ===\nimport gymnasium as gym\n\n\ndef run_random_episode(env: gym.Env) -&gt; float:\n    env.reset()\n    total_reward = 0.0\n    done = False\n\n    while not done:\n        action = env.action_space.sample()\n        _, reward, terminated, truncated, _ = env.step(action)\n        total_reward += reward\n        done = terminated or truncated\n\n    return total_reward\n\nI’m including info boxes like the one below go into more detail of the code at many occasions. They are meant to clarify any potential open points. I often use them to introduce new Python syntax or concepts that will be used in subsequent code without further explanation.\n\n\n\n\n\n\nNote\n\n\n\n\nif you’re feeling lost, check out this Gymnasium basics guide which explains more of the fundamentals\nI use type hints like env: gym.Env as lightweight documentation. You’ll see this used frequently\nin this random agent, we don’t store the initial observation from env.reset() because the agent doesn’t use it. Normally, we need store it like so state, _ = env.reset()\nSimilarly, we ignore the new state returned by env.step(action) for the same reason\nenv.action_space.sample() returns a sample (random) action for the current state of the environment\n\n\n\nLet’s see how well the random agent fares over a few episodes:\n\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nepisodes = 5\nfor episode in range(1, episodes + 1):\n    print(f\"\\n=== Episode {episode}/{episodes} ===\")\n    reward = run_random_episode(env)\n    print(f\"Reward: {reward}\")\n\n\n=== Episode 1/5 ===\nReward: 18.0\n\n=== Episode 2/5 ===\nReward: 16.0\n\n=== Episode 3/5 ===\nReward: 11.0\n\n=== Episode 4/5 ===\nReward: 38.0\n\n=== Episode 5/5 ===\nReward: 10.0\n\n\nThe total reward in each episode corresponds to how long the pole stays balanced, because in CartPole-v1, the agent receives a reward of +1 for each step.\nObviously, it doesn’t perform any good. Our overall goal is to create agents that improve these rewards using reinforcement learning techniques.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/02-gymnasium.html#footnotes",
    "href": "quarto/chapters/02-gymnasium.html#footnotes",
    "title": "2  OpenAI Gym API and Gymnasium",
    "section": "",
    "text": "This has been checked as of the summer of 2025↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>OpenAI Gym API and Gymnasium</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html",
    "title": "3  Deep Learning with PyTorch",
    "section": "",
    "text": "3.1 PyTorch\nPyTorch is a library for deep learning. In this chapter, we’ll introduce its most important features, and we’ll cover everything else along the way later on.\nWe can import it with:\n# === importing pyTorch ===\nimport torch",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#tensors",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#tensors",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.2 Tensors",
    "text": "3.2 Tensors\nThe term “tensor” can have different meanings depending on the context. In PyTorch, a tensor is essentially a multidimensional array and it’s the fundamental data structure used for all computations.\nTo turn a Python list into a rank-1 tensor (a vector), we do:\n\n# === a vector in pyTorch ===\ntorch.tensor([1, 2, 3])\n\ntensor([1, 2, 3])\n\n\nHere is some key terminology for talking about tensors, based on the terminology used in TensorFlow’s introduction to tensors:\n\naxis: each axis corresponds to one index in the tensor.\nrank: the number of axes of a tensor.\n\na scalar has rank 0, e.g., torch.tensor(5).\na vector has rank 1, e.g., torch.tensor([1, 2, 3]).\na matrix has rank 2, e.g., torch.tensor([[1, 2], [3, 4]]).\n\nsize:\n\nthe size of an axis is the number of elements along it\nthe size of a tensor is the total number of elements it contains\n\nshape: a tuple giving the size along each axis.\n\nfor torch.tensor([1, 2, 3]), it’s shape is is (3,).\nfor torch.tensor([[1, 2], [3, 4]]), it’s shape is (2, 2).\n\n\nYou will often see “dimension” used interchangeably with rank, or say “the dimension” instead of the axis. Let me briefly motivate the use of the word dimension.\nIt’s not the same as the “dimension” in the phrase “a 3-dimensional vector”, which refers to the number of degrees of freedom in a vector space. Here, dimension refers to the number of indices you need to access the individual elements. For example, a 2D tensor needs two indices m[i][j].\nYou could also think of a tensor as a grid or a discrete space, where the rank tells you how many independent directions you have to move between elements.\nHere are some more ways to define tensors in pyTorch, we always use the same shape (3,2) for the tensors.\n\n# === some elemental ways to generate tensors ===\nprint(\"\\n a rank-2 tensor full of 0s\")\nprint(torch.zeros((3, 2)))\n\nprint(\"\\n a rank-2 tensor full of 7s\")\nprint(torch.full((3, 2), 7))\n\nprint(\"\\n a rank-2 tensor randomly filled with [0-9]\")\nprint(torch.randint(0, 10, (3, 2)))\n\n\n a rank-2 tensor full of 0s\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\n a rank-2 tensor full of 7s\ntensor([[7, 7],\n        [7, 7],\n        [7, 7]])\n\n a rank-2 tensor randomly filled with [0-9]\ntensor([[1, 1],\n        [1, 6],\n        [3, 7]])\n\n\nTo practise the terminology from above: the tuple \\((3,2)\\) defines the shape. So, the first axis has size 3 and the second axis has size 2. We can query it with .shape, which returns an object of type torch.Size.\n\n# === the shape of a tensor ===\nt = torch.zeros((3, 2))\nprint(t.shape)\n\ntorch.Size([3, 2])\n\n\nWe can combine multiple rank-d tensors into one rank-(d+1) tensor, by stacking them along a new axis\n\n# === stacking tensors ===\n# create three rank-2 tensors\ntensor1 = torch.tensor([[1, 2], [3, 4]])\ntensor2 = torch.tensor([[5, 6], [7, 8]])\ntensor3 = torch.tensor([[9, 10], [11, 12]])\n\n# stack the tensors along a new axis\nstacked_tensor = torch.stack((tensor1, tensor2, tensor3))\nstacked_tensor\n\ntensor([[[ 1,  2],\n         [ 3,  4]],\n\n        [[ 5,  6],\n         [ 7,  8]],\n\n        [[ 9, 10],\n         [11, 12]]])\n\n\nTo retrieve the second tensor back, we simply extract the second slice of the stacked tensor like this:\n\n# === slicing tensors ===\nstacked_tensor[1]\n\ntensor([[5, 6],\n        [7, 8]])\n\n\nTo clarify how indexing works in tensors, here’s an example using a tensor of shape (2, 3, 4), where each element contains an integer representing its index written in “mathematical notation”:\n\n# === index positions of tensors ===\n# each entry in this tensor shows it's coordinate in mathematical notation\nt = torch.tensor(\n    [\n        [[111, 112, 113, 114], [121, 122, 123, 124], [131, 132, 133, 134]],\n        [[211, 212, 213, 214], [221, 222, 223, 224], [231, 232, 233, 234]],\n    ]\n)\nprint(t)\nprint(f\"The element at mathematical index (2,3,4) is: {t[1][2][3]}\")\n\ntensor([[[111, 112, 113, 114],\n         [121, 122, 123, 124],\n         [131, 132, 133, 134]],\n\n        [[211, 212, 213, 214],\n         [221, 222, 223, 224],\n         [231, 232, 233, 234]]])\nThe element at mathematical index (2,3,4) is: 234",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#linear-neural-nets",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#linear-neural-nets",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.3 Linear neural nets",
    "text": "3.3 Linear neural nets\nWe’ll get to know the simplest neural nets. Linear neural nets. But along the way we discuss sme stuff that is important for all neural nets.\nFor working with neural nets (NN) in general we need to import\n\nimport torch.nn as nn\n\nA linear neural net, has a bunch of input and output nodes and forwards each signal coming to an input node to an output node multiplied with the weight of that connection. Additionally each output node has a constant bias applied to it. \nLet’s look at a little example.\n\n# === linear nn with custom parameters ===\n# create a linear layer\nlinear_nn = nn.Linear(3, 2)\n\n# manually set the weights\nweights = torch.FloatTensor([[1, 0, 1], [0, 1, -1]])\n\n# manually set the bias\nbias = torch.FloatTensor([2, -3])\n\n# assign the weights and bias to the linear layer\nlinear_nn.weight = nn.Parameter(weights)\nlinear_nn.bias = nn.Parameter(bias)\n\n\n\n\n\n\n\nNote\n\n\n\n\nwe have to make sure that the tensors for a neutral net are float tensors. That’s why I used `FloatTensor’ for creating the weights adn biases\na parameter is a tensor that is a parameter of neural net. \n\n\n\nWhen we apply this network to to a vector \\((x,y,z)\\) it does this computation \\[\n\\mathrm{linear\\_nn}(x,y,z) =\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\ y \\\\ z\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n2 \\\\ -3\n\\end{pmatrix}\n\\]\nFor example for the input \\((1,0,0)\\) we should get \\((3,-3)\\). We can verify this by by using forward to plug a tensor into the net:\n\n# === applying a net to input ===\nv = torch.tensor([1.0, 0.0, 0.0])\nlinear_nn.forward(v)\n\ntensor([ 3., -3.], grad_fn=&lt;ViewBackward0&gt;)\n\n\n\n\n\n\n\n\nNote\n\n\n\ninstead of writing torch.FloatTensor, we just can just use dedicated floats in the array by adding a decimal point.\n\n\nWe get the right tensor back, but also a bit extra information grad_fn=&lt;ViewBackward0&gt;. This reference to a gradient node, but we for that we have to talk about gradients first. And before we do even that, let’s see why we need gradients in the first place. But don’t worry in Section 3.5.1 we discuss the mystery of the viewBackward.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#stochastic-gradient-descent",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#stochastic-gradient-descent",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.4 Stochastic gradient descent",
    "text": "3.4 Stochastic gradient descent\nAt its core, training a neural network means finding model parameters θ (weights and biases) that minimize a loss function \\[\nL(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f(x_i;\\theta), y_i),\n\\]\nwhere \\(\\ell\\) measures prediction error on sample \\((x_i, y_i)\\), \\(L\\) is the (mean) loss on the whole batch of samples.\nFor basic gradient descent we try to minimize the loss by walking the parameters against the gradient \\[\n\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L(\\theta)\n\\]\nWhere \\(f\\) is represented as a neural net and \\(\\theta\\) are it’s parameters.\nLet’s look at a very simple example. Let’s pick a linear neural net of size \\((1,1)\\), i.e., one input and one output.  For this net the parameters are \\(\\theta = (w,b)\\) where \\(w\\) is the single weight and \\(b\\) the single bias of the net. For input \\(x\\) the net produces \\(f(x) = xw + b\\), a linear function. Our sample batches are just of size \\(1\\) so we get one input \\(x\\) and one desired output \\(y\\). As a loss function we use squarred error which is just \\(L(f(x;\\theta), y) = (f(x;\\theta) - y)^2\\).\nFor the sample \\((x,y) = (3,5)\\) we want to see which way the gradient points for parameters \\(\\theta = (0,0)\\): \\[\nL((w,b)) = (3w + b - 5)^2 = 9w^2 + 6w(b-5) + (b-5)^2\n\\] So the gradient is \\[\n\\nabla_\\theta L((w,b)) =\n\\begin{pmatrix}\n18w + 6b - 30 \\\\\n6w + 2b - 10\n\\end{pmatrix}\n\\]\nAnd thus for \\((w,b) = (0,0)\\) the gradient \\[\n\\nabla_\\theta L((0,0)) = \\begin{pmatrix}\n- 30 \\\\\n- 10\n\\end{pmatrix}\n\\]\nUsing pyTorch we get the same result (and that feels kind of magical):\n\n# === one stochastic GD step ===\nimport torch.nn as nn\nimport torch\n\n# linear net representing f(x) = 0.0 x + 0.0\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\nx = torch.tensor([3.0])\ny = torch.tensor([5.0])\nf_x = model_net.forward(x)\n\nL = (f_x - y).pow(2)\n# this says: computes ∇L\nL.backward()\n\n# this is how we access the components of ∇L\nprint(f\"grad(w): {model_net.weight.grad}\")\nprint(f\"grad(b): {model_net.bias.grad}\")\n\ngrad(w): tensor([[-30.]])\ngrad(b): tensor([-10.])\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nthe code has some stuff about tensor shapes that can trip you up when you’re new\nthe weight parameter is a tensor of shape (inputs, outputs)=(1,1), as a list this is [[w]]\nthe same is true for the shapes of the gradients\nneutral nets like their arguments to be batched. So in our example even though the basic input shape for the nn is (1) it really would like tho have a batch of (1), i.e., a shape (b,1). Our batch has only size 1, so we want to just add an extra dimension at the beginning, which is done by .unsqueeeze(0) (new 0 dimension please)\n\n\n\nNote the nice terminology. .forward pushes a tensor x through our net. Abd with .backward we update our net according to how well it’s result was for x. \nWe can use this to create a proper little stochastic gradient descent. We will extend the setup from above. We have our model function \\(f(x;\\theta_0)\\) with \\(\\theta_0 = (0,0)\\). We want to approximate a target function \\(f(x;\\theta_*)\\) with \\(\\theta_* = (2,1)\\). Of course we should imagine here that we don’t know our the parameters of the target function. We will do stochastic gradient descent by sampling \\(x\\) uniformly from \\([0,1)\\) (this region is chosen arbitrarily) and do our updates according to \\[\n\\theta_{t+1} \\gets \\theta_t - \\eta \\nabla_{\\theta}L(f(x_t;\\theta_t),y_t),\n\\] where \\(L(a,b) = (a - b)^2\\) and \\(y_t = f(x_t, \\theta_*)\\).\n\n# === stochastic GD ===\ntorch.manual_seed(7)\nSTEP_SIZE = 0.1  # η\nSTEPS = 200\n\n# target function with (w,b) = (2,1)\ntarget_function = nn.Linear(1, 1)\ntarget_function.weight = nn.Parameter(torch.tensor([[2.0]]))\ntarget_function.bias = nn.Parameter(torch.tensor([1.0]))\n\n# model function initialized with (w,b) = (0,0)\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n\ndef step(x, y):\n    # compute the loss gradient\n    o = model_net.forward(x)\n    loss = (o - y).pow(2)\n    loss.backward()\n\n    # step the parameters according to gradient\n    model_net.weight = nn.Parameter(\n        model_net.weight - STEP_SIZE * model_net.weight.grad\n    )\n    model_net.bias = nn.Parameter(model_net.bias - STEP_SIZE * model_net.bias.grad)\n\n\n# do steps and record each θ = (w,b)\ntorch_path = []\nfor _ in range(STEPS):\n    torch_path.append(\n        (model_net.weight.data[0].item(), model_net.bias.data.data[0].item())\n    )\n    x = torch.rand((1, 1))\n    y = target_function.forward(x)\n    step(x, y)\nprint(f\"the final θ = {torch_path[-1]}\")\n\nthe final θ = (1.9013594388961792, 1.0614407062530518)\n\n\nThis actually worked. And it this recipe of\n\nget samples (or a batch)\ncompute loss\nstep the parameters\n\nalso work for much more complicated nets and other loss functions. Of course, there should be a theory behind the concrete loss function and how to step the parameters.\nActually I want to go a little bit into this for our example. It might still be kind of very abstract and not really palpable why it does work actually. Especially when you look at the first couple of results it’s not clear they are at all going the right way.\n\ntorch_path[0:5]\n\n[(0.0, 0.0),\n (0.0682234913110733, 0.2931046187877655),\n (0.19812387228012085, 0.5987083911895752),\n (0.23019880056381226, 0.7535954117774963),\n (0.4016020596027374, 1.0257779359817505)]\n\n\nLet’s see what stochastic gradient descent does in average. That is what is the expected gradient. What we do basically, in each step take a sample of the expected loss \\[\n\\begin{split}\nF(w,b) &= \\mathbb{E}_{X\\sim U(0,1)}\\big[(wX + b - (2X +1))^2\\big] \\\\\n&= \\int_0^1 ((w-2)x + (b-1))^2 \\mathrm{d}x\\\\\n&= \\frac{(w-2)^2}{3} + (w-2)(b-1) + (b-1)^2\n\\end{split}\n\\] and the gradient \\[\n\\nabla F(w,b) =\n\\begin{pmatrix}\n\\frac{2}{3}(w-2) + (b-1)\\\\\n(w-2) + 2(b-1)\n\\end{pmatrix}\n\\]\nThis is called deterministic GD as the expectation removes any stochasticity (and of course we can’t use it in pracitce becaues we can’t calculate this expectadion.)\nWe can write the formula for deterministic GD with step size \\(\\eta\\), as \\[\n\\begin{pmatrix}\nw_{t+1}\\\\\nb_{t+1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix}\n- \\eta\n\\begin{pmatrix}\n\\frac{2}{3} & 1 \\\\\n1 & 2\n\\end{pmatrix}\n\\left(\n\\begin{pmatrix}\nw_{t}\\\\\nb_{t}\n\\end{pmatrix} -\n\\begin{pmatrix}\n2\\\\\n1\n\\end{pmatrix}\n\\right)\n\\]\nSo there is some theory behind this that this converges when when \\(\\eta A\\) is a contraction and \\(I - A\\) invertible, but let us rather plot the vector field given by the update rule \\(-A (\\theta - \\theta^*)\\) and see how the deterministic GD and the stochastic GD we have computed earlier fare\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ── Settings ───────────────────────────────────\nw_star, b_star = 2.0, 1.0\nA = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\nlr = 0.1\nnum_steps = 200\n\n# ── Build grid for vector field ───────────────\nw_vals = np.linspace(0, 3, 15)\nb_vals = np.linspace(0, 2, 15)\nW, B = np.meshgrid(w_vals, b_vals)\nU = np.zeros_like(W)\nV = np.zeros_like(B)\n\n# Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\nfor i in range(W.shape[0]):\n    for j in range(W.shape[1]):\n        theta = np.array([W[i, j], B[i, j]])\n        grad = A.dot(theta - np.array([w_star, b_star]))\n        U[i, j] = -grad[0]\n        V[i, j] = -grad[1]\n\n# ── Deterministic GD trajectory ──────────────\ndet_path = [(0.0, 0.0)]\nfor _ in range(num_steps):\n    w, b = det_path[-1]\n    grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n    det_path.append((w - lr * grad[0], b - lr * grad[1]))\nw_det, b_det = zip(*det_path)\n\n\n# from pyTorch\nw_torch, b_torch = zip(*torch_path)\n\n# ── Plot vector field + paths ────────────────\nplt.figure(figsize=(8, 6))\nplt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\nplt.plot(w_det, b_det, \"o-\", label=\"Deterministic GD\")\nplt.plot(w_torch, b_torch, \".-\", label=\"Stochastic GD\")\nplt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\nplt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\nplt.xlabel(\"Weight $w$\")\nplt.ylabel(\"Bias $b$\")\nplt.title(\"Gradient Vector Field\\nand GD Trajectories\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s also so interesting to see that the even though the stochastic GD bumbles along the 2D-plane and the end it’s not much worse that deterministic GD.\n\n3.4.1 Batches\nActually nets want batches of data. Until now pyTorch just applied some behind the scenes magic so that we didn’t notice.\nSo in our model_net we gave it one vector of size 1, i.e., a 1-Dimensional tensor. The basic function actually expects a batch a tensor of shape (b,1) where \\(b\\) is the lengtht of the batch and it returns all the results as a shape (b,1) tensor again. Just to make it general. If we have a nn that is can process tensors of shape s_in and produces tensors of shape s_out we actually should feed the network tensors of shape (b,s_in) and the forward result is a tensor of shape (b,s_out). And now let’s go back again to what happens when we have only one tensor. Then behind the scenes pyTorch takes our shape (1) tensor and transforms it into a shape (1,1) tensor (so [x] became [[x]], which looks pointless without context). For that we can use unsqueeze\n\nt = torch.tensor([1, 2])\nt.unsqueeze(0)\n\ntensor([[1, 2]])\n\n\nIt basically wraps an extra dimension around the dimension that we have specified (here it was the most outer one).\nIt basically introduces another dimension of size 1. Actually squeeze does get rid of (pointless) dimensions of size 1\n\nt = torch.randn(2, 1, 2, 1)\nprint(f\"before squeeze: {t.shape}\")\nt = t.squeeze()\nprint(f\"after squeeze: {t.shape}\")\n\nbefore squeeze: torch.Size([2, 1, 2, 1])\nafter squeeze: torch.Size([2, 2])\n\n\nBut batches are great they can reduce the bumbleness of our updates and pyTorch can compute them also much faster internally then feeding the tensors 1 by 1 in python loops.\nLet’s look at the linear layer example from above again and introduce some batches.\n\n# === Simple example with linear layers ===\ntorch.manual_seed(0)\nBATCH_SIZE = 20\nSTEP_SIZE = 0.1\nSTEPS = 100\n\n# target function with (w,b) = (2,1)\ntarget_function = nn.Linear(1, 1)\ntarget_function.weight = nn.Parameter(torch.tensor([[2.0]]))\ntarget_function.bias = nn.Parameter(torch.tensor([1.0]))\n\n# model function initialized with (w,b) = (0,0)\nmodel_net = nn.Linear(1, 1)\nmodel_net.weight = nn.Parameter(torch.tensor([[0.0]]))\nmodel_net.bias = nn.Parameter(torch.tensor([0.0]))\n\n\n# this is one step\ndef step(x_batch, y_batch):\n    loss = (model_net.forward(x_batch) - y_batch).pow(2).mean()\n    loss.backward()\n\n    model_net.weight = nn.Parameter(\n        model_net.weight - STEP_SIZE * model_net.weight.grad\n    )\n    model_net.bias = nn.Parameter(model_net.bias - STEP_SIZE * model_net.bias.grad)\n\n\n# do steps and record each θ = (w,b)\ntorch_path = []\nfor _ in range(STEPS):\n    torch_path.append(\n        (model_net.weight.data[0].item(), model_net.bias.data.data[0].item())\n    )\n    x_batch = torch.rand((BATCH_SIZE, 1))\n    y_batch = target_function.forward(x_batch)\n    step(x_batch, y_batch)\nprint(f\"the final θ = {torch_path[-1]}\")\nprint((7 / 30, 0.4))\n\nthe final θ = (1.693508267402649, 1.1575511693954468)\n(0.23333333333333334, 0.4)\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ── Settings ───────────────────────────────────\nw_star, b_star = 2.0, 1.0\nA = np.array([[2 / 3, 1.0], [1.0, 2.0]])  # Hessian of expected MSE\nlr = 0.1\nnum_steps = 100\n\n# ── Build grid for vector field ───────────────\nw_vals = np.linspace(0, 3, 15)\nb_vals = np.linspace(0, 2, 15)\nW, B = np.meshgrid(w_vals, b_vals)\nU = np.zeros_like(W)\nV = np.zeros_like(B)\n\n# Compute field:   (U,V) = - ∇F = -A·(θ−θ*)\nfor i in range(W.shape[0]):\n    for j in range(W.shape[1]):\n        theta = np.array([W[i, j], B[i, j]])\n        grad = A.dot(theta - np.array([w_star, b_star]))\n        U[i, j] = -grad[0]\n        V[i, j] = -grad[1]\n\n# ── Deterministic GD trajectory ──────────────\ndet_path = [(0.0, 0.0)]\nfor _ in range(num_steps):\n    w, b = det_path[-1]\n    grad = A.dot(np.array([w, b]) - np.array([w_star, b_star]))\n    det_path.append((w - lr * grad[0], b - lr * grad[1]))\nw_det, b_det = zip(*det_path)\n\n\n# from pyTorch\nw_torch, b_torch = zip(*torch_path)\n\n# ── Plot vector field + paths ────────────────\nplt.figure(figsize=(8, 6))\nplt.quiver(W, B, U, V, angles=\"xy\", scale_units=\"xy\", alpha=0.5)\nplt.plot(w_det, b_det, \"o-\", label=\"Deterministic GD\")\nplt.plot(w_torch, b_torch, \".-\", label=\"Stochastic GD\")\nplt.scatter([w_star], [b_star], marker=\"x\", s=100, label=\"Optimum\")\nplt.scatter([7 / 30], [0.4], marker=\"x\", s=100, label=\"first step\")\nplt.text(w_star, b_star, \"  (2,1)\", va=\"bottom\")\n\nplt.xlabel(\"Weight $w$\")\nplt.ylabel(\"Bias $b$\")\nplt.title(\"Gradient Vector Field\\nand GD Trajectories\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we can see that the batched stochastic GD follows the determinsitce GD much more closely.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#gradients",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#gradients",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.5 Gradients",
    "text": "3.5 Gradients\nNow we look at calculating gradients a more detailed.\nLet’s take something super simple \\(f(x,y) = x + 2y\\) and use pyTorch to compute it’s gradient. For that we need the tensors to be float tensors, otherwise we get something like RuntimeError: Only Tensors of floating point and complex dtype can require gradients which means a tensor must be a float tensor so we can compute gradients (which is quite sensible, it’s hard to compute gradients for integers). We also have to add requires_grad=True because for manually created tensors this is false by default.\n\nimport numpy as np\n\nx = torch.tensor([1.0], requires_grad=True)\ny = torch.tensor([2.0], requires_grad=True)\n\nresult = x + 2 * y\nresult.backward()\n\n\nprint(f\"gradient for x:  {x.grad}\")\nprint(f\"gradient for y: {y.grad}\")\n\ngradient for x:  tensor([1.])\ngradient for y: tensor([2.])\n\n\nWhen we print plus now we see a function reference that pyTorch uses to determine these gradients.\n\nresult\n\ntensor([5.], grad_fn=&lt;AddBackward0&gt;)\n\n\nWe get a reference to the function AddBackward, because this function ‘knows’ how to handle the backward pass of computing the gradients for an addition (because result came from an addition).\nJust for fun we can check other operations\n\nprint(x * y)\nprint(x / y)\nprint(max(x, y))\nprint(x &lt; y)\nprint(x.unsqueeze(0))\n\ntensor([2.], grad_fn=&lt;MulBackward0&gt;)\ntensor([0.5000], grad_fn=&lt;DivBackward0&gt;)\ntensor([2.], requires_grad=True)\ntensor([True])\ntensor([[1.]], grad_fn=&lt;UnsqueezeBackward0&gt;)\n\n\n\n3.5.1 The mysterious of the viewBackward0\nBecause we didn’t input a batch of tensors internally it created a batch (but just as a view) like this:\n\nprint(x.view(-1, x.size(-1)))\n\ntensor([[1.]], grad_fn=&lt;ViewBackward0&gt;)\n\n\nAnd that’s where the view came from",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#nn-building-blocks",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#nn-building-blocks",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.6 NN building blocks",
    "text": "3.6 NN building blocks\nWe can connect layers with Sequential. \n\n# === combining networks ===\ninput_layer = nn.Linear(4, 125)\nhidden_layer = nn.Linear(125, 125)\noutput_layer = nn.Linear(125, 2)\n\nsequential_nn = nn.Sequential(input_layer, hidden_layer, output_layer)\n\n\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0]).unsqueeze(0)\n\n# Applying sequential_nn produces the same as...\nprint(f\"sequential_nn: {sequential_nn(tensor)}\")\n# ... applying the individual layers in order\nprint(f\"concatenated: {output_layer(hidden_layer(input_layer(tensor)))}\")\n\nsequential_nn: tensor([[-0.4515,  0.2871]], grad_fn=&lt;AddmmBackward0&gt;)\nconcatenated: tensor([[-0.4515,  0.2871]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nI haven’t used the forward function for the input but just applied the layer to the input directly. This does just use the forward function \nAnd yes, we have to make manually that our layers do fit together othewise we might get something like this: RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x126 and 125x2)\nthere are, of course, packages that take this work away from you, but we’re learning here. So we don’t use them \n\n\n\nWe combined 3 linear layers, which actually doesn’t make much sense as this is basically multiplying three matrices together and this is just a single matrix again. And similarly we could have just done with a single layer from 4 inputs to 2 outputs.\nBut we can add other layers in between. For example a Rectified Linear Unit (ReLU) which is very easy \\[\n\\mathrm{ReLU}(x) = \\max(0,x)\n\\] and when we add as a layer it just ‘rectifies’ each input and forwards it (and it doesn’t need any size specification).\nIn this example we can see that the third component gets clipped.\n\n# === ReLU clips input ===\nlayer = nn.ReLU()\n\nlayer(torch.tensor([1, 0, -1]).unsqueeze(0))\n\ntensor([[1, 0, 0]])\n\n\nSo and if we are for example using a net like this, it can’t be simplified (or at least nobody knows how and if):\n\n# === combining linear with relu ===\nnn.Sequential(\n    nn.Linear(6, 125), nn.ReLU(), nn.Linear(125, 125), nn.ReLU(), nn.Linear(125, 2)\n)\n\nSequential(\n  (0): Linear(in_features=6, out_features=125, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=125, out_features=125, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=125, out_features=2, bias=True)\n)\n\n\nWe will actually use this (or very similar) net to solve the cartpole environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#example---gan-on-atari-images",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#example---gan-on-atari-images",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.7 Example - GAN on Atari images",
    "text": "3.7 Example - GAN on Atari images\nThis example from the book is quite big and I think as a beginner it’s ok not to bother to understand it. However, it’s fun to run it out of the box (when it works and it didn’t for me at the beginning and that was quite demotivating). It’s also nice to identify the things that we have already seen in a proper example and also teaser some things that are still to come.\nThe running file is under chapter-03/03_atari_gan.py. It looks a bit daunting at first but nearly half of it is “just” declaration of the neural networks. Of course there is quite some stuff to learn here as well, but for the beginning I would say “there are two sufficently complicated nets” is enough.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/03-deep-learning-with-pytorch.html#discriminator-vs-generator-loss",
    "href": "quarto/chapters/03-deep-learning-with-pytorch.html#discriminator-vs-generator-loss",
    "title": "3  Deep Learning with PyTorch",
    "section": "3.8 Discriminator vs Generator Loss",
    "text": "3.8 Discriminator vs Generator Loss\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# adjust these paths to wherever you saved your CSVs\ndisc_csv = \"quarto/data/atari_gan_dis_loss.csv\"\ngen_csv = \"quarto/data/atari_gan_gen_loss.csv\"\n\n# TensorBoard CSVs typically have columns: wall_time, step, value\ndf_disc = pd.read_csv(disc_csv)\ndf_gen = pd.read_csv(gen_csv)\n\n# Create plot\nfig, ax1 = plt.subplots(figsize=(8, 5))\n\n# Left y-axis: Discriminator loss\nax1.plot(\n    df_disc[\"Step\"], df_disc[\"Value\"], color=\"tab:blue\", label=\"Discriminator Loss\"\n)\nax1.set_xlabel(\"Training Step\")\nax1.set_ylabel(\"Discriminator Loss\", color=\"tab:blue\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\n# Right y-axis: Generator loss\nax2 = ax1.twinx()\nax2.plot(df_gen[\"Step\"], df_gen[\"Value\"], color=\"tab:orange\", label=\"Generator Loss\")\nax2.set_ylabel(\"Generator Loss\", color=\"tab:orange\")\nax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n\n# Title and layout\nplt.title(\"GAN Losses (Dual Axis)\")\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Deep Learning with PyTorch</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html",
    "href": "quarto/chapters/04-the-cross-entropy-method.html",
    "title": "4  The cross entropy method",
    "section": "",
    "text": "4.1 Softmax\nWe will use the cross entropy method to solve the stick on a pole challenge. Solved means that we can balance the pole so long that the episode is clapped\nWhich is after 500 time steps.\nWe will solve it with a very simple net. It has 4 observations: cart position, cart velocity, pole angle, pole angular velocity and two actions: move car left, move car right.\nA function that takes \\(n\\) inputs and makes them all positive and into a probability distribution. “Slice along dim” = fix all dimensions except dim, and let dim vary. A slice along dim means: fix all indices except for the one at position dim, and let that one vary.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The cross entropy method</span>"
    ]
  },
  {
    "objectID": "quarto/chapters/04-the-cross-entropy-method.html#the-cross-entropy-method-in-practice",
    "href": "quarto/chapters/04-the-cross-entropy-method.html#the-cross-entropy-method-in-practice",
    "title": "4  The cross entropy method",
    "section": "4.2 The cross-entropy method in practice",
    "text": "4.2 The cross-entropy method in practice\nSo with softmax we can make a fully fledged cartpole agent: It gets 4 inputs. Internally it usses its net to produce a probability distribution for the outputs and then it randomly chooses an action according to that distribution.\nThis means our agent is a policy-based agent, because the output of the nn is directly a policy \\(\\pi(a|s)\\).\nSampling We repeatedly let the agent interact with the environment to generate a diverse set of episodes. Each episode is a sequence of state–action pairs and the cumulative reward obtained.\nSelection Not all episodes are equally informative. We focus on the top‑performing episodes—the “elite set”—which represent trajectories that achieved above‑average returns. By ranking episodes by total reward and choosing a threshold (for instance, the 70th percentile), we discard the lower‑performing ones.\nFitting We treat the states and actions from elite episodes as supervised data: observations as inputs, the taken actions as “labels.” We then perform a gradient‑based update—typically via cross‑entropy (log‑likelihood) loss—to push the policy network toward reproducing these high‑reward behaviors.\nIteration As the policy improves, more episodes exceed the threshold, shifting the boundary upward. Abrupt jumps in performance are smoothed out over many iterations, yielding a stable learning curve.\n\nimport torch\nimport torch.nn.functional as F\n\nt = torch.FloatTensor([[[1,2,3], [4,5,6]], [[7,8,9],[10,11,12]], [[13,14,15],[16,17,18]]])\nprint(t)\nprint(F.softmax(t, dim=0))\n\ntensor([[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.]],\n\n        [[ 7.,  8.,  9.],\n         [10., 11., 12.]],\n\n        [[13., 14., 15.],\n         [16., 17., 18.]]])\ntensor([[[6.1290e-06, 6.1290e-06, 6.1290e-06],\n         [6.1290e-06, 6.1290e-06, 6.1290e-06]],\n\n        [[2.4726e-03, 2.4726e-03, 2.4726e-03],\n         [2.4726e-03, 2.4726e-03, 2.4726e-03]],\n\n        [[9.9752e-01, 9.9752e-01, 9.9752e-01],\n         [9.9752e-01, 9.9752e-01, 9.9752e-01]]])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The cross entropy method</span>"
    ]
  }
]